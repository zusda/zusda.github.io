<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>java审计1:ModelAttribute</title>
    <url>/2024/10/10/java%E5%AE%A1%E8%AE%A11/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>本章节是按照 <a href="https://aluvion.github.io/2019/04/02/%E8%87%AA%E5%8A%A8%E7%BB%91%E5%AE%9A%E6%BC%8F%E6%B4%9E-war%E8%BF%9C%E7%A8%8B%E8%B0%83%E8%AF%95/">文章</a>
进行 <span class="citation" data-cites="ModelAttribute">@ModelAttribute</span>
功能点的漏洞审计，以及实际debug中各种执行。</p>
<p>程序 shop1.war可以在原博客主中下载。</p>
<span id="more"></span>
<h2 id="配置debug环境">配置debug环境</h2>
<p>tomcat + idea 远程调试代码</p>
<p><strong>1.安装tomcat</strong></p>
<p>• Tomcat 9.x：支持 Java 8、Java 11、Java 17（及更高版本）。</p>
<p>• Tomcat 10.x：要求 Java 11 或更高版本，主要用于支持 Servlet 4.0
规范及其相关特性。</p>
<p>主机里只有1.8，因此我们用的tomcat 9</p>
<p>https://tomcat.apache.org/download-90.cgi</p>
<p>点击如下zip进行下载</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010202936176.png" alt="image-20241010202936176">
<figcaption aria-hidden="true">image-20241010202936176</figcaption>
</figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ unzip apache-tomcat-9.0.95.zip</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> apache-tomcat-9.0.95/</span><br><span class="line"></span><br><span class="line">$ TC=$(<span class="built_in">pwd</span>)</span><br></pre></td></tr></table></figure>
<p><strong>2.war包存放路径</strong></p>
<p>$TC/webapps/shop-1.0.0.war</p>
<p><strong>3.tomcat开启远程调试端口</strong></p>
<p>修改文件 catalina.sh，在注释后第一行添加如下</p>
<p>CATALINA_OPTS="-Xdebug
-Xrunjdwp:transport=dt_socket,address=<strong>8000</strong>,suspend=n,server=y"</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010203111195.png" alt="image-20241010203111195">
<figcaption aria-hidden="true">image-20241010203111195</figcaption>
</figure>
<p>apache开启与关闭</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="variable">$TC</span>/bin/catalina.sh start</span><br><span class="line"></span><br><span class="line"><span class="variable">$TC</span>/bin/catalina.sh stop</span><br></pre></td></tr></table></figure>
<p>检测调试端口是否开启成功</p>
<p>netstat -tuln | grep 8000</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010203208761.png" alt="image-20241010203208761">
<figcaption aria-hidden="true">image-20241010203208761</figcaption>
</figure>
<p>如果你想url的根路径就是你的war包的话，可以修改/opt/apache-tomcat-9.0.17/conf/server.xml</p>
<blockquote>
<p><Host name="localhost" appbase unpackwars="true" autodeploy="true"></Host></p>
<p><Context path docbase="/home/student/FSWA/module-2/apache-tomcat-9.0.95/webapps/shop-1.0.0" debug="0" reloadable="true" crosscontext="true"></Context></p>
</blockquote>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010203315018.png" alt="image-20241010203315018">
<figcaption aria-hidden="true">image-20241010203315018</figcaption>
</figure>
<p>默认web服务开放端口8080</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010203330170.png" alt="image-20241010203330170">
<figcaption aria-hidden="true">image-20241010203330170</figcaption>
</figure>
<p>开启apache后，通过浏览器访问即可</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010203442840.png" alt="image-20241010203442840">
<figcaption aria-hidden="true">image-20241010203442840</figcaption>
</figure>
<p><strong>4.idea2024配置</strong></p>
<p>由于我们是war包，因此得反编译相同结构的java文件内容才行。</p>
<p>如果是当前项目打包好的war放到tomcat中直接debug应该是没啥问题的，但如果是war包反编译弄成java代码后进行debug的话，可能会有行数不一致的情况</p>
<p>在classes文件夹隔壁，新创建一个java文件夹</p>
<p>打开如下设置</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145226148.png" alt="image-20241011145226148">
<figcaption aria-hidden="true">image-20241011145226148</figcaption>
</figure>
<p>选择刚才新创建的java文件夹，然后按照如下进行设置</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145243568.png" alt="image-20241011145243568">
<figcaption aria-hidden="true">image-20241011145243568</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145256100.png" alt="image-20241011145256100">
<figcaption aria-hidden="true">image-20241011145256100</figcaption>
</figure>
<p>然后吧classes中所有文件复制过去，把.class文件换成反编译后的.java文件</p>
<p>当然，弄过去后会一堆报错，但实际没关系，关闭语法报错提示正常调试</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145327715.png" alt="image-20241011145327715">
<figcaption aria-hidden="true">image-20241011145327715</figcaption>
</figure>
<p>java文件滑动条这里右键出来</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/1.png" alt="file://C:/Users/admin/AppData/Local/Temp/.MWY8U2/1.png">
<figcaption aria-hidden="true">file://C:/Users/admin/AppData/Local/Temp/.MWY8U2/1.png</figcaption>
</figure>
<p>接下来配置调试器</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145449711.png" alt="image-20241011145449711">
<figcaption aria-hidden="true">image-20241011145449711</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145454446.png" alt="image-20241011145454446">
<figcaption aria-hidden="true">image-20241011145454446</figcaption>
</figure>
<p>关联到之前apache开放的调试端口</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145507746.png" alt="image-20241011145507746">
<figcaption aria-hidden="true">image-20241011145507746</figcaption>
</figure>
<h2 id="开始调试">开始调试</h2>
<p>在 Java Web
开发中，框架默认是否开启<strong>参数自动绑定</strong>取决于所使用的框架。Spring
MVC
框架默认开启自动绑定，会根据请求中的参数自动将其绑定到控制器方法中的对象或参数。</p>
<p><strong>对象绑定</strong>：当请求中的参数名与对象的属性名一致时，Spring
MVC 会自动将请求参数的值绑定到对象的对应属性上。例如：</p>
<p>如果请求中有 balance=100 这样的参数，Spring 会将其自动赋值给 user
对象中的 balance 属性。</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241010202743976.png" alt="image-20241010202743976">
<figcaption aria-hidden="true">image-20241010202743976</figcaption>
</figure>
<h3 id="buy路径">1.buy路径</h3>
<p>路由中我们看到buy路径有 <span class="citation" data-cites="ModelAttribute">@ModelAttribute</span> 修饰的对象 user</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145557343.png" alt="image-20241011145557343">
<figcaption aria-hidden="true">image-20241011145557343</figcaption>
</figure>
<p>这意味着我们能够修改user中的任意属性</p>
<p>我们查看他有哪些属性</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145608992.png" alt="image-20241011145608992">
<figcaption aria-hidden="true">image-20241011145608992</figcaption>
</figure>
<p>我们能看到代码中有一段使用了setBalance将修改当前的user对象内容。（自动绑定我理解为只是在当前函数中，使用到该参数时，才替换成我们绑定的值，但对象实际的值并不会修改）</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145640382.png" alt="image-20241011145640382">
<figcaption aria-hidden="true">image-20241011145640382</figcaption>
</figure>
<p>那么我们修改user对象中balance的值，就能够修改当前user.getBalance()的内容了。</p>
<p>当然，源代码中也有对该参数进行禁止绑定的配置，但是能够通过大小写进行绕过。</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145652303.png" alt="image-20241011145652303">
<figcaption aria-hidden="true">image-20241011145652303</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145658795.png" alt="image-20241011145658795">
<figcaption aria-hidden="true">image-20241011145658795</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145704590.png" alt="image-20241011145704590">
<figcaption aria-hidden="true">image-20241011145704590</figcaption>
</figure>
<h3 id="profile路径">2.profile路径</h3>
<p>除了/buy以外，/profile也使用了 <span class="citation" data-cites="ModelAttribute">@ModelAttribute</span> 修饰user</p>
<p>并且注意到cart.add中利用user的内容进行添加。</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145757832.png" alt="image-20241011145757832">
<figcaption aria-hidden="true">image-20241011145757832</figcaption>
</figure>
<p>那么我们把productId为4的flag添加到我们购物车中即可</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145807571.png" alt="image-20241011145807571">
<figcaption aria-hidden="true">image-20241011145807571</figcaption>
</figure>
<p>还是先debug查看需要什么</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145818220.png" alt="image-20241011145818220">
<figcaption aria-hidden="true">image-20241011145818220</figcaption>
</figure>
<p>那么我们修改这个id即可，注意User中对cart的赋值使用的是
setCartItems，因此传参的时候，传的是 cartItems（大小写应该无所谓）</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145829136.png" alt="image-20241011145829136">
<figcaption aria-hidden="true">image-20241011145829136</figcaption>
</figure>
<p>cartItems[0].id=4</p>
<p>因为是get请求，因此需要全部url编码</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145841144.png" alt="image-20241011145841144">
<figcaption aria-hidden="true">image-20241011145841144</figcaption>
</figure>
<p>（我应该设置的[0]，不过不知道为什么所有item都变了）</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145857520.png" alt="image-20241011145857520">
<figcaption aria-hidden="true">image-20241011145857520</figcaption>
</figure>
<p>当然，我们也能用post请求</p>
<p>别直接GET改POST，burp中有修改post功能，帮你添加上post需要的字段</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145909441.png" alt="image-20241011145909441">
<figcaption aria-hidden="true">image-20241011145909441</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145913975.png" alt="image-20241011145913975">
<figcaption aria-hidden="true">image-20241011145913975</figcaption>
</figure>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145923473.png" alt="image-20241011145923473">
<figcaption aria-hidden="true">image-20241011145923473</figcaption>
</figure>
<h3 id="注意">注意</h3>
<p>正如之前所说，自动绑定修改的是当前函数修饰的对象所用到的值，对象本身并没改变，因此刷新当前页面就恢复了</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011145949334.png" alt="image-20241011145949334">
<figcaption aria-hidden="true">image-20241011145949334</figcaption>
</figure>
<h2 id="修复">修复</h2>
<ol type="1">
<li></li>
</ol>
<p>使用 DTO 模式，在对象中放入能修改的字段，在后续进行更新</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011150017124.png" alt="image-20241011150017124">
<figcaption aria-hidden="true">image-20241011150017124</figcaption>
</figure>
<p>这样，就不能直接自动绑定到user的balance等字段了。</p>
<ol start="2" type="1">
<li></li>
</ol>
<p><span class="citation" data-cites="InitBinder">@InitBinder</span>：通过 WebDataBinder 的
setDisallowedFields() 方法指定哪些字段不被绑定。</p>
<p>如源程序中定义了如下</p>
<figure>
<img src="/2024/10/10/java%E5%AE%A1%E8%AE%A11/imgs/java%E5%AE%A1%E8%AE%A11/image-20241011150046952.png" alt="image-20241011150046952">
<figcaption aria-hidden="true">image-20241011150046952</figcaption>
</figure>
<p>但是由于大小写原因，使用Balance也能进行绑定。。</p>
<p>虽然也有办法检测传输来的参数然后进行手动验证，但感觉还是dto方式好点，直接禁了</p>
]]></content>
      <tags>
        <tag>java代码审计</tag>
      </tags>
  </entry>
  <entry>
    <title>jolokia exploit</title>
    <url>/2024/11/08/jolokia-exploit1/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="fafd55bc4690f15e06bf8f28774191cb326dffd87adc69a5c2b032562d583e51">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>java审计</category>
      </categories>
      <tags>
        <tag>java审计</tag>
        <tag>jolokia</tag>
      </tags>
  </entry>
  <entry>
    <title>CVE-2024-5932/CVE-2024-8353 wordpress givewp漏洞分析</title>
    <url>/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>CVE-2024-8353相对于CVE-2024-5932，多了部分绕过的样子，核心利用还是不变的。这个漏洞是个人开发工具的时候作为样本测试分析的，因此没有关注里面的绕过内容。为什么用CVE-2024-8353呢，因为在使用CVE-2024-5932的poc的时候发现无法成功，即使我版本啥的都没问题也还是没成，后续用CVE-2024-8353能够触发payload，因此用它来分析了。</p>
<p>那么正式开始正文，总的来说，在默认设置下，捐款的时候新用户（新的邮箱）会收到一份邮件，该邮件模板中存在{name}，会到达我们的漏洞函数中触发toString方法从而完成pop链</p>
<span id="more"></span>
<h2 id="业务流程分析">业务流程分析</h2>
<ol type="1">
<li>创建 捐款表单</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094406119.png" alt="image-20250102094406119">
<figcaption aria-hidden="true">image-20250102094406119</figcaption>
</figure>
<ol start="2" type="1">
<li>虽然各种类型表单都行，我们payload中修改表单id号都能用，但是第二个classic
form发送到请求包才是我们要的格式</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094424113.png" alt="image-20250102094424113">
<figcaption aria-hidden="true">image-20250102094424113</figcaption>
</figure>
<ol start="3" type="1">
<li>默认设置就行，添加名字，然后直接发布</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094559148.png" alt="image-20250102094559148">
<figcaption aria-hidden="true">image-20250102094559148</figcaption>
</figure>
<ol start="4" type="1">
<li>回到all forms页面，我们能发现已经有我们刚创建的表单了</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094613022.png" alt="image-20250102094613022">
<figcaption aria-hidden="true">image-20250102094613022</figcaption>
</figure>
<ol start="5" type="1">
<li>创建新的文章，添加该表单</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094630672.png" alt="image-20250102094630672">
<figcaption aria-hidden="true">image-20250102094630672</figcaption>
</figure>
<p>成功</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094643317.png" alt="image-20250102094643317">
<figcaption aria-hidden="true">image-20250102094643317</figcaption>
</figure>
<p>6.捐100，填好以下内容，然后发送请求包（名字不能有数字）</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094721039.png" alt="image-20250102094721039">
<figcaption aria-hidden="true">image-20250102094721039</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094727371.png" alt="image-20250102094727371">
<figcaption aria-hidden="true">image-20250102094727371</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094751845.png" alt="image-20250102094751845">
<figcaption aria-hidden="true">image-20250102094751845</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094849198.png" alt="image-20250102094849198">
<figcaption aria-hidden="true">image-20250102094849198</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094816655.png" alt="image-20250102094816655">
<figcaption aria-hidden="true">image-20250102094816655</figcaption>
</figure>
<p>所以我们入口函数从 give_process_donation_form() 开始</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094830367.png" alt="image-20250102094830367">
<figcaption aria-hidden="true">image-20250102094830367</figcaption>
</figure>
<ol start="7" type="1">
<li>在进入正式的代码跟踪前，先说明到底发生了什么。</li>
</ol>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102094911931.png" alt="image-20250102094911931">
<figcaption aria-hidden="true">image-20250102094911931</figcaption>
</figure>
<p>根据描述，发送给新的，使用offline donation的用户（新邮箱）。</p>
<p>除此外我们也能在form表单中的设置中查看到，可以选择全局定义的邮件，就是上图。或是自定义的邮件。</p>
<p>默认使用全局。</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095134290.png" alt="image-20250102095134290">
<figcaption aria-hidden="true">image-20250102095134290</figcaption>
</figure>
<p>点进去该New Offline
Donation发现其邮件内容是使用了{name}标签，在处理该标签的函数中存在我们的漏洞点，触发title属性的toString方法</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095152941.png" alt="image-20250102095152941">
<figcaption aria-hidden="true">image-20250102095152941</figcaption>
</figure>
<h2 id="代码调用链">代码调用链</h2>
<p>多图杀猫警告。</p>
<p>首先入口点</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095259358.png" alt="image-20250102095259358">
<figcaption aria-hidden="true">image-20250102095259358</figcaption>
</figure>
<p>经过以下一系列变换到达我们函数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095336395.png" alt="image-20250102095336395">
<figcaption aria-hidden="true">image-20250102095336395</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095345847.png" alt="image-20250102095345847">
<figcaption aria-hidden="true">image-20250102095345847</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095351067.png" alt="image-20250102095351067">
<figcaption aria-hidden="true">image-20250102095351067</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095357497.png" alt="image-20250102095357497">
<figcaption aria-hidden="true">image-20250102095357497</figcaption>
</figure>
<p>接下来除了必要说明，会一直贴图顺序调用</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095450346.png" alt="image-20250102095450346">
<figcaption aria-hidden="true">image-20250102095450346</figcaption>
</figure>
<p>通过debug直到进入了give_gateway_offline，这大概是请求包这里自定义的（还记得我们之前业务发送请求包的时候选择的支付方式是offline吗）</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095553604.png" alt="image-20250102095553604">
<figcaption aria-hidden="true">image-20250102095553604</figcaption>
</figure>
<p>查询对应函数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095602524.png" alt="image-20250102095602524">
<figcaption aria-hidden="true">image-20250102095602524</figcaption>
</figure>
<p>进入对应函数的42到44行，发现是匿名函数的调用</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095612042.png" alt="image-20250102095612042">
<figcaption aria-hidden="true">image-20250102095612042</figcaption>
</figure>
<p>我们从这里继续</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095845999.png" alt="image-20250102095845999">
<figcaption aria-hidden="true">image-20250102095845999</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102095851204.png" alt="image-20250102095851204">
<figcaption aria-hidden="true">image-20250102095851204</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100127544.png" alt="image-20250102100127544">
<figcaption aria-hidden="true">image-20250102100127544</figcaption>
</figure>
<p>其内部实际上也还是执行do_action，如下图</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100144501.png" alt="image-20250102100144501">
<figcaption aria-hidden="true">image-20250102100144501</figcaption>
</figure>
<p>查询对应函数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100215578.png" alt="image-20250102100215578">
<figcaption aria-hidden="true">image-20250102100215578</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100227867.png" alt="image-20250102100227867">
<figcaption aria-hidden="true">image-20250102100227867</figcaption>
</figure>
<p>在 PHP 中，<code>__invoke</code>
是一个魔术方法，当试图将对象当作函数来调用时会被触发。</p>
<p>上面在new了对象后，调用了该对象的__invoke函数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100352906.png" alt="image-20250102100352906">
<figcaption aria-hidden="true">image-20250102100352906</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100358458.png" alt="image-20250102100358458">
<figcaption aria-hidden="true">image-20250102100358458</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100409034.png" alt="image-20250102100409034">
<figcaption aria-hidden="true">image-20250102100409034</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100414516.png" alt="image-20250102100414516">
<figcaption aria-hidden="true">image-20250102100414516</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100430425.png" alt="image-20250102100430425">
<figcaption aria-hidden="true">image-20250102100430425</figcaption>
</figure>
<p>我们可以观察上图传入该函数的content内容</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100935049.png" alt="image-20250102100935049">
<figcaption aria-hidden="true">image-20250102100935049</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100444042.png" alt="image-20250102100444042">
<figcaption aria-hidden="true">image-20250102100444042</figcaption>
</figure>
<p>preg_replace_callback是php的方法，简单说就是查询类似{name}
{siteurl}等大括号tag内容，然后执行本对象的do_tag方法</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102100521420.png" alt="image-20250102100521420">
<figcaption aria-hidden="true">image-20250102100521420</figcaption>
</figure>
<p>这部分就是对content中每个tag如{site_url}进行识别，然后分配到对应的执行函数中。</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101000579.png" alt="image-20250102101000579">
<figcaption aria-hidden="true">image-20250102101000579</figcaption>
</figure>
<p>通过{name} 的tag，我们进入下一个函数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101040714.png" alt="image-20250102101040714">
<figcaption aria-hidden="true">image-20250102101040714</figcaption>
</figure>
<p>可以查看该参数内容，title就是我们注入点</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101049069.png" alt="image-20250102101049069">
<figcaption aria-hidden="true">image-20250102101049069</figcaption>
</figure>
<p>如果进去give_get_payment_meta_user_info()内部的话，会发现它实际上是根据payment_id提取数据，然后进行了反序列化(maybe_unserialize是wordpress的检查并反序列化函数)。</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/1.png" alt="file://C:/Users/admin/AppData/Local/Temp/.B333Z2/1.png">
<figcaption aria-hidden="true">file://C:/Users/admin/AppData/Local/Temp/.B333Z2/1.png</figcaption>
</figure>
<p>回到刚才继续进入到give_get_email_names中</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101242437.png" alt="image-20250102101242437">
<figcaption aria-hidden="true">image-20250102101242437</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101247633.png" alt="image-20250102101247633">
<figcaption aria-hidden="true">image-20250102101247633</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101253721.png" alt="image-20250102101253721">
<figcaption aria-hidden="true">image-20250102101253721</figcaption>
</figure>
<p>最终到达上图的漏洞点，title的反序列化对象能触发tostring方法。</p>
<h2 id="利用链">利用链</h2>
<p>说实话让我构建我肯定构建不出来，而且这也不是我的重点，所以我就把分析过程简单放在下面了。</p>
<p>payload如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\O:19:&quot;Stripe\\\\StripeObject&quot;:1:&#123;s:10:&quot;\0*\0_values&quot;;a:1:&#123;s:3:&quot;foo&quot;;O:62:&quot;Give\\\\PaymentGateways\\\\DataTransferObjects\\\\GiveInsertPaymentData&quot;:1:&#123;s:8:&quot;userInfo&quot;;a:1:&#123;s:7:&quot;address&quot;;O:4:&quot;Give&quot;:1:&#123;s:12:&quot;\0*\0container&quot;;O:33:&quot;Give\\\\Vendors\\\\Faker\\\\ValidGenerator&quot;:3:&#123;s:12:&quot;\0*\0validator&quot;;s:10:&quot;shell_exec&quot;;s:12:&quot;\0*\0generator&quot;;O:34:&quot;Give\\\\Onboarding\\\\SettingsRepository&quot;:1:&#123;s:11:&quot;\0*\0settings&quot;;a:1:&#123;s:8:&quot;address1&quot;;s:4:&quot;calc&quot;;&#125;&#125;s:13:&quot;\0*\0maxRetries&quot;;i:10;&#125;&#125;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102102426340.png" alt="image-20250102102426340">
<figcaption aria-hidden="true">image-20250102102426340</figcaption>
</figure>
<p>直接从StripeObject类的__tostring方法开始</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101615455.png" alt="image-20250102101615455">
<figcaption aria-hidden="true">image-20250102101615455</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101629885.png" alt="image-20250102101629885">
<figcaption aria-hidden="true">image-20250102101629885</figcaption>
</figure>
<p>获取字典_values中每个key对应的value</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102101643310.png" alt="image-20250102101643310">
<figcaption aria-hidden="true">image-20250102101643310</figcaption>
</figure>
<p>根据payload内容直到，我们是只有一个userInfo属性的GiveInsertPaymentData对象</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103103015.png" alt="image-20250102103103015">
<figcaption aria-hidden="true">image-20250102103103015</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103011266.png" alt="image-20250102103011266">
<figcaption aria-hidden="true">image-20250102103011266</figcaption>
</figure>
<p>继续</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103134879.png" alt="image-20250102103134879">
<figcaption aria-hidden="true">image-20250102103134879</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103143356.png" alt="image-20250102103143356">
<figcaption aria-hidden="true">image-20250102103143356</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103153730.png" alt="image-20250102103153730">
<figcaption aria-hidden="true">image-20250102103153730</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103201048.png" alt="image-20250102103201048">
<figcaption aria-hidden="true">image-20250102103201048</figcaption>
</figure>
<p>userInfo['address']
中的内容是一个类对象Give，上图中赋值语句触发了Give对象的_get方法</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103637000.png" alt="image-20250102103637000">
<figcaption aria-hidden="true">image-20250102103637000</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103648780.png" alt="image-20250102103648780">
<figcaption aria-hidden="true">image-20250102103648780</figcaption>
</figure>
<p>上图中触发container对象ValidGenerator的get方法，但实际上并不存在，因此触发该对象的__call方法，即我们最终调用函数的方法</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103808056.png" alt="image-20250102103808056">
<figcaption aria-hidden="true">image-20250102103808056</figcaption>
</figure>
<p>我们可以观察方法传入了什么参数</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103846223.png" alt="image-20250102103846223">
<figcaption aria-hidden="true">image-20250102103846223</figcaption>
</figure>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102103850477.png" alt="image-20250102103850477">
<figcaption aria-hidden="true">image-20250102103850477</figcaption>
</figure>
<p>然后再看看执行参数分别是什么内容</p>
<p><img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104403036.png" alt="image-20250102104403036"><img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104417980.png" alt="image-20250102104417980"><img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104441820.png" alt="image-20250102104441820"></p>
<p>payload中我们知道generator中的对象是
SettingsRepository，他确实有个对象get</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104703241.png" alt="image-20250102104703241">
<figcaption aria-hidden="true">image-20250102104703241</figcaption>
</figure>
<p>简单说，执行settingsRepository的get()函数，获取了address1中的字符串calc</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104934451.png" alt="image-20250102104934451">
<figcaption aria-hidden="true">image-20250102104934451</figcaption>
</figure>
<p>最后根据maxRetries，调用了maxRetries次数的最终执行
shell_exec('calc')</p>
<figure>
<img src="/2025/01/02/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/imgs/CVE-2024-5932-CVE-2024-8353-wordpress%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90/image-20250102104950996.png" alt="image-20250102104950996">
<figcaption aria-hidden="true">image-20250102104950996</figcaption>
</figure>
<h2 id="参考链接">参考链接</h2>
<ol type="1">
<li>https://xz.aliyun.com/t/15699?time__1311=GqjxnieDqYqmqGNPeeqBK0QG8W7vTwh3EbD</li>
<li>https://github.com/EQSTLab/CVE-2024-8353/tree/main</li>
</ol>
]]></content>
      <categories>
        <category>php审计</category>
      </categories>
      <tags>
        <tag>php审计</tag>
        <tag>wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title>jolokia rce vulnerability</title>
    <url>/2024/12/03/jolokia-rce-vulnerability/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="0d2f02829670831de636045cef3a6c764cc04b9a1c2c6083fbd82163e5887c42">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>java审计</category>
      </categories>
      <tags>
        <tag>java审计</tag>
        <tag>jolokia</tag>
      </tags>
  </entry>
  <entry>
    <title>test_my_site</title>
    <url>/2024/10/07/test-my-site/</url>
    <content><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <script id="hbeData" type="hbeData" data-hmacdigest="ccf6419c692ab8751ffa6a81e5b8762e7808dd79fc9ac2b9c56c92a408a832ba">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</script>
  <div class="hbe hbe-content">
    <div class="hbe hbe-input hbe-input-default">
      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">
      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">
        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>
      </label>
    </div>
  </div>
</div>
<script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>jQuery_1_test</tag>
        <tag>jQuery_1_test2 3</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.10 多层感知机的简洁实现</title>
    <url>/2025/04/02/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-10/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="定义模型">1. 定义模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>和softmax回归章节模型唯一的不同在于，我们多加了一个全连接层作为隐藏层。它的隐藏单元个数为256，并使用ReLU函数作为激活函数。</p>
<h2 id="读取数据并训练模型">2. 读取数据并训练模型</h2>
<p>关于load_data_fashion_mnist和train_ch3请查看之前章节的介绍，或查看官方源码理解，后续不再将其定义贴出。避免占太多空间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.11 模型选择、欠拟合和过拟合</title>
    <url>/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="模型选择">1. 模型选择</h2>
<p>在机器学习中，通常需要<strong>评估若干候选模型的表现并从中选择模型</strong>。这一过程称为<strong>模型选择</strong>（model
selection）。</p>
<p>以多层感知机为例，我们可以<strong>选择隐藏层的个数</strong>，以及<strong>每个隐藏层中隐藏单元个数</strong>和<strong>激活函数</strong>。</p>
<h3 id="验证数据集">1.1 验证数据集</h3>
<p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。</p>
<p>然而在实际应用中，由于数据不容易获取，测试数据极少只使用一次就丢弃。因此，实践中验证数据集和测试数据集的界限可能比较模糊。</p>
<h3 id="k折交叉验证">1.2 K折交叉验证</h3>
<p>K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集。</p>
<p>我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K−1个子数据集来训练模型。</p>
<p>最后，我们对这K次训练误差和验证误差分别求平均。</p>
<h2 id="欠拟合和过拟合">2. 欠拟合和过拟合</h2>
<h3 id="模型复杂度">2.1 模型复杂度</h3>
<p>为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征x和对应的标量标签y组成的训练数据集，多项式函数拟合的目标是找一个K阶多项式函数来近似y。
<span class="math display">\[
\hat{y}=b+\sum ^{K}_{k=1} {x^kw_k}
\]</span> 在上式中，<span class="math inline">\(w_k\)</span>是模型的权重参数，<span class="math inline">\(b\)</span>是偏差参数。</p>
<p>一阶多项式函数拟合又叫线性函数拟合。</p>
<p>直观感受举例k=4的话，则4阶多项式如下 <span class="math display">\[
\hat{y}=b+x^1w_1+x^2w_2+x^3w_3+x^4w_4
\]</span></p>
<p>因为高阶多项式函数模型参数更多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高。因此，<strong>高阶多项式函数
比 低阶多项式函数
更容易在相同的训练数据集上得到更低的训练误差</strong>。</p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250407161159606.png" alt="image-20250407161159606" style="zoom: 50%;"></p>
<h2 id="训练数据集大小">2.2 训练数据集大小</h2>
<p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。</p>
<p>一般来说，如果训练数据集中
<strong>样本数过少</strong>，特别是比模型参数数量（按元素计）更少时，<strong>过拟合更容易发生</strong>。</p>
<p>此外，泛化误差不会随训练数据集里样本数量增加而增大。</p>
<p>因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>
<h2 id="多项式函数拟合实验">3. 多项式函数拟合实验</h2>
<p>为了理解模型复杂度和训练数据集大小对欠拟合和过拟合的影响，下面我们以多项式函数拟合为例来实验。</p>
<h3 id="生成数据集">3.1 生成数据集</h3>
<p>我们将生成一个人工数据集。在训练数据集和测试数据集中，给定样本特征x，我们使用如下的三阶多项式函数来生成该样本的标签：
<span class="math display">\[
y=1.2x−3.4x^2 +5.6x^3 +5+ϵ
\]</span>
其中噪声项ϵ服从均值为0、标准差为0.01的正态分布。训练数据集和测试数据集的样本数都设为100。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_train, n_test, true_w, true_b = <span class="number">100</span>, <span class="number">100</span>, [<span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>], <span class="number">5</span></span><br><span class="line">features = torch.randn((n_train + n_test, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">poly_features = torch.cat((features, torch.<span class="built_in">pow</span>(features, <span class="number">2</span>), torch.<span class="built_in">pow</span>(features, <span class="number">3</span>)), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">labels = (true_w[<span class="number">0</span>] * poly_features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * poly_features[:, <span class="number">1</span>]</span><br><span class="line">          + true_w[<span class="number">2</span>] * poly_features[:, <span class="number">2</span>] + true_b)</span><br><span class="line"></span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>torch.randn 生成(200,1)的随机数</p>
<p>torch.pow(features<strong>,</strong> 2) 与
torch.pow(features<strong>,</strong> 3)
分别生成features中每个数2次幂与3次幂构成的tensor</p>
<p>torch.cat中dim为1，即从列方向，将3个tensor拼接起来。即生成列表tensor
<span class="math inline">\([x,x^2,x^3]\)</span></p>
<p>下图举例</p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408144904225.png" alt="image-20250408144904225" style="zoom:50%;"></p>
<h3 id="定义训练和测试模型">3.2 定义、训练和测试模型</h3>
<p>我们先定义作图函数 semilogy （画图具体实现自行查看代码）。</p>
<p>和线性回归一样，多项式函数拟合也使用平方损失函数。因为我们将尝试使用不同复杂度的模型来拟合生成的数据集，所以我们把模型定义部分放在
fit_and_plot 函数中。</p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408150427641.png"></p>
<h3 id="三阶多项式函数拟合正常">3.3 三阶多项式函数拟合（正常）</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408150637587.png" style="zoom: 67%;"></p>
<h3 id="线性函数拟合欠拟合">3.4 线性函数拟合（欠拟合）</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408150834893.png" style="zoom:50%;"></p>
<p>为了不让大伙迷糊3.3和3.4有什么区别，以下进行简单说明。</p>
<p>根据之前的定义，features只有一列，即x。poly_features中除了x，还有x²，x三次方。而我们函数中的网络定义的权重参数，则是根据输入的列数进行确定的</p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408151007959.png" style="zoom:67%;"></p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408151119227.png" style="zoom: 50%;"></p>
<p>尝试将原本的3权重换成4权重，添加多一维无意义的组，label还是原来的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_features2 = torch.cat((features, torch.<span class="built_in">pow</span>(features, <span class="number">2</span>), torch.<span class="built_in">pow</span>(features, <span class="number">3</span>),torch.<span class="built_in">pow</span>(features, <span class="number">4</span>)), <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408153114209.png" style="zoom: 33%;"></p>
<p>发现虽然没原来顺滑，但loss也会逐渐变小，原因是它包含了必要的前3维数据，可以认为经过训练后网络将第四维新加权重置为接近0的值。</p>
<p>而如果仅保留部分维度的数据，即上面的仅保留了1维的线性网络，或者自行构建仅保留2维（下图）网络计算，其loss都是降不下来的。</p>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408153707239.png" alt="image-20250408153707239" style="zoom:67%;"></p>
<h3 id="训练样本不足过拟合">3.5 训练样本不足（过拟合）</h3>
<p>事实上，即便使用与数据生成模型同阶的三阶多项式函数模型，如果训练样本不足，该模型依然容易过拟合。</p>
<p>让我们只使用两个样本来训练模型。显然，训练样本过少了，甚至少于模型参数的数量。这使模型显得过于复杂，以至于容易被训练数据中的噪声影响。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fit_and_plot(poly_features[<span class="number">0</span>:<span class="number">2</span>, :], poly_features[n_train:, :], labels[<span class="number">0</span>:<span class="number">2</span>], labels[n_train:])</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-11/image-20250408151302418.png" style="zoom:67%;"></p>
<h2 id="小结">4 小结</h2>
<p>本章节挺适合做个小结进行说明的。</p>
<p>我们初始的输入其实只有x，然后我们目标label是
3阶的多项式的一个结果。</p>
<p>如果我们隐藏层参数越多，我们能获取的x的次方就越多，即我们实验中的输入poly_features，这里我们将隐藏层拟合这一步骤直接用手动计算poly_features代替了。</p>
<p>简单说如果隐藏层参数不够，则仅拟合出 <span class="math inline">\(x\)</span> 或仅拟合出 <span class="math inline">\(x,x^2\)</span>
然后传递给后面的层（如实验函数中的net =
torch.nn.Linear(train_features.shape[-1],
1)）进行计算，那么无论如何，其loss都是降不下来的。</p>
<p>反过来说，如果隐藏层参数过多，不仅拟合出 <span class="math inline">\(x,x^2,x^3\)</span> 还有其他一堆 <span class="math inline">\(x^4,x^5,x^6\)</span>
等，那也是没关系的，因为已经包含了必要的 <span class="math inline">\(x,x^2,x^3\)</span>
，其他多余的参数会在后续进行优化，会被某一层的网络权重置为0。</p>
<p>因此能简单判断，如果loss一直很高，欠拟合的话参数加多点，过拟合的话样本加多点。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.12 权重衰减</title>
    <url>/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>虽然增大训练数据集可能会减轻过拟合，但是获取额外的训练数据往往代价高昂。</p>
<p>本节介绍应对过拟合问题的常用方法：权重衰减（weight decay）。</p>
<h2 id="方法">1. 方法</h2>
<p>权重衰减等价于 L2范数正则化（regularization）</p>
<p>正则化通过<strong>为模型损失函数添加惩罚项</strong>
使学出的模型参数值较小，是应对过拟合的常用手段。L2范数正则化在模型原损失函数基础上添加L2范数惩罚项，<strong>从而得到训练所需要最小化的函数</strong>。</p>
<p>L2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。</p>
<p>以3.1节（线性回归）中的线性回归损失函数为例，</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411161518508.png" style="zoom: 67%;"></p>
<p>其中 <span class="math inline">\(w_1,w_2\)</span>
是权重参数，b是偏差参数，样本 <span class="math inline">\(i\)</span>
的输入为 <span class="math inline">\(x^{(i)}_1、
x^{(i)}_2\)</span>，标签为 <span class="math inline">\(y^{(i)}\)</span>，样本数为 <span class="math inline">\(n\)</span>。将权重参数用向量 <span class="math inline">\(w=[w_1,w_2]\)</span> 表示，带有L2
范数惩罚项的新损失函数为</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411161809353.png" alt="image-20250411161809353" style="zoom:67%;"></p>
<p>其中超参数λ&gt;0.</p>
<p>上式中L2范数平方 <span class="math inline">\(\left | \left | {w}
\right | \right |\)</span> 展开后得到 <span class="math inline">\(w^2_1+w^2_2\)</span></p>
<p>感觉下图gpt解释的公式更亲民点</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411162117892.png"></p>
<p>当权重参数均为0时，惩罚项最小。当λ较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。</p>
<p>当λ设为0时，惩罚项完全不起作用。</p>
<p>有了L2范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重
<span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span> 的迭代方式更改为</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411162334337.png" alt="image-20250411162334337" style="zoom:67%;"></p>
<p>其求导可以参考下图，对 <span class="math inline">\(\sum
{w^2_i}\)</span> 中某特定 <span class="math inline">\(w_i\)</span>
的求导（先别管红框）</p>
<figure>
<img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411162423404.png" alt="image-20250411162423404">
<figcaption aria-hidden="true">image-20250411162423404</figcaption>
</figure>
<p>这意味着当前 <span class="math inline">\(w_i\)</span>
越大的话，他更新幅度越大，因此 <span class="math inline">\(w_i\)</span>
更新后越趋近于0</p>
<p>使得所有参数w之间的差距不会过大，每个都有更新的机会。因此很适合训练初期，当初期有对应特征之后，可以减少λ的大小。</p>
<p>有以下优点：</p>
<ol type="1">
<li>提高数值稳定性</li>
<li>L2
正则化鼓励权重较小，从而避免梯度过大或梯度爆炸的问题，在一定程度上
提高了训练的数值稳定性。</li>
<li>它有助于防止过拟合、平滑优化、提高数值稳定性。</li>
</ol>
<h2 id="高维线性回归实验">2. 高维线性回归实验</h2>
<p>下面，我们以高维线性回归为例来引入一个过拟合问题，并使用权重衰减来应对过拟合。设数据样本特征的维度为p。</p>
<p>对于训练数据集和测试数据集中特征为 <span class="math inline">\(x_1,x_2,…,x_p\)</span>
的任一样本，我们使用如下的线性函数来生成该样本的标签： <span class="math display">\[
y = 0.05+ \sum ^{p}_{i=1} {0.01x_i}+ϵ
\]</span>
其中噪声项ϵ，ϵ服从均值为0、标准差为0.01的正态分布。为了较容易地观察过拟合，我们考虑高维线性回归问题，如设维度p=200。</p>
<p>同时，我们特意把训练数据集的样本数设低，如20。</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411163943383.png"></p>
<h2 id="从零开始实现">3. 从零开始实现</h2>
<h3 id="初始化模型参数">3.1 初始化模型参数</h3>
<p>定义随机初始化模型参数的函数。该函数为每个参数都附上梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.randn((num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure>
<h3 id="定义l2范数惩罚项">3.2 定义L2范数惩罚项</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).<span class="built_in">sum</span>() / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="定义训练和测试">3.3 定义训练和测试</h3>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411164922637.png"></p>
<p>其中net和loss的函数如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="comment"># 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2</span></span><br><span class="line">    <span class="keyword">return</span> ((y_hat - y.view(y_hat.size())) ** <span class="number">2</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>当我们设置
lambd=0，<code>fit_and_plot(lambd=0)</code>，即不使用权重衰减，效果如下</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411165125539.png" style="zoom:67%;"></p>
<p>当我们设置 lambd=3，<code>fit_and_plot(lambd=3)</code>，效果如下</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411165157572.png" style="zoom:67%;"></p>
<h2 id="简洁实现">4. 简洁实现</h2>
<p>这里我们直接在构造优化器实例时通过weight_decay参数来指定权重衰减超参数。</p>
<p>（每个参数如果使用了参数衰减，那么更新的时候也只是
减去自身权重相关的一个数，不需要像之前那样在loss那里将所有权重加起来。可以像下图中在优化处进行指定参数的权重衰减。）</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411165943029.png"></p>
<p>设置
lambd=0，<code>fit_and_plot_pytorch(0)</code>，即不使用权重衰减，效果如下</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411170704613.png" style="zoom:67%;"></p>
<p>设置 lambd=3，<code>fit_and_plot_pytorch(3)</code>，效果如下</p>
<p><img src="/2025/04/08/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-12/image-20250411170658960.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.13 丢弃法</title>
    <url>/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="方法">1. 方法</h2>
<p>3.8节（多层感知机）的图3.3描述了一个单隐藏层的多层感知机。</p>
<p>其中输入个数为4，隐藏单元个数为5，且隐藏单元hi（i=1,…,5）的计算表达式为</p>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416152722779.png" style="zoom:67%;">
<span class="math display">\[
h_i = ϕ(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)
\]</span>
当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。</p>
<p>设丢弃概率为p，那么有p的概率 <span class="math inline">\(h_i\)</span>
会被清零，有1−p的概率会除以1−p做拉伸。</p>
<p>即使用丢弃法后，新的隐藏单元 <span class="math inline">\(h_i&#39;\)</span> 的结果如下 <span class="math display">\[
h&#39;_i =
\left\{
\begin{array}{ll}
0 &amp; \text{概率 } p \\
\frac {h_i} {1-p} &amp; \text{概率 } 1-p
\end{array}
\right.
\]</span> 丢弃概率p是丢弃法的超参数。</p>
<p>这里我们统一成 <span class="math display">\[
h&#39;_i=\frac {ξ_i} {1-p}
\]</span> 随机变量 <span class="math inline">\(ξ_i\)</span> 为 0 和 1
的概率分别为 p 和 1−p。</p>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416153309125.png"></p>
<p>由于 <span class="math inline">\(E(ξ_i)=1-p\)</span> ，因此 <span class="math display">\[
E(h&#39;_i) = \frac {E(ξ_i)} {1-p} h_i = h_i
\]</span> 即丢弃法不改变其输入的期望值。</p>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416153714434.png" style="zoom:80%;"></p>
<p>让我们对图3.3中的隐藏层使用丢弃法，一种可能的结果如图3.5所示，其中h2和h5被清零。这时输出值的计算不再依赖h2和h5，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。</p>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416154648510.png"></p>
<p>由于在训练中隐藏层神经元的丢弃是随机的，即h1,…,h5都有可能被清零，输出层的计算无法过度依赖h1,…,h5中的任一个，从而<strong>在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。</strong></p>
<h2 id="从零开始实现">2. 从零开始实现</h2>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416160406611.png"></p>
<p>我们运行几个例子来测试一下dropout函数。其中丢弃概率分别为0、0.5和1。</p>
<h3 id="定义模型参数">2.1 定义模型参数</h3>
<p>实验中，我们依然使用3.6节（softmax回归的从零开始实现）中介绍的Fashion-MNIST数据集。我们将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256。</p>
<p>输入784，输出10，两个隐藏层都为256节点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens1, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.zeros(num_hiddens2, requires_grad=<span class="literal">True</span>)</span><br><span class="line">W3 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.<span class="built_in">float</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b3 = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>
<h3 id="定义模型">2.2 定义模型</h3>
<p>下面定义的模型将全连接层和激活函数ReLU串起来，并<strong>对每个激活函数的输出使用丢弃法</strong>。</p>
<p>我们可以分别设置各个层的丢弃概率。通常的<strong>建议是把靠近输入层的丢弃概率设得小一点</strong>。</p>
<p>在这个实验中，我们把第一个隐藏层的丢弃概率设为0.2，把第二个隐藏层的丢弃概率设为0.5。我们可以通过参数is_training来判断运行模式为训练还是测试，并只需在训练模式下使用丢弃法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">drop_prob1, drop_prob2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X, is_training=<span class="literal">True</span></span>):</span><br><span class="line">    X = X.view(-<span class="number">1</span>, num_inputs)</span><br><span class="line">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:  <span class="comment"># 只在训练模型时使用丢弃法</span></span><br><span class="line">        H1 = dropout(H1, drop_prob1)  <span class="comment"># 在第一层全连接后添加丢弃层</span></span><br><span class="line">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        H2 = dropout(H2, drop_prob2)  <span class="comment"># 在第二层全连接后添加丢弃层</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H2, W3) + b3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>我们在对模型评估的时候不应该进行丢弃，所以我们修改一下d2lzh_pytorch中的evaluate_accuracy函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">            net.<span class="built_in">eval</span>() <span class="comment"># 评估模式, 这会关闭dropout</span></span><br><span class="line">            acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">            net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 自定义的模型</span></span><br><span class="line">            <span class="keyword">if</span>(<span class="string">&#x27;is_training&#x27;</span> <span class="keyword">in</span> net.__code__.co_varnames): <span class="comment"># 如果有is_training这个参数</span></span><br><span class="line">                <span class="comment"># 将is_training设置成False</span></span><br><span class="line">                acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="训练和测试模型">2.3 训练和测试模型</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3_2</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class="line"><span class="params">              params=<span class="literal">None</span>, lr=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy2(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line">        </span><br><span class="line">num_epochs, lr, batch_size = <span class="number">5</span>, <span class="number">100.0</span>, <span class="number">256</span></span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3_2(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416164320407.png" alt style="zoom:67%;"></p>
<h2 id="简洁实现">3. 简洁实现</h2>
<p>在训练模型时，Dropout层将以指定的丢弃概率随机丢弃上一层的输出元素；</p>
<p>在测试模型时（即model.eval()后，如下图），Dropout层并不发挥作用。</p>
<figure>
<img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416165053106.png" alt="image-20250416165053106">
<figcaption aria-hidden="true">image-20250416165053106</figcaption>
</figure>
<p>在PyTorch中，我们只需要在全连接层后添加<strong>Dropout层</strong>并指定丢弃概率。</p>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416164839683.png" style="zoom:67%;"></p>
<p>下面训练并测试模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">d2l.train_ch3_2(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-13/image-20250416170218802.png" style="zoom:67%;"></p>
<h2 id="小结">小结</h2>
<p>个人理解，丢弃法怎么说呢，我原本理解是一个网络通过丢弃法内部产生多个网络，但实际上使用的时候还是最终网络，因此相互其实会有影响，一个网络单元也可能是另一个网络单元的一部分，因此不能直接理解成多网络。</p>
<p>感觉更好的理解是高影响的单元会有概率被去除，使得所有神经元都有机会更新，从而应对过拟合的问题。</p>
<p>除此外，直观的感受每个节点的能力更加综合，当某个其他节点不起作用时，自己也能顶上。
更加确保稳定性。</p>
<p>总有种俺寻思能行于是就上的感觉，因此丢弃法的使用仁者见仁智者见智，根据个人经验使用吧。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.14 正向传播、反向传播和计算图</title>
    <url>/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>前面几节里我们使用了小批量随机梯度下降的优化算法来训练模型。</p>
<p>在实现中，我们只提供了模型的正向传播（forward
propagation）的计算，即对输入计算模型输出，</p>
<p>然后通过autograd模块来调用系统自动生成的backward函数计算梯度。</p>
<p>本节我们将使用数学和计算图（computational
graph）两个方式来描述正向传播和反向传播。</p>
<h2 id="正向传播">1. 正向传播</h2>
<p>正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量（包括输出）。</p>
<p>假设输入是一个特征为 <span class="math inline">\(x\)</span>(d * 1)
且不考虑偏差项，那么中间变量 <span class="math display">\[
z = W^{(1)}x
\]</span> 其中 <span class="math inline">\(W^{(1)}\)</span>(m * d)
是隐藏层的权重参数。把中间变量 <span class="math inline">\(z\)</span> (m
* 1)输入按元素运算的激活函数 <span class="math inline">\(ϕ\)</span>
后，将得到向量长度为m的隐藏层变量 <span class="math display">\[
h=ϕ(z)
\]</span> 隐藏层变量h (m * 1) 也是一个中间变量。假设输出层参数只有权重
<span class="math inline">\(W^{(2)}\)</span>(q * m)
可以得到向量长度为q的输出层变量 <span class="math display">\[
o=W^{(2)}h
\]</span> <span class="math inline">\(o\)</span> 对 <span class="math inline">\(W^{(2)}\)</span> 的求导结果为 <span class="math inline">\(h\)</span> 的转置即 <span class="math inline">\(h^T\)</span> 并复制与 <span class="math inline">\(o\)</span> 相同n行。（即求导结果为n行的 <span class="math inline">\(h^T\)</span> ）</p>
<blockquote>
<p>假设 <span class="math inline">\(o_i\)</span> 为 o
的第i行元素（o为列向量，即大小为n*1）, <span class="math inline">\(W_i\)</span>
为W的第i行元素。每行元素有m列。变量h有m行 <span class="math display">\[
o_i=W_ih=w_{i1}h_1+w_{i2}h_2+...+w_{im}h_m
\]</span> 那么<span class="math inline">\(oi\)</span>对<span class="math inline">\(w_{i1}\)</span>的偏导为<span class="math inline">\(h_1\)</span>，对<span class="math inline">\(w_{i2}\)</span>的偏导为<span class="math inline">\(h_2\)</span> ，同理可得<span class="math inline">\(o_i\)</span>对<span class="math inline">\(W_i\)</span>的求导结果为横向量 {<span class="math inline">\(h_1,h_2,...,h_m\)</span>} 即<span class="math inline">\(h^T\)</span> 。</p>
<p>一共n个<span class="math inline">\(o_i\)</span> ，因此一共n行 <span class="math inline">\(h^T\)</span></p>
</blockquote>
<p>假设损失函数为ℓ，且样本标签为y，可以计算出<strong>单个数据样本</strong>的损失项（其实这里的o是一个元素，并非列向量）
<span class="math display">\[
L=ℓ(o,y)
\]</span> 根据L2范数正则化的定义，给定超参数λ，正则化项即</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423152925680.png" alt style="zoom: 50%;"></p>
<p>其中矩阵的Frobenius范数等价于将矩阵变平为向量后计算L2范数。最终，模型在给定的数据样本上带正则化的损失为
<span class="math display">\[
J = L + s
\]</span> 也就是之前说的</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423161947615.png"></p>
<p>我们将J称为有关给定数据样本的目标函数，并在以下的讨论中简称目标函数。</p>
<h2 id="正向传播的计算图">2. 正向传播的计算图</h2>
<p>我们通常绘制计算图来可视化运算符和变量在计算中的依赖关系。图3.6绘制了本节中样例模型正向传播的计算图，其中左下角是输入，右上角是输出。</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153121947.png" style="zoom:67%;"></p>
<p>接下来我会按照之前正向传播顺序逐个框出来所做的操作</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153201399.png" style="zoom:67%;"></p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153217504.png" style="zoom:67%;"></p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153236852.png" style="zoom:67%;"></p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153248338.png" style="zoom:67%;"></p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153310205.png" style="zoom:67%;"></p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153321633.png" style="zoom:67%;"></p>
<h2 id="反向传播">3. 反向传播</h2>
<p>反向传播指的是计算 神经网络参数梯度 的方法。</p>
<p>总的来说，反向传播依据微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。</p>
<p>对输入或输出X,Y,Z为任意形状张量的函数Y=f(X)和Z=g(Y)，通过链式法则，我们有</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153418769.png" style="zoom: 50%;"></p>
<p>其中prod运算符将根据两个输入的形状，在必要的操作（如转置和互换输入位置）后对两个输入做乘法。</p>
<p>回顾一下本节中样例模型，它的参数是 <span class="math inline">\(W^{(1)}\)</span> 和 <span class="math inline">\(W^{(2)}\)</span> ，因此反向传播的目标是计算 <span class="math inline">\(\frac{∂J}{∂W^{(1)}}\)</span> 和<span class="math inline">\(\frac{∂J}{∂W^{(2)}}\)</span>。asd</p>
<p>我们将应用链式法则依次计算各中间变量和参数的梯度，其计算次序与前向传播中相应中间变量的计算次序恰恰相反。首先，分别计算目标函数J=L+s有关损失项L和正则项s的梯度</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423153923998.png" style="zoom: 67%;"></p>
<p>其次，依据链式法则计算目标函数有关输出层变量o的梯度</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423160126237.png" alt style="zoom:50%;"></p>
<p>其结果指的是loss函数对o的求导.</p>
<p>接下来，计算正则项有关两个参数的梯度：</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423160324130.png" style="zoom:50%;"></p>
<p>现在，我们可以计算最靠近输出层的模型参数的梯度(之前说过o是单个样本的输出，因此导数是一个横向量)</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423160424730.png" style="zoom:80%;"></p>
<p>沿着输出层向隐藏层继续反向传播，隐藏层变量的梯度</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423160842073.png" style="zoom:67%;"></p>
<p>由于激活函数ϕ是按元素运算的，中间变量z的梯度的计算需要使用按元素乘法符⊙：</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423160945944.png" style="zoom:67%;"></p>
<p>最终，我们可以得到最靠近输入层的模型参数的梯度。依据链式法则，得到</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423161049506.png" style="zoom:80%;"></p>
<h2 id="训练深度学习模型">4. 训练深度学习模型</h2>
<p>一方面，<strong>正向传播</strong>的计算可能<strong>依赖于模型参数的当前值</strong>。而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。</p>
<p>例如，计算正则化项
<img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423161219587.png" alt style="zoom: 50%;">
依赖模型参数 <span class="math inline">\(W^{(1)}\)</span> 和 <span class="math inline">\(W^{(2)}\)</span>
的当前值，，而这些当前值是优化算法最近一次根据反向传播算出梯度后迭代得到的。</p>
<p>另一方面，<strong>反向传播</strong>的梯度计算可能<strong>依赖于各变量的当前值</strong>，而这些变量的当前值是通过正向传播计算得到的。</p>
<p>举例来说，参数梯度
<img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/1.png" style="zoom: 67%;">
的计算需要依赖隐藏层变量的当前值h。这个当前值是通过从输入层到输出层的正向传播计算并存储得到的。</p>
<p><img src="/2025/04/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-14/image-20250423161559816.png" alt="h由x获得的路径" style="zoom:50%;"></p>
<p>因此，在模型参数初始化完成后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。</p>
<p>既然我们<strong>在反向传播中使用了正向传播中计算得到的中间变量来避免重复计算</strong>，那么<strong>这个复用也导致正向传播结束后不能立即释放中间变量内存</strong>。这也是<strong>训练要比预测占用更多内存的一个重要原因</strong>。</p>
<p>另外需要指出的是，这些<strong>中间变量的个数大体上与网络层数线性相关</strong>，<strong>每个变量的大小跟批量大小和输入个数也是线性相关的</strong>，它们是导致<strong>较深的神经网络使用较大批量训练时更容易超内存的主要原因</strong>。</p>
<h2 id="小结">小结</h2>
<p>这部分要点内容在于，理解正向传播与反向传播是什么</p>
<p>正向传播就是通过网络求值。</p>
<p>反向传播就是通过网络求导。</p>
<p>正向传播相当于 y=wx, L=(y'-y)² （平方差）</p>
<p>通过 初始化 或 反向传播 获取w的值，计算出y</p>
<p>反向传播相当于 ∂L/∂y=-2y'+y , ∂y/∂w=x ⇒ ∂L/∂w=(-2y'+y)x ⇒
w=w-∂L/∂w</p>
<p>通过正向传播中的计算值y，获得w的偏导，更新w</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.15 数值稳定性和模型初始化</title>
    <url>/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-15/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="衰减和爆炸">1. 衰减和爆炸</h2>
<p>深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。</p>
<p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为<span class="math inline">\(L\)</span> 的多层感知机的第 <span class="math inline">\(l\)</span> 层 <span class="math inline">\(H^{(l)}\)</span>的权重参数为<span class="math inline">\(W^{(l)}\)</span>，输出层<span class="math inline">\(H^{(L)}\)</span> 的权重参数为<span class="math inline">\(W^{(L)}\)</span> 。</p>
<p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity
mapping）<span class="math inline">\(ϕ(x)=x\)</span> 。</p>
<p>给定输入<span class="math inline">\(X\)</span> ，多层感知机的第<span class="math inline">\(l\)</span> 层的输出 <span class="math inline">\(H^{(l)}=XW^{(1)}W^{(2)}…W^{(l)}\)</span>。此时，如果层数<span class="math inline">\(l\)</span> 较大，<span class="math inline">\(H^{(l)}\)</span> 的计算可能会出现衰减或爆炸。</p>
<p>举个例子，假设输入和所有层的权重参数都是标量，如权重参数为 <span class="math inline">\(0.2\)</span> 和 <span class="math inline">\(5\)</span>，多层感知机的第<span class="math inline">\(30\)</span>层输出为输入<span class="math inline">\(X\)</span> 分别与 <span class="math inline">\(0.2^{30}≈1×10^{−21}\)</span>（衰减）和 <span class="math inline">\(5^{30}≈9×10^{20}\)</span>（爆炸）的乘积。</p>
<p>随着内容的不断深入，我们会在后面的章节进一步介绍深度学习的数值稳定性问题以及解决方法。</p>
<h2 id="随机初始化模型参数">2. 随机初始化模型参数</h2>
<p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。为了方便解释，假设输出层只保留一个输出单元
<span class="math inline">\(o_1\)</span>,且隐藏层使用相同的激活函数。</p>
<p>如果<strong>将每个隐藏单元的参数都初始化为相等的值</strong>，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。<strong>在反向传播中，每个隐藏单元的参数梯度值相等</strong>。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。</p>
<p>在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p>
<h3 id="pytorch的默认随机初始化">2.1 PyTorch的默认随机初始化</h3>
<p>在3.3节（线性回归的简洁实现）中，我们 torch.nn.init.normal_()
使模型net的权重参数采用正态分布的随机初始化方式。</p>
<p>不过，PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考<a href="https://github.com/pytorch/pytorch/tree/master/torch/nn/modules">源代码</a>），因此一般不用我们考虑。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》3.2 线性回归从零开始实现</title>
    <url>/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">1前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>在了解了线性回归的背景知识之后，现在我们可以动手实现它了。尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用<strong>Tensor</strong>和<strong>autograd</strong>来实现一个线性回归的训练。</p>
<p>首先，导入本节中实验所需的包或模块，其中的matplotlib包可用于作图，且设置成嵌入显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h2 id="构造一个简单的人工训练数据集">2
构造一个简单的人工训练数据集</h2>
<p>设训练数据集<strong>样本数为1000</strong>，输入个数<strong>（特征数）为2</strong>。给定随机生成的批量样本特征X，使用线性回归模型真实权重
<span class="math inline">\(w=[2,−3.4]^⊤\)</span> 和偏差
b=4.2，以及一个随机噪声项 ϵ 来生成标签 <span class="math display">\[
y=Xw+b+ϵ
\]</span> <strong>torch.randn</strong> 是 PyTorch
中用于生成服从<strong>标准正态分布</strong>（均值为 0，标准差为
1）的随机张量的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># features是一个行数为num_examples，列数为num_inputs的张量，其每个内容都是从正态分布弄出来的随机数</span></span><br><span class="line">features = torch.randn(num_examples, num_inputs, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  features中每行进行=&gt;  第一列与第一个w相乘  +  第二列与第二个w相乘  + b</span></span><br><span class="line"><span class="comment">#  生成与features同行数的张量labels</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(labels))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(labels))</span><br><span class="line"></span><br><span class="line"><span class="comment"># labels中每个数也加上噪音</span></span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()),</span><br><span class="line">                       dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<h2 id="读取数据">3.读取数据</h2>
<p><img src="/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/image-20250228143122160.png"></p>
<p>上图中，j 通过torch.longTensor,获取了一个由标量组成的列表张量，如 [
1, 3, 4, 67, 22] 。</p>
<p>index_select(dim, index) 用于在<strong>指定维度 dim</strong>
上，根据索引 index 选择张量的特定元素。</p>
<p>features.index_select(<strong>0</strong>, j)：从 features
<strong>按行</strong>选择索引 j 对应的样本</p>
<p>函数执行后就直接从fetures与labels中返回对应大小行数的的tensor了</p>
<p><img src="/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/image-20250228145747983.png"></p>
<p><img src="/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/image-20250228145800616.png"></p>
<h2 id="初始化模型参数">4. 初始化模型参数</h2>
<p>将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们要让它们的requires_grad=True。</p>
<p>（可以看出这里是w是列向量）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>## 5.定义模型</p>
<p>我们使用mm函数做矩阵乘法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">6.定义损失函数</h2>
<p>使用上一节描述的平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值y变形成预测值y_hat的形状。以下函数返回的结果也将和y_hat的形状相同。</p>
<p>这里的 y.view(y_hat.size()) 主要作用是让 y 的形状与 y_hat
保持一致，以便执行后续的减法运算 y_hat - y</p>
<p>y 的原始形状: torch.Size(<strong>[3]</strong>)</p>
<p>y 经过 view 变成: torch.Size(<strong>[3, 1]</strong>)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def squared_loss(y_hat, y):</span><br><span class="line">    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2</span><br><span class="line">    return (y_hat - y.view(y_hat.size())) ** 2 / 2</span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法">7.定义优化算法</h2>
<p>以下的sgd函数实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</p>
<p><img src="/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/image-20250304100914564.png" alt="image-20250304100914564" style="zoom:80%;"></p>
<p>param.grad 和 param 的形状是完全相同的，这样可以逐元素计算梯度，并对
param 中的每个数值进行更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># 注意这里更改param时用的param.data</span></span><br></pre></td></tr></table></figure>
<h2 id="训练模型">8.训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会遍历训练数据所有样本一次（假设样本数能够被批量大小整除）。X和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = squared_loss(linreg(X, w, b), y).<span class="built_in">sum</span>()  <span class="comment"># l是有关小批量linreg(X, w, b)和y的损失</span></span><br><span class="line"></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不要忘了梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">    train_l = squared_loss(linreg(features, w, b), labels)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br></pre></td></tr></table></figure>
<p>训练完成后，我们可以比较学到的参数和用来生成训练集的真实参数。它们应该很接近。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/02/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-2/image-20250304101534565.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》3.3 线性回归的简洁实现</title>
    <url>/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>导入库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h2 id="生成与上一节中相同的数据集">1.生成与上一节中相同的数据集。</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, num_inputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="读取数据">2.读取数据</h2>
<p>效果与之前的一样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里data_iter的使用跟上一节中的一样。让我们读取并打印第一个小批量数据样本。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="built_in">print</span>(X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305095318225.png" alt="image-20250305095318225">
<figcaption aria-hidden="true">image-20250305095318225</figcaption>
</figure>
<h2 id="定义模型">3.定义模型</h2>
<p>我们需要定义模型参数，并使用它们一步步描述模型是怎样计算的。当模型结构变得更复杂时，这些步骤将变得更繁琐。其实，PyTorch提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用PyTorch更简洁地定义线性回归。</p>
<p>导入torch.nn模块。实际上，“nn”是neural
networks（神经网络）的缩写。顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了autograd，而nn就是利用autograd来定义模型。nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。<strong>在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络层</strong>。</p>
<p>下面先来看看如何用nn.Module实现一个线性回归模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(n_feature, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># forward 定义前向传播</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = <span class="variable language_">self</span>.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="built_in">print</span>(net) <span class="comment"># 使用print可以打印出网络的结构</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305095515977.png"></p>
<p>事实上我们还可以用nn.Sequential来更加方便地搭建网络，Sequential是一个有序的容器，网络层将按照在传入Sequential的顺序依次被添加到计算图中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 此处还可以传入其他层</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># net.add_module ......</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">          <span class="comment"># ......</span></span><br><span class="line">        ]))</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305100441728.png"></p>
<p>可以通过net.parameters()来查看模型所有的可学习参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305100612074.png"></p>
<h2 id="初始化模型参数">4.初始化模型参数</h2>
<p>在使用net前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。</p>
<p>PyTorch在init模块中提供了多种参数初始化方法。这里的init是initializer的缩写形式。我们通过init.normal_将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的<strong>正态分布</strong>。偏差会初始化为零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0</span>)  <span class="comment"># 也可以直接修改bias的data: net[0].bias.data.fill_(0)</span></span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">5.定义损失函数</h2>
<p>PyTorch在nn模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch也将这些损失函数实现为nn.Module的子类。</p>
<p>我们现在使用它提供的<strong>均方误差</strong>损失作为模型的损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法">6.定义优化算法</h2>
<p>同样，我们也无须自己实现小批量随机梯度下降算法。</p>
<p>torch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。</p>
<p>下面我们创建一个用于优化net所有参数的优化器实例，并指定学习率为0.03的小批量随机梯度下降（SGD）为优化算法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305101712105.png"></p>
<p>我们还可以为不同子网络设置不同的学习率，这在finetune时经常用到。例：</p>
<p>（这里并不存在第二层，net[1]只是用来举例说明的）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer =optim.SGD([</span><br><span class="line">                <span class="comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net[<span class="number">0</span>].parameters()&#125;, <span class="comment"># lr=0.03</span></span><br><span class="line">                <span class="comment"># &#123;&#x27;params&#x27;: net[1].parameters(), &#x27;lr&#x27;: 0.01&#125;</span></span><br><span class="line">            ], lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>
<p>有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。</p>
<p>一种是修改optimizer.param_groups中对应的学习率。</p>
<p>每一层网络的优化器学习率修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">&#x27;lr&#x27;</span>] *= <span class="number">0.1</span> <span class="comment"># 学习率为之前的0.1倍</span></span><br></pre></td></tr></table></figure>
<p>指定网络层的优化器学习率修改</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>]= <span class="number">0.1</span></span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br></pre></td></tr></table></figure>
<p>修改前后</p>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305102739758.png"></p>
<p>另一种是更简单也是较为推荐的做法——<strong>新建优化器</strong>，由于optimizer十分轻量级，构建开销很小，故而可以构建新的optimizer。但是后者对于使用动量的优化器（如Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。</p>
<h2 id="训练模型">7.训练模型</h2>
<p>在使用Gluon训练模型时，我们通过调用optim实例的step函数来迭代模型参数。按照小批量随机梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss: %f&#x27;</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure>
<p>其中的y.view(-1, 1)，-1 让 PyTorch
自动计算该维度的大小，以保持元素总数不变。1 说明最终张量的
第二维（列）是 1，形成 (batch_size, 1) 形状的列向量。</p>
<p>下面我们分别比较学到的模型参数和真实的模型参数。我们从net获得需要的层，并访问其权重（weight）和偏差（bias）。学到的参数和真实的参数很接近。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dense = net[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(true_w,<span class="string">&#x27;\n&#x27;</span>, dense.weight)</span><br><span class="line"><span class="built_in">print</span>(true_b,<span class="string">&#x27;\n&#x27;</span>, dense.bias)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/05/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-3/image-20250305103637458.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习3.5 图像分类数据集</title>
    <url>/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>图像分类数据集中最常用的是手写数字识别数据集MNIST[1]。但大部分模型在MNIST上的分类精度都超过了95%。为了更直观地观察算法之间的差异，我们将使用一个图像内容更加复杂的数据集<strong>Fashion-MNIST[2]</strong>（这个数据集也比较小，只有几十M，没有GPU的电脑也能吃得消）。本节我们将使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。</p>
<p>torchvision主要由以下几部分构成：</p>
<ol type="1">
<li><strong>torchvision.datasets</strong> :
一些加载数据的函数及常用的<strong>数据集接口</strong>；</li>
<li>torchvision.models :
包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li>
<li>torchvision.transforms : 常用的图片变换，例如裁剪、旋转等；</li>
<li>torchvision.utils : 其他的一些有用的方法。</li>
</ol>
<h2 id="获取数据集">1. 获取数据集</h2>
<p>使用以下两行指定我们数据集所在目录。download=True，如果数据集不存在则自动从网上下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(mnist_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-5/image-20250320161343134.png"></p>
<p>我们可以通过下标来访问任意一个样本:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">feature, label = mnist_train[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(feature.shape, label)  <span class="comment"># Channel x Height x Width</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-5/image-20250320161442928.png"></p>
<p>需要注意的是，feature的尺寸是 (C x H x W) 的，而不是 (H x W x
C)。<strong>第一维是通道数</strong>，因为数据集中是灰度图像，所以通道数为1。后面两维分别是图像的高和宽。</p>
<p>Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle
boot（短靴）。以下函数可以将数值标签转成相应的文本标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels = [<span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br></pre></td></tr></table></figure>
<p>下面定义一个可以在一行里画出多张图像和对应标签的函数。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-5/image-20250320161619168.png"></p>
<p>现在，我们看一下训练数据集中前10个样本的图像内容和文本标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = [], []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    X.append(mnist_train[i][<span class="number">0</span>])</span><br><span class="line">    y.append(mnist_train[i][<span class="number">1</span>])</span><br><span class="line">show_fashion_mnist(X, get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-5/image-20250320161718280.png"></p>
<h2 id="读取小批量">2.读取小批量</h2>
<p>我们将在训练数据集上训练模型，并将训练好的模型在测试数据集上评价模型的表现。<strong>mnist_train是torch.utils.data.Dataset的子类</strong>，所以我们可以将其传入torch.utils.data.DataLoader来创建一个读取小批量数据样本的DataLoader实例。</p>
<p>在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的<strong>DataLoader</strong>中一个很方便的功能是<strong>允许使用多进程来加速数据读取</strong>。这里我们通过参数num_workers来设置4个进程读取数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>):</span><br><span class="line">    num_workers = <span class="number">0</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>
<p>根据GPT回答，Windows
多进程不太适用我们当前场景，所以最好还是在linux中运行。但学习阶段两者都行。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.7 softmax回归的简洁实现</title>
    <url>/2025/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="获取和读取数据">1. 获取和读取数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span>, root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Download the fashion mnist dataset and then load into memory.&quot;&quot;&quot;</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>):</span><br><span class="line">        num_workers = <span class="number">0</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_workers = <span class="number">4</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h2 id="定义和初始化模型">2.2定义和初始化模型</h2>
<p>在3.4节（softmax回归）中提到，softmax回归的输出层是一个全连接层，所以我们用一个线性模块就可以了。因为前面我们数据返回的每个batch样本x的形状为(batch_size,
1, 28, 28), 所以我们要先用view()将x的形状转换成(batch_size,
784)才送入全连接层。</p>
<p>我们将对x的形状转换的这个功能自定义一个FlattenLayer网络层，但是不做任何网络计算操作，仅仅修改输入x的形状，将x变成扁平。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(FlattenLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, *, *, ...)</span></span><br><span class="line">        <span class="keyword">return</span> x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这样我们就可以更方便地定义我们的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    OrderedDict([</span><br><span class="line">        (<span class="string">&#x27;flatten&#x27;</span>, FlattenLayer()),</span><br><span class="line">        (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, num_outputs))</span><br><span class="line">    ])</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后，我们使用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>这里<code>net.linear</code>指的是OrderedDict中的<code>'linear'</code>关键字。</p>
<h2 id="softmax和交叉熵损失函数">3. softmax和交叉熵损失函数</h2>
<p>PyTorch提供了一个包括softmax运算和交叉熵损失计算的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>类似于之前手动实现的两个函数的结合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(-<span class="number">1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<p>结果是一个列向量，即每行样本结果输出一个交叉熵损失</p>
<h2 id="定义优化算法">4. 定义优化算法</h2>
<p>使用学习率为0.1的小批量随机梯度下降作为优化算法。将net的parameters当作参数填入，然后设定学习率即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>我们看到之前optimizer为None的时候，我们手动对每个参数进行清零，并且获取梯度后，手动修改每个参数的值。</p>
<p>这次设定了optimizer后，直接通过 <code>optimizer.zero_grad()</code>
来清零梯度，通过 <code>optimizer.step()</code> 来使用梯度进行更新。</p>
<p><img src="/2025/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-7/image-20250327100238687.png" style="zoom:67%;"></p>
<h2 id="训练模型">5. 训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class="line"><span class="params">              params=<span class="literal">None</span>, lr=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2025/03/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-7/image-20250327100333812.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习3.6 softmax回归的从零开始实现</title>
    <url>/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="获取和读取数据">1. 获取和读取数据</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>):</span><br><span class="line">    num_workers = <span class="number">0</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    num_workers = <span class="number">4</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="初始化模型参数">2. 初始化模型参数</h2>
<p>跟线性回归中的例子一样，我们将使用向量表示每个样本。</p>
<p>已知<strong>每个样本</strong>输入是高和宽均为28像素的图像。模型的<strong>输入向量的长度是
28×28=784</strong>：该向量的每个元素对应图像中每个像素。</p>
<p>由于图像有10个类别，单层神经网络输出层的输出个数为10，因此softmax回归的权重和偏差参数分别为784×10和1×10的矩阵。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321101731654.png"></p>
<h2 id="实现softmax运算">3.实现softmax运算</h2>
<p>实现代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = X.exp()</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>
<p>首先来看<code>X_exp.sum(dim=1, keepdim=True)</code></p>
<p>我们可以只对其中同一列（dim=0）或同一行（dim=1）的元素求和，并在结果中保留行和列这两个维度（keepdim=True）。</p>
<p>举例如下，对X分别进行行和列上的求和并保留维度</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321101833127.png" style="zoom:67%;"></p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321102022467.png" style="zoom: 50%;"></p>
<p><strong>dim=0</strong>
指的是<strong>在行的方向上修改</strong>，即<strong>行数以外的不变，</strong></p>
<p><strong>dim=1</strong>
指的是在<strong>列的方向上修改</strong>，即<strong>列数以外的不变</strong>，在二维的情况下列数以外不变则是保持行数，将每行元素相加</p>
<p>复习一下softmax的公式</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321102656346.png"></p>
<p>那么我们这里X_exp.sum(dim=1,
keepdim=True)的作用则是将每行样本的输出<span class="math inline">\(exp(o_i)\)</span>进行相加，即上图中的分母部分。</p>
<p>接下来是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">return</span> X_exp / partition  <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>
<p>经过前面的计算我们得到partition是 n x 1
的向量，这里n是行数，而X_exp则和输入的X一样，是n x m</p>
<p>利用广播机制（应该是自动转换的），partition是 n x 1会重复m次，变成n x
m的矩阵</p>
<p>举例如下</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321103442718.png" style="zoom: 50%;"></p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321103501252.png" style="zoom:50%;"></p>
<p>那么X_exp中每个元素都能与对应partition进行除法，然后生成新的矩阵。</p>
<p>这也意味着我们之前softmax公式的分子</p>
<h2 id="定义模型">4.定义模型</h2>
<p>有了softmax运算，我们可以定义上节描述的softmax回归模型了。这里通过view函数将每张原始图像改成长度为num_inputs的向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.mm(X.view((-<span class="number">1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">5.定义损失函数</h2>
<p>我们介绍了softmax回归使用的交叉熵损失函数。为了得到标签的预测概率，我们可以使用gather函数。</p>
<h3 id="gather函数">5.1 gather函数</h3>
<p>首先以一个例子介绍gather函数，在下面的例子中，变量y_hat是2个样本在3个类别的预测概率，变量y是这2个样本的标签类别。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321104651859.png" alt="image-20250321104651859" style="zoom: 67%;"></p>
<p>dim=1 指的是要在列方向上进行操作。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321105006574.png" style="zoom:50%;"></p>
<p>gather(1, y.view(-1, 1)) 操作的作用是从 y_hat
中根据每个样本的真实标签 y 选择对应的预测值。</p>
<p>这里y_hat.gather(1, y.view(-1, 1))的输出矩阵和 y.view(-1, 1)
形状一致，也是n x
1。或者反过来说，我们填入gather的第二个参数行数一定要和y_hat一样，然后列数为1，这样才能从y_hat的每行中挑一个组成新的矩阵。</p>
<p>在转换后的，如果每行再添加行号就很明显了。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321153544770.png" style="zoom:50%;"></p>
<p>相当于 <span class="math inline">\(y\_hat[0][0]、y\_hat[1][2]\)</span>。</p>
<p>通过使用gather函数，我们得到了2个样本的标签的预测概率。与3.4节（softmax回归）数学表述中标签类别离散值从1开始逐一递增不同，在代码中，标签类别的离散值是从0开始逐一递增的。</p>
<h3 id="定义损失函数-1">5.2 定义损失函数</h3>
<p>我们知道交叉熵误差公式如下 <span class="math display">\[
H(y^{(i)},\hat{y}^{(i)}) = -\sum ^{q}_{j=1}
{y^{(i)}_jlog\hat{y}^{(i)}_j}
\]</span>
因为除了目标类y为1，其他的都为0。因此我们进获取目标类y的值来进行计算(下图在下标的<span class="math inline">\(y^{(i)}\)</span>起始就是上图的j，不知道是不是课本打印公式打错了，指的是第i个样本的label
j才为1) <span class="math display">\[
H(y^{(i)},\hat{y}^{(i)}) = -log\hat{y}^{(i)}_j
\]</span>
同样，在神经网络计算的结果中，我们也不去管其他任何结果，只用gather函数去获取y_hat中目标类i的值。因此我们就需要用gather函数去获取目标第i类的值。</p>
<p>下面实现了3.4节（softmax回归）中介绍的交叉熵损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> - torch.log(y_hat.gather(<span class="number">1</span>, y.view(-<span class="number">1</span>, <span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
<h2 id="计算分类准确率">6.计算分类准确率</h2>
<p>下面定义准确率accuracy函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().mean().item()</span><br></pre></td></tr></table></figure>
<p><code>y_hat.argmax(dim=1)</code>返回矩阵y_hat每行中最大元素的索引。</p>
<p><code>(y_hat.argmax(dim=1) == y)</code>是一个类型为ByteTensor的Tensor。</p>
<p>也就是说样本预测正确即为1，预测错误即为0，所有结果相加并除以样本数（上面的mean函数作用），当成是准确率。</p>
<p>类似地，我们可以计算评价模型net在数据集data_iter上的准确率。y.shape[0]
返回的是该数组的行数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">data_iter, net</span>):</span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>
<p>和之前不同，我们无法一次将所有样本进行计算，每次只能获取部分的data，因此我们每次累计正确数acc_sum与样本数n，最后进行acc_sum
/ n获取平均数（就相当于之前的mean函数）</p>
<h2 id="训练模型">7. 训练模型</h2>
<p>代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class="line"><span class="params">              params=<span class="literal">None</span>, lr=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                d2l.sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()  <span class="comment"># “softmax回归的简洁实现”一节将用到</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line">        </span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>
<p>我们能注意到每批样本的loss计算：<code>l = loss(y_hat, y).sum()</code></p>
<ul>
<li>使用 sum() 时，损失值的绝对大小随着 batch size
增大而增大，因此梯度也增大，影响训练稳定性。</li>
<li>使用 mean() 时，损失值不会因为 batch size
变化而变化，使得训练更稳定，适用于可变 batch size。</li>
</ul>
<p>因此</p>
<ul>
<li>如果 batch size 是固定的，.sum() 和 .mean()
都可以使用。(不过mean生成的loss更小,因此更新更慢)</li>
<li>如果 batch size 变化较大，推荐使用 .mean()，训练更稳定。</li>
</ul>
<p>除此外,这里因为optimizer是None，因此更新方式<code>d2l.sgd(params, lr, batch_size)</code>是直接减去偏导数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="comment"># 为了和原书保持一致，这里除以了batch_size，但是应该是不用除的，因为一般用PyTorch计算loss时就默认已经</span></span><br><span class="line">    <span class="comment"># 沿batch维求了平均了。</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># 注意这里更改param时用的param.data</span></span><br></pre></td></tr></table></figure>
<h3 id="预测">8. 预测</h3>
<p>训练完成后，现在就可以演示如何对图像进行分类了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_iter))</span><br><span class="line">true_labels = d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class="line">pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=<span class="number">1</span>).numpy())</span><br><span class="line">titles = [true + <span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(true_labels, pred_labels)]</span><br><span class="line"></span><br><span class="line">d2l.show_fashion_mnist(X[<span class="number">0</span>:<span class="number">9</span>], titles[<span class="number">0</span>:<span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<p>记得把下图注释去掉</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321160716398.png" style="zoom:50%;"></p>
<p>给定一系列图像（第三行图像输出），我们比较一下它们的真实标签（第一行文本输出）和模型预测结果（第二行文本输出）。</p>
<p><img src="/2025/03/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-6/image-20250321160732356.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》3.4 softmax回归</title>
    <url>/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="分类问题">1.分类问题</h2>
<p><strong>softmax模型是用来解决分类问题的一种方法</strong>。</p>
<p>我们将图像中的4像素分别记为<span class="math inline">\(x_1,x_2,x_3,x_4\)</span>。</p>
<p>假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值<span class="math inline">\(y_1,y_2,y_3\)</span>。</p>
<p>举个例子，第i个图像样本<span class="math inline">\(x^{(i)}=[x_1,x_2,x_3,x_4]\)</span>，在经过了相应大小的的w和b进行计算后，该第i图像样本对应的输出为<span class="math inline">\(\hat{y}^{(i)}=[y_1,y_2,y_3]\)</span>，其中<span class="math inline">\(y_1,y_2,y_3\)</span>结果对应狗、猫、鸡3个结果。</p>
<p>以该图像是狗为例，在样本的label中<span class="math inline">\(y^{(i)}=[1,0,0]\)</span>，而我们计算结果可能是<span class="math inline">\(\hat{y}^{(i)}=[0.8,0.1,0.07]\)</span></p>
<h2 id="softmax回归模型">2.softmax回归模型</h2>
<p>softmax回归跟线性回归一样将输入特征与权重做线性叠加。</p>
<p>与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。
<span class="math display">\[
\begin{aligned}
o_1=x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}+b_1
\\
o_2=x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}+b_2
\\
o_3=x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}+b_3
\end{aligned}
\]</span>
<img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/image-20250311102451667.png" alt="image-20250311102451667" style="zoom: 67%;"></p>
<p>softmax回归同线性回归一样，也是一个单层神经网络。</p>
<p>由于每个输出<span class="math inline">\(o_1,o_2,o_3\)</span>的计算都要依赖于所有的输入<span class="math inline">\(x_1,x_2,x_3,x_4\)</span>，softmax回归的输出层也是一个全连接层。</p>
<p>softmax运算符（softmax
operator）通过下式将输出值变换成值为正且和为1的概率分布： <span class="math display">\[
\hat{y}_1,\hat{y}_2,\hat{y}_3 = softmax(o_1,o_2,o_3)
\]</span> 其内部具体计算为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/image-20250311102822395.png" alt="image-20250311102822395" style="zoom:67%;"></p>
<p>容易看出 <span class="math inline">\(\hat{y}_1+\hat{y}_2+\hat{y}_3=1
且 0&lt;=\hat{y}_1,\hat{y}_2,\hat{y}_3&lt;=1\)</span>。</p>
<p>其中exp 代表的是指数函数，即 e 的幂次运算，数学上表示为：<span class="math inline">\(exp(x)=e^x\)</span></p>
<p>指数函数 <span class="math inline">\(exp(x) = e^x\)</span>
会放大较大的数值，而较小的数值则被压缩得更接近 0。例如： <span class="math display">\[
\begin{aligned}
exp(o_1=3)=20.09,\quad exp(o_2=1)=2.72,\quad exp(o_3=-1)=0.37\\
\\
y_1=\frac {20.09} {20.09+2.72+0.37}=0.86,\quad y_1=0.12 \quad y_3=0.02
\end{aligned}
\]</span> 可以看到 较大的值 <span class="math inline">\(o_1\)</span>
产生的概率明显更高。</p>
<p>这种特性可以让模型在做决策时更明确，避免输出值之间的差距太小，导致不确定性过高。</p>
<h2 id="单样本分类的矢量计算表达式">3.单样本分类的矢量计算表达式</h2>
<p>为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。</p>
<p>在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/image-20250311105027439.png"></p>
<p>设高和宽分别为2个像素的图像样本i的特征为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/1.png"></p>
<p>输出层的输出为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/1-1741661478996-2.png"></p>
<p>预测为狗、猫或鸡的概率分布为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/1-1741661490775-4.png"></p>
<p>综上，softmax回归对样本i分类的矢量计算表达式为</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/image-20250311105203493.png"></p>
<h2 id="小批量样本分类的矢量计算表达式">4.小批量样本分类的矢量计算表达式</h2>
<p><span class="math display">\[
\begin{align}
\mathbf{O} = \mathbf{XW+b} \\
\mathbf{\hat{Y}} = softmax(\mathbf{O})
\end{align}
\]</span></p>
<p>其中<span class="math inline">\(\mathbf{X}\)</span>是多个<span class="math inline">\(x^{(i)}\)</span>排列而成。</p>
<h2 id="交叉熵损失函数">5.交叉熵损失函数</h2>
<p><span class="math display">\[
H(y^{(i)},\hat{y}^{(i)}) = -\sum ^{q}_{j=1}
{y^{(i)}_jlog\hat{y}^{(i)}_j}
\]</span></p>
<p>其中<span class="math inline">\(y^{(i)}_j\)</span>是向量<span class="math inline">\(y^{(i)}\)</span>中非0即1的元素</p>
<p>在上式中，我们知道向量<span class="math inline">\(y^{(i)}\)</span>中只有第k个元素<span class="math inline">\(y^{(i)}_k\)</span>为1，其余全为0，因此我们能够改写如下
<span class="math display">\[
H(y^{(i)},\hat{y}^{(i)}) = -log\hat{y}^{(i)}_k
\]</span> 举例 <span class="math display">\[
y^{(i)}=[0,1,0]
\]</span> 代入交叉熵公式中 <span class="math display">\[
H(y^{(i)},\hat{y}^{(i)}) =
-(0*log\hat{y}^{(i)}_1+1*log\hat{y}^{(i)}_2+0*log\hat{y}^{(i)}_3)=-log\hat{y}^{(i)}_2
\]</span></p>
<p>因此我们能看出损失函数<span class="math inline">\(H(y^{(i)},\hat{y}^{(i)})\)</span>实际上是一个log函数</p>
<p>如果模型对真实类别的<strong>预测概率高</strong>（接近 1），那么
-log(概率) 也就小，<strong>损失低</strong>，说明模型预测正确。</p>
<p>如果模型对真实类别的<strong>预测概率低</strong>（接近 0），那么
-log(概率) 会变大，<strong>损失也就更高</strong>，说明模型预测错误。</p>
<p><img src="/2025/03/11/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-4/image-20250311145719398.png" alt="image-20250311145719398" style="zoom:67%;"></p>
<p>假设训练数据集的样本数为n，交叉熵损失函数定义为 <span class="math display">\[
L=\frac {1} {n}\sum ^{n}_{i=1} {H(y^{(i)},\hat{y}^{(i)})}
\]</span></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.9 多层感知机的从零开始实现</title>
    <url>/2025/04/02/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-9/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>这节我们用多层感知机来解决之前的图片分类问题。</p>
<p>本节基础代码较多，也有许多以前用过的函数，不会进行过多解释占用额外篇幅。</p>
<h2 id="获取和读取数据">1. 获取和读取数据</h2>
<p>继续用之前的图片数据fashion_mnist。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span>, root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Download the fashion mnist dataset and then load into memory.&quot;&quot;&quot;</span></span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.append(torchvision.transforms.Resize(size=resize))</span><br><span class="line">    trans.append(torchvision.transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">    transform = torchvision.transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    <span class="keyword">if</span> sys.platform.startswith(<span class="string">&#x27;win&#x27;</span>):</span><br><span class="line">        num_workers = <span class="number">0</span>  <span class="comment"># 0表示不用额外的进程来加速读取数据</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num_workers = <span class="number">4</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<h2 id="定义模型参数">2. 定义模型参数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="定义激活函数">3. 定义激活函数</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(<span class="built_in">input</span>=X, other=torch.tensor(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure>
<p>torch.max函数接受2个参数，第一个数输入的矩阵X，第二个other参数是标量0。其作用是X中每个数与0进行比较，区最大的值作为新矩阵中的元素。其结果是与X同大小的，大于等于0的新矩阵。</p>
<h2 id="定义模型">4. 定义模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.view((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">5. 定义损失函数</h2>
<p>为了得到更好的数值稳定性，我们直接使用PyTorch提供的包括softmax运算和交叉熵损失计算的函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h2 id="训练模型">6. 训练模型</h2>
<p>训练多层感知机的步骤和3.6节中训练softmax回归的步骤没什么区别。我们直接调用d2lzh_pytorch包中的train_ch3函数，它的实现已经在3.6节里介绍过。我们在这里设超参数迭代周期数为5，学习率为100.0。</p>
<p>(学习率之所以这么大，应该是因为d2lzh_pytorch里面的sgd函数在更新的时候除以了batch_size，其实PyTorch在计算loss的时候已经除过一次了，sgd这里应该不用除了)</p>
<p><img src="/2025/04/02/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-9/image-20250402145119006.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class="line"><span class="params">              params=<span class="literal">None</span>, lr=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 梯度清零</span></span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">            <span class="keyword">elif</span> params <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> params[<span class="number">0</span>].grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">                    param.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line">            <span class="keyword">if</span> optimizer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                sgd(params, lr, batch_size)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line"></span><br><span class="line">            train_l_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line">        </span><br><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">100.0</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/02/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-9/image-20250402145352165.png" style="zoom:67%;"></p>
<p>当多层感知机的层数较多时，本节的实现方法会显得较烦琐，例如在定义模型参数的时候。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 3.8 多层感知机</title>
    <url>/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="隐藏层">1.隐藏层</h2>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250327101017490.png" style="zoom:50%;"></p>
<p>多层感知机中，输入和输出个数分别为4和3，中间的隐藏层中包含了5个隐藏单元（hidden
unit）。由于输入层不涉及计算，图3.3中的多层感知机的层数为2。</p>
<p>给定一个小批量样本<span class="math inline">\(X(n *
d)\)</span>，其批量大小为 <span class="math inline">\(n\)</span>，输入个数为 <span class="math inline">\(d\)</span>。</p>
<p>其中隐藏单元个数为 <span class="math inline">\(h\)</span>
。那么<strong>隐藏层的输出</strong>（也称为隐藏层变量或隐藏变量）为<span class="math inline">\(H(n *
h)\)</span>。因为隐藏层和输出层均是全连接层，因此
隐藏层的<strong>权重参数</strong>和<strong>偏差参数</strong>分别为<span class="math inline">\(W_h(d * h)\)</span>和<span class="math inline">\(b_h(1 *
h)\)</span>。其中d为前一层输出的列数，而这里的前一层输出则直接是输入样本X。且每个隐藏层节点的输出需要一个偏差b，h个隐藏层节点则需要h个偏差b。</p>
<p>输出层同理，输出层的输出为$ O(n * q) $ 其中 <span class="math inline">\(q\)</span> 为输出单元个数
。输出层的权重和偏差参数分别为 <span class="math inline">\(W_o(h *
q)\)</span> 和 <span class="math inline">\(b_o(1*q)\)</span> <span class="math display">\[
\begin{aligned}
H=XW_h*b_h \\
O=HW_o*b_o
\end{aligned}
\]</span>
将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到
<span class="math display">\[
O=(XW_h+b_h)W_o+b_o=XW_hW_o+b_hW_o+b_o
\]</span>
从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络。其中输出层权重参数为<span class="math inline">\(W_hW_o\)</span>，偏差为<span class="math inline">\(b_hW_o+b_o\)</span>
。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p>
<h2 id="激活函数">2. 激活函数</h2>
<h3 id="relu函数">2.1 ReLU函数</h3>
<p>上述问题的根源在于全连接层只是对数据做仿射变换（affine
transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是<strong>引入非线性变换</strong>，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。</p>
<p>这个非线性函数被称为<strong>激活函数（activation
function）</strong>。下面我们介绍几个常用的激活函数。 <span class="math display">\[
ReLU(x)=max(x,0)
\]</span> 可以看出，ReLU函数只保留正数元素，并将负数元素清零。</p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328101000294.png" style="zoom:67%;"></p>
<p>下图是ReLU的导数图。显然，当输入为负数时，ReLU函数的导数为0；当输入为正数时，ReLU函数的导数为1。尽管输入为0时ReLU函数不可导，但是我们可以取此处的导数为0。</p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328101048509.png" style="zoom:67%;"></p>
<h3 id="sigmoid函数">2.2 sigmoid函数</h3>
<p><span class="math display">\[
sigmoid(x)=\frac{1}{1+exp(-x)}
\]</span></p>
<p>sigmoid函数在<strong>早期的神经网络中较为普遍</strong>，但它<strong>目前逐渐被更简单的ReLU函数取代</strong>。在后面“循环神经网络”一章中我们会介绍如何利用它值域在0到1之间这一特性来控制信息在神经网络中的流动。</p>
<p>下图是sigmoid函数图</p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328101933951.png" style="zoom:67%;"></p>
<p>依据链式法则，sigmoid函数的导数 <span class="math display">\[
sigmoid&#39;(x)=sigmoid(x)(1-sigmoid(x))
\]</span>
下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328102109339.png" alt="sigmoid导数图" style="zoom:67%;"></p>
<h3 id="tanh函数">2.3 tanh函数</h3>
<p><span class="math display">\[
tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}
\]</span></p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328103336028.png" style="zoom:67%;"></p>
<p>依据链式法则，tanh函数的导数 <span class="math display">\[
tanh&#39;(x)=1=tanh^2(x)
\]</span>
下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p>
<p><img src="/2025/03/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A03-8/image-20250328103430953.png" style="zoom:67%;"></p>
<h2 id="多层感知机">3 多层感知机</h2>
<p>多层感知机就是<strong>含有至少一个隐藏层</strong>的<strong>由全连接层组成</strong>的神经网络，且<strong>每个隐藏层的输出通过激活函数进行变换</strong>。
<span class="math display">\[
\begin{aligned}
H=ϕ(XW_h*b_h) \\
O=HW_o*b_o
\end{aligned}
\]</span> <span class="math inline">\(ϕ\)</span>表示激活函数。H是隐藏层的计算，O是最后的输出层的计算，因此O不用激活函数处理</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 4.4 自定义层</title>
    <url>/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>本节将介绍如何使用Module来自定义层，从而可以被重复调用。</p>
<h2 id="不含模型参数的自定义层">1 不含模型参数的自定义层</h2>
<p>下面的CenteredLayer类通过继承Module类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了forward函数里。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(CenteredLayer, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br></pre></td></tr></table></figure>
<p>我们可以实例化这个层，然后做前向计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">layer = CenteredLayer()</span><br><span class="line">output = layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-4/image-20250520143416098.png" style="zoom: 67%;"></p>
<p>我们也可以用它来构造更复杂的模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br></pre></td></tr></table></figure>
<h2 id="含模型参数的自定义层">2 含模型参数的自定义层</h2>
<p>在4.2节（模型参数的访问、初始化和共享）中介绍了Parameter类其实是Tensor的子类，如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里。</p>
<p>nn.Parameter 本质上是 Tensor，默认
requires_grad=True，因此parameter对象默认都是可求导的。</p>
<h3 id="parameterlist-参数列表">2.1 ParameterList 参数列表</h3>
<p><strong>ParameterList</strong>
接收一个Parameter实例的列表作为输入然后得到一个参数列表，可以使用append和extend在列表后面新增参数。</p>
<p>手动创建
3个4x4的w权重参数与1个4x1的w权重参数，通过ParameterList将所有参数放在一个列表中管理，然后进行手动计算的前向传播的网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDense</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDense, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        <span class="variable language_">self</span>.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>.params)):</span><br><span class="line">            x = torch.mm(x, <span class="variable language_">self</span>.params[i])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">net = MyDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-4/image-20250520145011568.png" style="zoom: 67%;"></p>
<h3 id="parameterdict-参数字典">2.2 ParameterDict 参数字典</h3>
<p>ParameterDict接收一个Parameter实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。例如使用update()新增参数，使用keys()返回所有键值，使用items()返回所有键值对等等。</p>
<p>下面自定义模型包装多个网路层，模型构建时，用字符串进行网络层选择（这里的网络层是单纯的w权重参数）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyDictDense</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="variable language_">self</span>.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, <span class="variable language_">self</span>.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-4/image-20250520150252799.png" style="zoom:67%;"></p>
<p>这样就可以根据传入的键值来进行不同的前向传播：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear1&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear2&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear3&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-4/image-20250520150656403.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第4节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 4.1 模型构造</title>
    <url>/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>本章节我们介绍另外一种基于Module类的模型构造方法：它让模型构造更加灵活。</p>
<h2 id="继承module类来构造模型">1. 继承Module类来构造模型</h2>
<p>Module类是nn模块里提供的一个模型构造类，是所有神经网络模块的基类，我们可以继承它来定义我们想要的模型。</p>
<p>下面继承Module类构造本节开头提到的多层感知机。</p>
<p>这里定义的MLP类重载了Module类的<strong>init函数</strong>和<strong>forward函数</strong>。它们分别用于<strong>创建模型参数</strong>和<strong>定义前向计算</strong>。前向计算也即正向传播。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 调用MLP父类Module的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        <span class="variable language_">self</span>.act = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        a1 = <span class="variable language_">self</span>.hidden(x)</span><br><span class="line">        a2 = <span class="variable language_">self</span>.act(a1)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(a2)</span><br></pre></td></tr></table></figure>
<p>以上的MLP类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的backward函数。</p>
<p>我们可以实例化MLP类得到模型变量net。下面的代码初始化net并传入输入数据X做一次前向计算。其中，net(X)会调用MLP继承自Module类的__call__函数，这个函数将调用MLP类定义的forward函数来完成前向计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">784</span>)</span><br><span class="line">net = MLP()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506094427873.png" style="zoom:67%;"></p>
<p>注意，这里并没有将Module类命名为Layer（层）或者Model（模型）之类的名字，这是因为该类是一个可供自由组建的部件。它的子类既可以是一个层（如PyTorch提供的Linear类），又可以是一个模型（如这里定义的MLP类），或者是模型的一个部分。我们下面通过两个例子来展示它的灵活性。</p>
<h2 id="module的子类">2. Module的子类</h2>
<p>Module类是一个通用的部件。事实上，PyTorch还实现了继承自Module的可以方便构建模型的类:
如Sequential、ModuleList和ModuleDict等等。</p>
<h3 id="sequential类">2.1 Sequential类</h3>
<p>当模型的前向计算为<strong>简单串联各个层</strong>的计算时，Sequential类可以通过更加简单的方式定义模型。</p>
<p>这正是Sequential类的目的：它可以接收一个子模块的有序字典（OrderedDict）或者一系列子模块作为参数来逐一添加Module的实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。</p>
<p>手动实现Sequential类，这里叫做MySequential类</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>(MySequential, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(args) == <span class="number">1</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(args[<span class="number">0</span>], OrderedDict):</span><br><span class="line">            <span class="comment"># 如果传入的是一个OrderedDict</span></span><br><span class="line">            <span class="keyword">for</span> key, module <span class="keyword">in</span> args[<span class="number">0</span>].items():</span><br><span class="line">                <span class="variable language_">self</span>.add_module(key, module)</span><br><span class="line">                <span class="comment"># add_module方法会将module添加进self._modules(一个OrderedDict)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 传入的是一些Module</span></span><br><span class="line">            <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">                <span class="variable language_">self</span>.add_module(<span class="built_in">str</span>(idx), module)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成员</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> <span class="variable language_">self</span>._modules.values():</span><br><span class="line">            <span class="built_in">input</span> = module(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span></span><br></pre></td></tr></table></figure>
<p>我们用MySequential类来实现前面描述的MLP类，并使用随机初始化的模型做一次前向计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">784</span>)</span><br><span class="line">net = MySequential(</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506094813631.png" style="zoom:67%;"></p>
<p>可以观察到这里MySequential类的使用跟3.10节（多层感知机的简洁实现）中Sequential类的使用没什么区别。</p>
<h3 id="modulelist类">2.2 ModuleList类</h3>
<p>ModuleList<strong>接收一个子模块的列表</strong>作为输入，然后也可以<strong>类似List</strong>那样进行append和extend操作:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.ModuleList([</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    ])</span><br><span class="line">net.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>)) <span class="comment"># # 类似List的append操作</span></span><br><span class="line"><span class="built_in">print</span>(net[-<span class="number">1</span>])  <span class="comment"># 类似List的索引访问</span></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506095711209.png" style="zoom:67%;"></p>
<p>既然Sequential和ModuleList都可以进行列表化构造网络，那二者区别是什么呢。</p>
<p>ModuleList仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现forward功能需要自己实现，所以上面执行net(torch.zeros(1,
784))会报NotImplementedError；</p>
<p>而<strong>Sequential内的模块</strong>需要按照顺序排列，要保证相邻层的输入输出大小相匹配，<strong>内部forward功能已经实现</strong>。</p>
<p>ModuleList的出现只是让网络定义前向传播时更加灵活，见下面官网的例子。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModule, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.linears = nn.ModuleList([</span><br><span class="line">            nn.Linear(<span class="number">10</span>, <span class="number">10</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># ModuleList can act as an iterable, or be indexed using ints</span></span><br><span class="line">        <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.linears):</span><br><span class="line">            x = <span class="variable language_">self</span>.linears[i // <span class="number">2</span>](x) + l(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>另外，ModuleList不同于一般的Python的list，加入到ModuleList里面的所有模块的参数会被自动添加到整个网络中，下面看一个例子对比一下。</p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506100336968.png" style="zoom:67%;"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net1 = Module_ModuleList()</span><br><span class="line">net2 = Module_List()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;net1:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> net1.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p.size())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;net2:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> net2.parameters():</span><br><span class="line">    <span class="built_in">print</span>(p)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506100449670.png" style="zoom: 67%;"></p>
<p>普通list和nn.ModuleList的区别可以参考GPT的回复</p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506101215693.png" style="zoom:50%;">
。</p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506101345275.png" style="zoom:50%;"></p>
<h3 id="moduledict类">2.3 ModuleDict类</h3>
<p>ModuleDict接收一个子模块的字典作为输入,
然后也可以类似字典那样进行添加访问操作:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[<span class="string">&#x27;output&#x27;</span>] = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="string">&#x27;linear&#x27;</span>]) <span class="comment"># 访问</span></span><br><span class="line"><span class="built_in">print</span>(net.output)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506100659351.png" style="zoom:67%;"></p>
<p>和ModuleList一样，ModuleDict实例仅仅是存放了一些模块的字典，并没有定义forward函数需要自己定义。同样，ModuleDict也与Python的Dict有所不同，ModuleDict里的所有模块的参数会被自动添加到整个网络中。</p>
<h2 id="构造复杂的模型">3 构造复杂的模型</h2>
<p>虽然上面介绍的这些类可以使模型构造更加简单，且不需要定义forward函数，但直接继承Module类可以极大地拓展模型构造的灵活性。</p>
<p>下面我们构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用Tensor的函数和Python的控制流，并多次调用相同的层。</p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506102349143.png"></p>
<p>其中，</p>
<ol type="1">
<li>x.norm() 计算张量 x 的 范数（norm），默认计算 L2
范数（即欧几里得范数），相当于：</li>
</ol>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506102530863.png" style="zoom:50%;"></p>
<p>也可以指定不同的范数类型，例如 x.norm(p=1) 计算 L1 范数。</p>
<ol start="2" type="1">
<li>item() 方法用于将 单个元素的张量 转换为 Python 标量（float 或
int）。由于 x.norm() 计算的是一个标量（0 维张量），item()
作用是取出这个数值，变成普通的 Python 浮点数，方便用于 if 语句等 Python
逻辑判断。</li>
</ol>
<p>因为FancyMLP和Sequential类都是Module类的子类，所以我们可以嵌套调用它们。</p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506102702680.png"></p>
<p><img src="/2025/04/30/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-1/image-20250506102713988.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第4节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 4.2 模型参数的访问、初始化和共享</title>
    <url>/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>在3.3节（线性回归的简洁实现）中，我们通过init模块来初始化模型的参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。</p>
<p>我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们从nn中导入了init模块，它包含了多种模型初始化方法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">)  <span class="comment"># pytorch已进行默认初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(net)</span></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">Y = net(X).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<h2 id="访问模型参数">1 访问模型参数</h2>
<p>对于Sequential实例中含模型参数的层，我们可以通过Module类的<strong>parameters()</strong>或者<strong>named_parameters</strong>方法来访问所有参数（以迭代器的形式返回），后者除了返回参数Tensor外还会返回其名字。</p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512211254414.png"></p>
<p>两种用法简单举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name, param.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面，访问多层感知机net的所有参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net.named_parameters()))</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name,<span class="string">&#x27;\t\t&#x27;</span>, param.size())</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512162908682.png" style="zoom:67%;"></p>
<p>可见返回的名字自动加上了层数的索引作为前缀。</p>
<p>我们再来访问net中单层的参数。对于使用Sequential类构造的神经网络，我们可以通过方括号[
] 来访问网络的任一层。索引0表示隐藏层为Sequential实例最先添加的层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name,<span class="string">&#x27;\t&#x27;</span>, param.size(), <span class="built_in">type</span>(param))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512163338746.png" style="zoom: 67%;"></p>
<p>因为这里是单层的所以没有了层数索引的前缀。另外返回的param的类型为torch.nn.parameter.Parameter，其实这是Tensor的子类，和Tensor不同的是<strong>如果一个Tensor是Parameter，那么它会自动被添加到模型的参数列表里</strong>，来看下面这个例子。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.weight1 = nn.Parameter(torch.rand(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">        <span class="variable language_">self</span>.weight2 = torch.rand(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">n = MyModel()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> n.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512164242794.png" style="zoom:67%;"></p>
<p>上面的代码中weight1在参数列表中但是weight2却没在参数列表中。</p>
<p>因为<strong>Parameter是Tensor</strong>，即Tensor拥有的属性它都有，比如可以根据data来访问参数数值，用grad来访问参数梯度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight_0 = <span class="built_in">list</span>(net[<span class="number">0</span>].parameters())[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(weight_0.data)</span><br><span class="line"><span class="built_in">print</span>(weight_0.grad) <span class="comment"># 反向传播前,梯度为None</span></span><br><span class="line">Y.backward()</span><br><span class="line"><span class="built_in">print</span>(weight_0.grad)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512164827344.png" style="zoom:67%;"></p>
<h2 id="初始化模型参数">2 初始化模型参数</h2>
<p>PyTorch中nn.Module的模块参数都采取了较为合理的初始化策略，但我们经常需要使用其他方法来初始化权重。</p>
<p>PyTorch的init模块里提供了多种预设的初始化方法。</p>
<p>继续使用一开始的网络进行举例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">)  <span class="comment"># pytorch已进行默认初始化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print(net)</span></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">Y = net(X).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>在下面的例子中，我们将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512212018165.png" style="zoom:67%;"></p>
<p>下面使用常数来初始化权重参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.constant_(param, val=<span class="number">0</span>)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250512212205836.png" style="zoom:67%;"></p>
<h2 id="自定义初始化方法">3 自定义初始化方法</h2>
<p>先来看看PyTorch是怎么实现这些初始化方法的，例如torch.nn.init.normal_：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normal_</span>(<span class="params">tensor, mean=<span class="number">0</span>, std=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">return</span> tensor.normal_(mean, std)</span><br></pre></td></tr></table></figure>
<p>可以看到这就是一个inplace改变Tensor值的函数，而且这个过程是不记录梯度的。
类似的我们来实现一个自定义的初始化方法。</p>
<p>在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为[−10,−5]和[5,10]两个区间里均匀分布的随机数。</p>
<p>即图中的tensor对tensor内容的每个数做相同的计算赋值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init_weight_(param)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<p>tensor.uniform_(-10, 10) 将每个数均匀分布初始化为[-10,10]内的数</p>
<p>(tensor.abs() &gt;= 5)
如果该数的绝对值大于等于5，则该括号内的值为1否则为0</p>
<p>4 共享模型参数</p>
<p>在有些情况下，我们希望在多个层之间共享模型参数。4.1.3节提到了如何共享模型参数:
Module类的forward函数里多次调用同一个层。此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的，下面来看一个例子:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">net = nn.Sequential(linear, linear)</span><br><span class="line"><span class="comment"># print(net)</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    init.constant_(param, val=<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513152844109.png" style="zoom: 80%;"></p>
<p>在内存中，这两个线性层其实一个对象:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(net[<span class="number">0</span>]) == <span class="built_in">id</span>(net[<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(net[<span class="number">0</span>].weight) == <span class="built_in">id</span>(net[<span class="number">1</span>].weight))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513152948362.png" style="zoom: 80%;"></p>
<p>因为模型参数里包含了梯度，所以在反向传播计算时，这些<strong>共享的参数的梯度是累加的</strong>:</p>
<p>简单说，PyTorch 反向传播时，若某个参数 w 在多个路径中使用，则其 .grad
的计算会改变成多次计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">net = nn.Sequential(linear, linear)</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    init.constant_(param, val=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y = net(x).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.grad) </span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513153932558.png" style="zoom:67%;"></p>
<p>上面原本课程中的例子。我们假设该2层线性层都是分离的，即为下图的
linear1 和 linear2 .</p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155045523.png" style="zoom:67%;"></p>
<p>我们能看到我们算出了y=9的值。</p>
<p>此时反向传播，计算权重梯度如下</p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155219749.png" style="zoom:67%;"></p>
<p>倘若共享w的话，前向传播会变成如下</p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155311063.png" style="zoom:67%;"></p>
<p>其反向传播，更新w变成如下。</p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155522302.png" style="zoom:67%;"></p>
<p>成功计算出程序输出中的6。</p>
<p>倘若我们使用3层共享的linear。那么根据以上公式，我们会得出w的梯度是<span class="math inline">\(\frac {dy} {dw}=\frac {d}
{dw}(w^3x)=3w^2=27\)</span></p>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155851304.png" style="zoom:67%;"></p>
<p>重新计算</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">net.zero_grad()</span><br><span class="line">y = net(x).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.grad)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/12/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-2/image-20250513155942159.png" style="zoom:67%;"></p>
<p>符合预期。</p>
<p>因为参数梯度叠加的问题，我估计应该不会在训练中使用共享层，或者说在训练中锁定共享层的参数，不进行修改。即对共享层
<code>param.requires_grad = False</code> 。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第4节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 4.5 读取和存储</title>
    <url>/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="读写tensor">1. 读写Tensor</h2>
<p>可以直接使用 <strong>save函数</strong> 和 <strong>load函数</strong>
分别存储和读取Tensor。</p>
<p>下面的例子创建了Tensor变量x，并将其存在文件名同为x.pt的文件里。然后我们将数据从存储的文件读回内存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/image-20250520154421390.png" style="zoom:67%;"></p>
<p>我们还可以存储一个Tensor列表并读回内存。（毕竟底层是pickle实现的）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">3</span>)</span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">xy_list = torch.load(<span class="string">&#x27;xy.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(xy_list)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/image-20250520154817628.png" style="zoom:67%;"></p>
<p>存储并读取一个从字符串映射到Tensor的字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;, <span class="string">&#x27;xy_dict.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">xy = torch.load(<span class="string">&#x27;xy_dict.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(xy)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/image-20250520155853762.png" style="zoom:67%;"></p>
<h2 id="net.state_dict-介绍">2. net.state_dict() 介绍</h2>
<p>在 PyTorch 中，net.state_dict() 是一个
字典（dict）对象，它保存了模型中的所有 参数和缓冲区的名称和值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MLP, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.act = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):                                             </span><br><span class="line">        a = <span class="variable language_">self</span>.act(<span class="variable language_">self</span>.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(a)</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line"><span class="built_in">print</span>(net.state_dict())</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/image-20250520201628839.png" style="zoom:67%;"></p>
<p>以下代码能清晰查看 net.state_dict() 的键值结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> net.state_dict():</span><br><span class="line">    <span class="built_in">print</span>(key,<span class="string">&#x27;\t&#x27;</span>,net.state_dict()[key])</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-5/image-20250520201804967.png" style="zoom:67%;"></p>
<p>你可以理解 net.state_dict() 仅包含了模型中所有的可学习参数即
<code>nn.Parameter</code> 而不包含类似
<code>self.some_value = 123</code> 等。</p>
<h2 id="保存和加载模型">3. 保存和加载模型</h2>
<p>PyTorch中保存和加载训练模型有两种常见的方法:</p>
<ol type="1">
<li>仅保存和加载模型参数(state_dict)；</li>
<li>保存和加载整个模型。</li>
</ol>
<h3 id="保存和加载state_dict推荐方式">3.1
保存和加载state_dict(推荐方式)</h3>
<p>保存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(net.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是pt或pth</span></span><br></pre></td></tr></table></figure>
<p>加载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = TheModelClass()</span><br><span class="line"></span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<h3 id="保存和加载整个模型">3.2 保存和加载整个模型</h3>
<p>保存：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(net, PATH)</span><br></pre></td></tr></table></figure>
<p>加载：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = torch.load(PATH)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第4节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.1 二维卷积层</title>
    <url>/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>卷积神经网络（convolutional neural
network）是含有卷积层（convolutional
layer）的神经网络。本章中介绍的卷积神经网络均使用最常见的二维卷积层。它有高和宽两个空间维度，常用来处理图像数据。</p>
<h2 id="二维互相关运算">1. 二维互相关运算</h2>
<p>举例说明，输入是一个高和宽均为3的二维数组，也就是我们的图像。卷积核窗口是2×2的二维数组。</p>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526153341222.png" style="zoom:67%;"></p>
<p><code>0×0 + 1×1 + 3×2 + 4×3 = 19</code></p>
<p>往右移动一步</p>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526153439256.png" style="zoom:67%;"></p>
<p><code>1×0 + 2×1 + 4×2 + 5×3 = 25</code></p>
<p>第二行开始（初始位置往下移动一步）</p>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526153605435.png" style="zoom:67%;"></p>
<p><code>3×0 + 4×1 + 6×2 + 7×3 = 37</code></p>
<p>同理继续往右移动一步</p>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526153656460.png" style="zoom:67%;"></p>
<p><code>4×0 + 5×1 + 7×2 + 8×3 = 43</code></p>
<p>下面我们将上述过程实现在corr2d函数里。它接受输入数组X与核数组K，并输出数组Y。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  </span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i: i + h, j: j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<p>同大小的二维数组相乘，则结果是每个对应的数相乘结果组成的新的同大小二维数组，和我们之前的输入与核的计算一样，最后通过sum()函数将改二维数组每个数相加得到我们的最终结果。</p>
<p>构造之前图中的输入数组X、核数组K来验证二维互相关运算的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">Y= corr2d(X, K)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526154914067.png" style="zoom:67%;"></p>
<h2 id="二维卷积层">2. 二维卷积层</h2>
<p>二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。</p>
<p><strong>卷积层</strong>的模型参数包括了<strong>卷积核</strong>和<strong>标量偏差</strong>。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。</p>
<p>下面基于corr2d函数来实现一个自定义的二维卷积层。（能看到偏差是最终结果的二维数组所有数都加上的同一数字）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Conv2D, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(kernel_size))</span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.randn(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, <span class="variable language_">self</span>.weight) + <span class="variable language_">self</span>.bias</span><br></pre></td></tr></table></figure>
<h2 id="图像中物体边缘检测">3. 图像中物体边缘检测</h2>
<p>下面我们来看一个卷积层的简单应用：检测图像中物体的边缘，即找到像素变化的位置。</p>
<p>首先我们构造一张6×8
的图像（即高和宽分别为6像素和8像素的图像）。它中间4列为黑（0），其余为白（1）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.ones(<span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526155821409.png" style="zoom:67%;"></p>
<p>然后我们构造一个高和宽分别为1和2的卷积核K。当它与输入做互相关运算时，如果横向相邻元素相同，输出为0；否则输出为非0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = torch.tensor([[<span class="number">1</span>, -<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<p>下面将输入X和我们设计的卷积核K做互相关运算。可以看出，我们将从白到黑的边缘和从黑到白的边缘分别检测成了1和-1。其余部分的输出全是0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line"><span class="built_in">print</span>(Y)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526160102909.png" style="zoom:67%;"></p>
<p>由此，我们可以看出，卷积层可通过重复使用卷积核有效地表征局部空间。</p>
<h2 id="通过数据学习核数组">4. 通过数据学习核数组</h2>
<p>最后我们来看一个例子，它使用物体边缘检测中的输入数据X和输出数据Y来学习我们构造的核数组K。</p>
<p>我们首先构造一个卷积层，其卷积核将被初始化成随机数组。</p>
<p>接下来在每一次迭代中，我们使用平方误差来比较Y和卷积层的输出，然后计算梯度来更新权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个核数组形状是(1, 2)的二维卷积层</span></span><br><span class="line">conv2d = Conv2D(kernel_size=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">step = <span class="number">20</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = ((Y_hat - Y) ** <span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    conv2d.weight.data -= lr * conv2d.weight.grad</span><br><span class="line">    conv2d.bias.data -= lr * conv2d.bias.grad</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度清0</span></span><br><span class="line">    conv2d.weight.grad.data.zero_()</span><br><span class="line">    conv2d.bias.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Step %d, loss %.3f&#x27;</span> % (i + <span class="number">1</span>, l.item()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526164545820.png" style="zoom:67%;"></p>
<p>可以看到，20次迭代后误差已经降到了一个比较小的值。现在来看一下学习到的卷积核的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weight: &quot;</span>, conv2d.weight.data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bias: &quot;</span>, conv2d.bias.data)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526164643133.png" style="zoom:67%;"></p>
<p>可以看到，学到的卷积核的权重参数与我们之前定义的核数组K较接近，而偏置参数接近0。</p>
<h2 id="互相关运算和卷积运算">5. 互相关运算和卷积运算</h2>
<p>假设有两个离散信号（或函数）：</p>
<p>• 输入信号 <code>f(n)</code></p>
<p>• 核（滤波器）<code>g(n)</code></p>
<p>互相关 <code>f ⋆ g</code>
是滑动匹配计算，不翻转核。（就是我们之前滑动窗口进行的计算）</p>
<p>卷积 <code>f ∗ g</code>
需要先翻转核再计算，因此结果可能不同。（翻转核指上下左右反转后的核）</p>
<p>在深度学习（CNN）中，通常用互相关（没有翻转），但仍然称其为“卷积”。</p>
<p>那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中<strong>核数组都是学出来的</strong>：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。</p>
<p>在深度学习中，无论使用互相关运算还是标准的数学卷积运算，<strong>神经网络都能通过学习调整核数组</strong>，使得预测结果保持一致。因此，这两种操作不会影响最终的模型输出。</p>
<p>（意思就是，如果你实际使用互相关运算得出Y，那么利用该运算中的输入X、输出Y，神经网络还是能拟合出实际利用的核K。而如果你实际使用数学卷积运算得出Y，同理神经网络还是能拟合出实际运算中使用的翻转核K。从而达到实际运算结果与神经网络输出的结果一致）</p>
<p>因此，为了与大多数深度学习文献一致，如无特别说明，本书中提到的卷积运算均指互相关运算。</p>
<h2 id="特征图和感受野">6. 特征图和感受野</h2>
<p>二维<strong>卷积层输出</strong>的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫<strong>特征图（feature
map）</strong>。</p>
<p>影响特征图元素<code>x</code>的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做<code>x</code>的<strong>感受野（receptive
field）</strong>。</p>
<p>以图5.1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。</p>
<p><img src="/2025/05/26/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-1/image-20250526170139035.png" style="zoom:67%;"></p>
<p>我们将图5.1中形状为<code>2×2</code>的输出记为<code>Y</code>，并考虑一个更深的卷积神经网络：将<code>Y</code>与另一个形状为<code>2×2</code>的核数组做互相关运算，输出单个元素<code>z</code>。</p>
<p>那么，<code>z</code>在<code>Y</code>上的感受野包括Y的全部四个元素，在输入上的感受野包括其中全部<code>9</code>个元素。</p>
<p>可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 4.6 GPU计算</title>
    <url>/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>在本节中，我们将介绍如何使用单块NVIDIA
GPU来计算。所以需要确保已经安装好了PyTorch GPU版本。</p>
<h2 id="计算设备">1. 计算设备</h2>
<p>通过nvidia-smi命令来查看显卡信息了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!nvidia-smi  # 对Linux/macOS用户有效</span><br><span class="line"></span><br><span class="line">nvidia-smi   # windows中不用感叹号</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520204608894.png" style="zoom:67%;"></p>
<p>我的cuda版本是12.7</p>
<p>访问以下链接获取对面GPU版本的pytorch</p>
<p>https://pytorch.org/</p>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520204950506.png" style="zoom:67%;"></p>
<p>这里 cuda 11.8&lt; 我的cuda 12.7，因此能用</p>
<p>直接复制命令运行</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 </span><br></pre></td></tr></table></figure>
<p>要卸载还原成原来的cpu版本的话 (不加 --index-url 就会默认安装 CPU
版本)</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">pip3 uninstall torch torchvision torchaudio</span><br><span class="line"></span><br><span class="line">pip3 install torch torchvision torchaudio</span><br></pre></td></tr></table></figure>
<p>检查是否成功</p>
<p>用 <code>torch.cuda.is_available()</code> 查看GPU是否可用:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.cuda.is_available()) <span class="comment"># 输出 True</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520205653645.png" style="zoom:67%;"></p>
<p>查看GPU数量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.device_count()) <span class="comment"># 输出 1</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520205845026.png" style="zoom:67%;"></p>
<p>查看当前GPU索引号，索引号从0开始：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.current_device()) <span class="comment"># 输出 0</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520205937169.png" style="zoom:67%;"></p>
<p>根据索引号查看GPU名字:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.cuda.get_device_name(<span class="number">0</span>)) <span class="comment"># 输出 &#x27;GeForce GTX 1050&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250520210547713.png" style="zoom:67%;"></p>
<h2 id="tensor的gpu计算">2 Tensor的GPU计算</h2>
<p>默认情况下，Tensor会被存在内存上。因此，之前我们每次打印Tensor的时候看不到GPU相关标识。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521144625909.png" style="zoom:67%;"></p>
<p>使用.cuda()可以将CPU上的Tensor转换（复制）到GPU上。如果有多块GPU，我们用.cuda(i)来表示第
i 块GPU及相应的显存（i从0开始）且cuda(0)和cuda()等价。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521144917473.png" style="zoom:67%;"></p>
<p>我们可以通过Tensor的device属性来查看该Tensor所在的设备。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521145624314.png" style="zoom:67%;"></p>
<p>如果按照原来的则显示cpu。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.device)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521145807419.png" style="zoom:67%;"></p>
<p>我们可以直接在创建的时候就指定设备。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=device)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).to(device)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521145912610.png" style="zoom:67%;"></p>
<p>如果对在GPU上的数据进行运算，那么结果还是存放在GPU上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=device)</span><br><span class="line">y = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521150223018.png" style="zoom:67%;"></p>
<p>需要注意的是，<strong>存储在不同位置中的数据是不可以直接进行计算的</strong>。即存放在CPU上的数据不可以直接与存放在GPU上的数据进行运算，位于不同GPU上的数据也是不能直接进行计算的。</p>
<p>以下计算会报错</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = y + x.cpu()</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521150450437.png" style="zoom:67%;"></p>
<h2 id="模型的gpu计算">3 模型的GPU计算</h2>
<p>同Tensor类似，PyTorch模型也可以通过.cuda转换到GPU上。我们可以通过检查模型的参数的device属性来查看存放模型的设备。</p>
<p>可见模型在CPU上。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521152306657.png" style="zoom:67%;"></p>
<p>将其转换到GPU上:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Linear(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">net.cuda()</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/05/20/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A04-6/image-20250521152407741.png" style="zoom:67%;"></p>
<p>同样的，我么需要保证<strong>模型输入的Tensor</strong>和<strong>模型</strong>都在同一设备上，否则会报错。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第4节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.10 批量归一化</title>
    <url>/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>本节我们介绍批量归一化（batch
normalization）层，它能让较深的神经网络的训练变得更加容易。</p>
<p>在3.16节（实战Kaggle比赛：预测房价）里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的<code>均值为0</code>、<code>标准差为1</code>。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p>
<p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但<strong>对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化</strong>。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p>
<p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。</p>
<h2 id="批量归一化层">1. 批量归一化层</h2>
<p>对<code>全连接层</code>和<code>卷积层</code>做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p>
<h3 id="对全连接层做批量归一化">1.1 对全连接层做批量归一化</h3>
<p>我们先考虑如何对全连接层做批量归一化。</p>
<p>通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。（人话来说就是放在激活函数层前，或者是计算网络层后（如卷积层或者全连接层后））</p>
<p>设全连接层的输入为u，权重参数和偏差参数分别为W和b，激活函数为ϕ。设批量归一化的运算符为BN。那么，使用批量归一化的全连接层的输出为
<span class="math display">\[
ϕ(BN(x))
\]</span> 其中批量归一化输入x由仿射变换 <span class="math display">\[
x=Wu+b
\]</span>
得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量<span class="math inline">\(B={x^{(1)},…,x^{(m)}}\)</span>。它们正是批量归一化层的输入。对于小批量B中任意样本<span class="math inline">\(x^{(i)}\)</span>，批量归一化层的输出同样是d维向量
<span class="math display">\[
y^{(i)}=BN(x^{(i)})
\]</span> 以下是BN具体的计算内容。</p>
<p>首先，对小批量B求<code>均值</code>和<code>方差</code>： <span class="math display">\[
\begin{align}
μ_B = \frac{1}{m} \sum ^{m}_{i=1} {x^{(i)}}\\
σ_B^2 = \frac{1}{m} \sum ^{m}_{i=1} {(x^{(i)}-μ_B)}^2
\end{align}
\]</span>
其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对<span class="math inline">\(x^{(i)}\)</span>标准化： <span class="math display">\[
\hat{x}^{(i)} ←\frac{x^{(i)}-μ_B}{\sqrt {σ_B^2+ϵ}}
\]</span> 这里ϵ&gt;0是一个很小的常数，保证分母大于0。</p>
<p>在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，<code>拉伸（scale）参数 γ</code>
和<code>偏移（shift）参数 β</code> 。</p>
<p>这<strong>两个参数</strong>和<span class="math inline">\(x^{(i)}\)</span>形状相同，皆为<strong>d维向量</strong>。它们与<span class="math inline">\(x^{(i)}\)</span>分别做按元素乘法（符号⊙）和加法计算：</p>
<p><span class="math display">\[
y^{(i)} = γ⊙\hat{x}^{(i)}+β
\]</span> 至此，我们得到了<span class="math inline">\(x^{(i)}\)</span>的批量归一化的输出<span class="math inline">\(y^{(i)}\)</span> 。
值得注意的是，可学习的拉伸和偏移参数保留了不对<span class="math inline">\(x^{(i)}\)</span>做批量归一化的可能。此时只需学出 $
γ = $ 和 $β = μ_B $ 。</p>
<p>即将该两参数带入公式 <span class="math inline">\(\hat{x}^{(i)}
←\frac{x^{(i)}-μ_B}{\sqrt {σ_B^2+ϵ}}\)</span>
中刚好消除归一化计算添加的两个内容</p>
<h3 id="对卷积层做批量归一化">1.2 对卷积层做批量归一化</h3>
<p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。</p>
<p>如果卷积计算输出<strong>多个通道</strong>，我们需要对这些通道的输出<strong>分别做批量归一化</strong>，且每个通道都<strong>拥有独立的拉伸和偏移参数</strong>，并均为<strong>标量</strong>。</p>
<p>设小批量中有m个样本。在<strong>单个通道上</strong>，假设卷积计算输出的高和宽分别为p和q。我们需要对该通道中<code>m×p×q</code>个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即<strong>该通道中<code>m×p×q</code>个元素的均值和方差</strong>。</p>
<h3 id="预测时的批量归一化">1.3 预测时的批量归一化</h3>
<p>使用批量归一化训练时，我们<strong>可以将批量大小设得大一点</strong>，从而使批量内样本的均值和方差的计算都较为准确。</p>
<p>将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。</p>
<blockquote>
<p>🔧 举个例子：</p>
<p>假设我们训练图像分类模型时使用 BatchNorm：</p>
<p>训练时：</p>
<p>​ 输入一个 batch（比如32张图片），BatchNorm 会用这 32
张图的均值和方差做归一化。</p>
<p>​ 同时，它会更新一个 “移动平均” 的均值和方差（全局估计）。</p>
<p>预测时：</p>
<p>​ 输入一张图。不能用这一张图的均值和方差归一化（不准且不稳定）。</p>
<p>​ 要使用训练过程中积累下来的“全局均值和方差”。</p>
</blockquote>
<blockquote>
<p>回到实际中，归一化层 BatchNorm 中有两类“内容”</p>
<ol type="1">
<li>可学习的缩放系数 γ（gamma）和偏移量 β（beta）</li>
<li>非学习型的“均值和方差”</li>
</ol>
<p>训练时： 每个 mini-batch 动态计算 μ 和 σ（所以叫 Batch
Normalization）。</p>
<p>预测时： 没有
mini-batch（或batch太小不准），所以不能再用它们，只能用训练阶段保存下来的“全局均值和方差”。</p>
</blockquote>
<p>可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p>
<h2 id="从零开始实现">2. 从零开始实现</h2>
<p>下面我们自己实现批量归一化层。</p>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250729160526941.png"></p>
<p>代码整体易懂，就只说下两个框的意思。</p>
<p>第一个比较好懂：<code>assert len(X.shape) in (2, 4)</code> 指的是
判断X是否是2维或者4维。如果不是这两种情况则中断程序。</p>
<p>第二个比较复杂点。</p>
<blockquote>
<p>首先举个例子</p>
<p>假设X为 (3,4)大小的二维矩阵，执行X.mean(dim=0)，则意味着
X仅修改第0维的数据，即大小修改为
(1,N)，这里的N指的是原本维度所在的数字的大小，在这例子中是4，则变成(1,4)，由于并没有添加
keepdim=True
选项，所以除了N以外的其他维度会被消除，其结果大小变成(4)，如果加了
keepdim=True，则大小会保持为(1,4)。</p>
</blockquote>
<p>回到代码中
<code>X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)</code>
X原本大小为 <code>(N,C,W,H)</code>,以上结果相当于
<code>mean = X.mean(dim=(0, 2, 3), keepdim=True)</code> X大小变成
<code>(1,C,1,1)</code>
，即，X中所有样本上某一通道上所有值加起来进行求平均，</p>
<p>接下来，我们自定义一个BatchNorm层。它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta，同时也维护移动平均得到的均值和方差，以便能够在模型预测时被使用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>(BatchNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1</span></span><br><span class="line">        <span class="variable language_">self</span>.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        <span class="variable language_">self</span>.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># 不参与求梯度和迭代的变量，全在内存上初始化成0</span></span><br><span class="line">        <span class="variable language_">self</span>.moving_mean = torch.zeros(shape)</span><br><span class="line">        <span class="variable language_">self</span>.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.moving_mean.device != X.device:</span><br><span class="line">            <span class="variable language_">self</span>.moving_mean = <span class="variable language_">self</span>.moving_mean.to(X.device)</span><br><span class="line">            <span class="variable language_">self</span>.moving_var = <span class="variable language_">self</span>.moving_var.to(X.device)</span><br><span class="line">        <span class="comment"># 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false</span></span><br><span class="line">        Y, <span class="variable language_">self</span>.moving_mean, <span class="variable language_">self</span>.moving_var = batch_norm(<span class="variable language_">self</span>.training,</span><br><span class="line">            X, <span class="variable language_">self</span>.gamma, <span class="variable language_">self</span>.beta, <span class="variable language_">self</span>.moving_mean,</span><br><span class="line">            <span class="variable language_">self</span>.moving_var, eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure>
<h3 id="使用批量归一化层的lenet">2.1 使用批量归一化层的LeNet</h3>
<p>下面我们修改5.5节（卷积神经网络（LeNet））介绍的LeNet模型，从而应用批量归一化层。我们在所有的卷积层或全连接层之后、激活层之前加入批量归一化层。</p>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250729212201317.png"></p>
<p>下面我们训练修改后的模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250729213116201.png" style="zoom:67%;"></p>
<p>最后我们查看第一个批量归一化层学习到的拉伸参数gamma和偏移参数beta。</p>
<p>我们知道卷积层的拉伸跟偏移参数数量只和通道数有关（二维就只和fetures数量有关）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">1</span>].gamma.view((-<span class="number">1</span>,)))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">1</span>].beta.view((-<span class="number">1</span>,)))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250729213332131.png" style="zoom: 67%;"></p>
<h2 id="简洁实现">3. 简洁实现</h2>
<p>与我们刚刚自己定义的BatchNorm类相比，Pytorch中nn模块定义的<code>BatchNorm1d</code>和<code>BatchNorm2d</code>类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的num_features参数值。下面我们用PyTorch实现使用批量归一化的LeNet。</p>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250729214250581.png"></p>
<p>使用同样的超参数进行训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-10/image-20250730113605708.png" style="zoom:80%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.11 残差网络（ResNet）</title>
    <url>/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<hr>
<p>简单说残差网络是一种 2维图像方面加速学习的方法。</p>
<p>让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？</p>
<p>理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射f(x)=x，新模型和原模型将同样有效。</p>
<p>由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。</p>
<p>即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。</p>
<h2 id="残差块">1. 残差块</h2>
<p>让我们聚焦于神经网络局部。如图5.9所示，设输入为x。假设我们希望学出的理想映射为f(x)，从而作为图5.9上方激活函数的输入。</p>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821103546179.png" style="zoom:67%;"></p>
<p><strong>左图虚线框</strong> 中的部分需要直接拟合出该
<strong>映射f(x)</strong>。</p>
<p><strong>右图虚线框</strong> 中的部分则需要拟合出有关恒等映射的残差
<strong>映射f(x)−x</strong>。</p>
<p>残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射f(x)。我们只需将图5.9中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，</p>
<p>实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。图5.9右图也是ResNet的基础块，即<strong>残差块（residual
block）</strong>。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p>
<blockquote>
<p>举个例子。</p>
<p>左图原本训练的目标函数为f(x)= f1(x)，右边残差训练的函数为 f(x) =
f1(x) = f2(x)+x 。那么我们残差块训练的目标函数是 f2(x)</p>
<p>假如目标函数是恒等函数
f(x)=x，那么我们目标函数中所有权重和偏差为0得到 f2(x)=0 即可。</p>
<p>残差块训练的是 <strong>x</strong> 与 <strong>目标函数f(x)</strong>
之间相差的那一块，简单理解如果x与目标f(x)相近，我们只需要训练一点残差值即可，哲学上来说：<strong>如果你已经做得差不多了，那就别从头再来——只需要学会如何补上那一小步</strong>。</p>
<p>你可以理解：x是一张
猫狗特征图片，要求输出猫的特征，那么与其从0开始训练这个网络层，不如<strong>在原有的图片基础上进行删改</strong>，只留下猫的特征。</p>
<p>可以从直觉上感受到工作量减少了，因此加速了收敛。</p>
<p>残差网络还有重要的一点经验性假设：在很多实际任务中，神经网络每一层的输出与输入之间差异往往不大（即“相近”）。</p>
<p>•
在图像处理中，连续的卷积层往往只做一些局部特征提取、微调，不会让特征发生翻天覆地的变化。</p>
<p>•
在语言模型、时间序列模型中，特征的逐层表示也通常是逐步演化，而非完全替换。</p>
<p>•
因此：直接让网络只学习“如何改进”输入，而不是重新建造一切，更高效、更稳定。</p>
<p>但是这不是绝对的，因此残差网络变种中也有在最后一步 f(x)=x+f2(x)
这给x加上可学习参数的权重变成 f(x)=wx+f2(x) 。</p>
</blockquote>
<p>ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。</p>
<p>然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。</p>
<p>残差块的实现如下。它可以设定输出通道数、是否使用额外的1×1卷积层来修改通道数以及卷积层的步幅。</p>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821105910068.png"></p>
<p>下面我们来查看输入和输出形状一致的情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="built_in">print</span>(blk(X).shape)  <span class="comment"># torch.Size([4, 3, 6, 6])</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821110855914.png" style="zoom: 80%;"></p>
<p>我们也可以在增加输出通道数的同时减半输出的高和宽。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>)</span><br><span class="line">X = torch.rand((<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>))</span><br><span class="line"><span class="built_in">print</span>(blk(X).shape)  <span class="comment"># torch.Size([4, 6, 3, 3])</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821112227133.png" style="zoom:80%;"></p>
<h2 id="resnet模型">2. ResNet模型</h2>
<p>ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>GoogLeNet在后面接了4个由Inception块组成的模块。ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。<strong>第一个模块的通道数同输入通道数一致</strong>。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。</p>
<p>下面我们来实现这个模块。注意，这里对第一个模块做了特别处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>ResNet 第一模块输入输出通道一致，并不是卷积本身的限制，而是
残差恒等映射的设计选择。</p>
<p>简单说，就是残差块的最后一步需要与x相加，如果通道数一样的话，就可以直接相加，而通道数不一样的话，需要对x进行1x1网络的处理。</p>
<p>在网络的早期（尤其是第一组 block），作者希望 shortcut
(即最终与f(x)相加的x)
保持纯粹的恒等映射（identity），也就是不引入任何新参数。这样做的好处是：当残差分支刚初始化时，网络至少可以退化为“恒等映射”，避免训练困难。</p>
</blockquote>
<p>接着我们为ResNet加入所有残差块。这里每个模块使用两个残差块。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">&quot;resnet_block1&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>最后，与GoogLeNet一样，加入全局平均池化层后接上全连接层输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line"></span><br><span class="line">net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<p>这里每个模块里有4个卷积层（不计算1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。</p>
<p>通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。</p>
<p>虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。</p>
<p>在训练ResNet之前，我们来观察一下输入形状在ResNet不同模块之间的变化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27; output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821142400556.png" style="zoom:80%;"></p>
<h2 id="获取数据和训练模型">3. 获取数据和训练模型</h2>
<p>下面我们在Fashion-MNIST数据集上训练ResNet。</p>
<p>确实没想过效果这么好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/21/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-11/image-20250821145222455.png" style="zoom:80%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.12 稠密连接网络（DenseNet）</title>
    <url>/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<hr>
<p>ResNet中的跨层连接设计引申出了数个后续工作。本节我们介绍其中的一个：稠密连接网络（DenseNet）
[1]。 它与ResNet的主要区别如图5.10所示。</p>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824183325054.png"></p>
<p>与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是<strong>在通道维上连结</strong>。这样模块A的输出可以直接传入模块B后面的层。</p>
<p>DenseNet的主要构建模块是<strong>稠密块（dense
block）</strong>和<strong>过渡层（transition
layer）</strong>。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h2 id="稠密块">1. 稠密块</h2>
<p>DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构，我们首先在conv_block函数里实现这个结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(in_channels),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>稠密块由多个conv_block组成，每块使用相同的输出通道数。但在前向计算时，我们将每块的输入和输出在通道维上连结。</p>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824184750979.png"></p>
<p>简单理解模型组成</p>
<p>第一层 net( input , ouput)</p>
<p>第二层 net( input + output , output )</p>
<p>第三层 net( input + 2*output , output )</p>
<p>每层的作为输入的上一层输出都会叠加一次output。前向计算同理。</p>
<p>在下面的例子中，我们定义一个有<code>2</code>个<code>输出通道数为10</code>的卷积块。使用<code>通道数为3的输入</code>时，我们会得到通道数为<code>3+2×10 = 23</code>的输出。卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为增长率（growth
rate）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = DenseBlock(<span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(Y.shape) <span class="comment"># torch.Size([4, 23, 8, 8])</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824190039748.png" style="zoom: 67%;"></p>
<h2 id="过渡层">2. 过渡层</h2>
<p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。</p>
<p>它<strong>通过1×1卷积层来减小通道数</strong>，并使用<strong>步幅为2的平均池化层减半高和宽</strong>，从而进一步降低模型复杂度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transition_block</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>对上一个例子中稠密块的输出使用通道数为10的过渡层。此时输出的通道数减为10，高和宽均减半。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">blk = transition_block(<span class="number">23</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(blk(Y).shape) <span class="comment"># torch.Size([4, 10, 4, 4])</span></span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824191908379.png" style="zoom:67%;"></p>
<h2 id="densenet模型">3. DenseNet模型</h2>
<p>我们来构造DenseNet模型。DenseNet首先使用同ResNet一样的单卷积层和最大池化层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。</p>
<p>同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与上一节的ResNet-18保持一致。</p>
<p>稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">&quot;DenseBlosk_%d&quot;</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">&quot;transition_block_%d&quot;</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>同ResNet一样，最后接上全局池化层和全连接层来输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net.add_module(<span class="string">&quot;BN&quot;</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">&quot;relu&quot;</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>)))</span><br></pre></td></tr></table></figure>
<p>我们尝试打印每个子模块的输出维度确保网络无误：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27;\t\toutput shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824193718653.png" style="zoom:67%;"></p>
<h2 id="获取数据并训练模型">4. 获取数据并训练模型</h2>
<p>由于这里使用了比较深的网络，本节里我们将输入高和宽从224降到96来简化计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/24/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-12/image-20250824194932283.png" style="zoom:67%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.2 填充和步幅</title>
    <url>/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>在上一节的例子里，我们使用高和宽为<code>3</code>的输入与高和宽为<code>2</code>的卷积核得到高和宽为<code>2</code>的输出。</p>
<p>一般来说，假设输入形状是 <span class="math inline">\(n_h*n_w\)</span>
，卷积核窗口形状是 <span class="math inline">\(k_h*k_w\)</span>
，那么输出形状将会是<br>
<span class="math display">\[
(n_h-k_h+1)*(n_w-k_w+1)
\]</span>
所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p>
<h2 id="填充">1. 填充</h2>
<p>填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）</p>
<p>图5.2里我们在原输入高和宽的两侧分别添加了值为<code>0</code>的元素，使得输入高和宽从<code>3</code>变成了<code>5</code>，并导致输出高和宽由<code>2</code>增加到<code>4</code>。</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250527180720465.png" style="zoom:67%;"></p>
<p>一般来说，如果在高的两侧一共填充 <span class="math inline">\(p_h\)</span> 行，在宽的两侧一共填充 <span class="math inline">\(p_w\)</span> 列，那么输出形状将会是 <span class="math display">\[
(n_h-k_h+p_h+1)*(n_w-k_w+p_w+1)
\]</span> 也就是说，输出的高和宽会分别增加 <span class="math inline">\(p_h\)</span> 和 <span class="math inline">\(p_w\)</span> 。</p>
<p>在很多情况下，我们会设置 <span class="math inline">\(p_h =
k_h-1\)</span> 和 <span class="math inline">\(p_w = k_w-1\)</span>
来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。</p>
<p>假设 <span class="math inline">\(k_h\)</span> 是奇数，（那么 <span class="math inline">\(k_h-1\)</span>
就是偶数），我们会在高的<strong>两侧</strong>分别填充 <span class="math inline">\(\frac{p_h}{2}\)</span> 行 。</p>
<p>假设 <span class="math inline">\(k_h\)</span> 是偶数，（那么 <span class="math inline">\(k_h-1\)</span>
就是奇数），我们会在高或底的<strong>某一侧</strong>填充 <span class="math inline">\(\frac{p_h}{2}\)</span> 行 。</p>
<p>对于宽 <span class="math inline">\(k_w\)</span> 同理。</p>
<p>卷积神经网络经常使用奇数高宽的卷积核，如<code>1、3、5</code>和<code>7</code>，所以两端上的填充个数相等。</p>
<p>下面的例子里我们创建一个高和宽为<code>3</code>的二维卷积层，然后设输入高和宽两侧的填充数分别为<code>1</code>。给定一个高和宽为<code>8</code>的输入，我们发现输出的高和宽也是<code>8</code>。</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528104734786.png" style="zoom: 60%;"></p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528104814360.png" style="zoom:80%;"></p>
<p>当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。</p>
<p>在行的部分添加4行，即两侧分别添加2行，列的部分同理两侧添加1列（注意这里卷积核是<code>5x3</code>）</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528105241598.png" style="zoom:80%;"></p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528105501575.png" style="zoom: 80%;"></p>
<h2 id="步幅">2. 步幅</h2>
<p>我们将每次滑动的行数和列数称为步幅（stride）。目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528105555220.png" style="zoom: 80%;"></p>
<p>一般来说，当高上步幅为 <span class="math inline">\(s_h\)</span>
，宽上步幅为 <span class="math inline">\(s_w\)</span>
时，输出形状为（注意这里不是中括号，而是向下取整符号） <span class="math display">\[
\lfloor (n_h-k_h+p_h+s_h)/s_h\rfloor  * \lfloor
(n_w-k_w+p_w+s_w)/s_w\rfloor
\]</span> 如果我们设置 <span class="math inline">\(p_h = k_h-1\)</span>
和 <span class="math inline">\(p_w = k_w-1\)</span> ，么输出形状将简化为
<span class="math display">\[
\lfloor (n_h+s_h-1)/s_h\rfloor  * \lfloor (n_w+s_w-1)/s_w\rfloor
\]</span>
更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是
<span class="math display">\[
\lfloor n_h/s_h\rfloor  * \lfloor n_w/s_w\rfloor
\]</span></p>
<blockquote>
<p><span class="math inline">\(\lfloor (s_h-1)/s_h\rfloor\)</span>
为<code>0</code></p>
</blockquote>
<p>下面我们令高和宽上的步幅均为<code>2</code>，从而使输入的高和宽减半。</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528121446020.png" style="zoom: 80%;"></p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528121507764.png" style="zoom:80%;"></p>
<p>同理可以分别设置高和宽上的步幅</p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528121608852.png" style="zoom:80%;"></p>
<p><img src="/2025/05/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-2/image-20250528121620898.png" style="zoom:80%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.4 池化层</title>
    <url>/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>池化（Pooling）就是在降低特征图尺寸的同时，保留重要的特征（比如纹理、边缘、轮廓、最大响应）并提升模型的鲁棒性（不敏感于位置、旋转、缩放等）</p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623112632220.png" style="zoom: 50%;"></p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623112705786.png" style="zoom:50%;"></p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623120442560.png" style="zoom:50%;"></p>
<p>池化层只能容忍
小幅度的平移，对大幅度的偏移是无能为力的。所以并不用在意这个东西，<strong>池化层主要就是减少后续需要处理的参数，防止参数爆炸，从而加快训练，其他都是额外的</strong>。</p>
<h2 id="二维最大池化层和平均池化层">1. 二维最大池化层和平均池化层</h2>
<p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。</p>
<p>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。</p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623120649121.png" style="zoom: 50%;"></p>
<p>二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。</p>
<p>下面把池化层的前向计算实现在pool2d函数里。它跟5.1节（二维卷积层）里corr2d函数非常类似，唯一的区别在计算输出Y上。</p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623121525470.png" style="zoom: 67%;"></p>
<p>我们可以构造图5.6中的输入数组X来验证二维最大池化层的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">res = pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623121701427.png" style="zoom: 67%;"></p>
<h2 id="填充和步幅">2. 填充和步幅</h2>
<p>卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。</p>
<p>我们将通过nn模块里的二维最大池化层MaxPool2d来演示池化层填充和步幅的工作机制。我们先构造一个形状为<code>(1, 1, 4, 4)</code>的输入数据，前两个维度分别是<strong>批量（batch）</strong>和<strong>通道（chanel）</strong>（池化函数的输入格式）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.<span class="built_in">float</span>).view((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623160446133.png" style="zoom:67%;"></p>
<p>默认情况下，MaxPool2d实例里步幅和池化窗口形状相同。下面使用形状为<code>(3, 3)</code>的池化窗口，默认获得形状为<code>(3, 3)</code>的步幅。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">res = pool2d(X)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623122738687.png" style="zoom:67%;"></p>
<p>我们可以手动指定步幅和填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">res = pool2d(X)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623122956880.png" style="zoom:67%;"></p>
<p>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">4</span>), padding=(<span class="number">1</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line">res = pool2d(X)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><code>padding=(1, 2)</code> 指的是上下添加1行，左右添加2列</p>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623155026386.png" style="zoom:67%;"></p>
<h2 id="多通道">3. 多通道</h2>
<p>在处理多通道输入数据时，池化层<strong>对每个输入通道分别池化</strong>，而<strong>不是像卷积层那样将各通道的输入按通道相加</strong>。</p>
<p>下面将数组X和X+1在通道维上连结来构造通道数为2的输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623155458878.png" style="zoom: 67%;"></p>
<p>运行下面的池化代码后，我们发现输出通道数仍然是2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">r = pool2d(X)</span><br><span class="line"><span class="built_in">print</span>(r)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/23/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-4/image-20250623160101049.png" style="zoom:67%;"></p>
<p><strong>池化层的输出通道数跟输入通道数相同</strong>。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.5 卷积神经网络（LeNet）</title>
    <url>/2025/06/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>在3.9节（多层感知机的从零开始实现）里我们构造了一个含单隐藏层的多层感知机模型来对Fashion-MNIST数据集中的图像进行分类。每张图像高和宽均是28像素。我们<strong>将图像中的像素逐行展开，得到长度为784的向量，并输入进全连接层中</strong>。然而，这种分类方法有一定的局限性。</p>
<ol type="1">
<li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li>
<li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为1000像素的彩色照片（含3个通道）。即使全连接层输出个数仍是256，该层权重参数的形状是3,000,000×256：它占用了大约3
GB的内存或显存。这带来过复杂的模型和过高的存储开销。</li>
</ol>
<p>卷积层尝试解决这两个问题。</p>
<p>一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；</p>
<p>另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p>
<p>卷积神经网络就是含卷积层的网络。</p>
<p>本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet
。这个名字来源于LeNet论文的第一作者Yann LeCun。</p>
<p><img src="/2025/06/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-5/image-20250628184948273.png"></p>
<h2 id="lenet模型">1. LeNet模型</h2>
<p>LeNet分为 <strong>卷积层块</strong> 和 <strong>全连接层块</strong>
两个部分。</p>
<p>在卷积层块中，每个卷积层都使用5×5的窗口，并在输出上使用sigmoid激活函数。</p>
<p>第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。</p>
<p>卷积层块的两个最大池化层的窗口形状均为2×2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p>
<p>卷积层块的输出形状为<strong>(批量大小, 通道, 高,
宽)</strong>。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。</p>
<p>全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。</p>
<p>下面我们通过Sequential类来实现LeNet模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Sequential(</span><br><span class="line"></span><br><span class="line">            <span class="comment"># C1</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># S2</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># C3</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># S4</span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Sequential(</span><br><span class="line"></span><br><span class="line">            <span class="comment"># C5</span></span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># F6</span></span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># OUTPUT</span></span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = <span class="variable language_">self</span>.conv(img)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="获取数据和训练模型">2. 获取数据和训练模型</h2>
<p>下面我们来实验LeNet模型。实验中，我们仍然使用Fashion-MNIST作为训练数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure>
<p>因为卷积神经网络计算比多层感知机要复杂，建议使用GPU来加速计算。因此，我们对3.6节（softmax回归的从零开始实现）中描述的evaluate_accuracy函数略作修改，使其支持GPU计算。</p>
<p>PyTorch 要求张量在 CPU 上才能提取成普通数值，如果直接
.item()，会报错。所以必须先 .cpu() 把它搬回 CPU，然后 .item() 转成标准
Python 数值</p>
<p><img src="/2025/06/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-5/image-20250628190318749.png"></p>
<p>我们同样对3.6节中定义的train_ch3函数略作修改，确保计算使用的数据和模型同在内存或显存上。</p>
<p><img src="/2025/06/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-5/image-20250628190926912.png"></p>
<p>学习率采用0.001，训练算法使用Adam算法，损失函数使用交叉熵损失函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = LeNet()</span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/28/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-5/image-20250629171107629.png" style="zoom: 80%;"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.3 多输入通道和多输出通道</title>
    <url>/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。</p>
<p>例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是<code>h</code>和<code>w</code>（像素），那么它可以表示为一个<code>3×h×w</code>的多维数组。我们将大小为3的这一维称为通道（channel）维。</p>
<h2 id="多输入通道">1 多输入通道</h2>
<p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。</p>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616105416956.png" style="zoom:80%;"></p>
<p>接下来我们实现含多个输入通道的互相关运算。我们只需要对每个通道做互相关运算，然后通过<code>add_n</code>函数来进行累加。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 沿着X和K的第0维（通道维）分别计算再相加</span></span><br><span class="line">    res = d2l.corr2d(X[<span class="number">0</span>, :, :], K[<span class="number">0</span>, :, :])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, X.shape[<span class="number">0</span>]):</span><br><span class="line">        res += d2l.corr2d(X[i, :, :], K[i, :, :])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>这里<code>corr2d</code>互相关计算函数的步幅为1。</p>
<p>我们可以构造图5.4中的输入数组X、核数组K来验证互相关运算的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.tensor(</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">            [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]</span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">            [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">            [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">        ]</span><br><span class="line">    ])</span><br><span class="line">K = torch.tensor(</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">            [<span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">        ],</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">            [<span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">        ]</span><br><span class="line">    ])</span><br><span class="line">res = corr2d_multi_in(X, K)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616105524525.png" style="zoom:67%;"></p>
<h2 id="多输出通道">2 多输出通道</h2>
<p>我们之前使用<strong>一组核<code>K</code>仅获得一个输出</strong>，多输出的意思则是<strong>多组核</strong>获取多个输出</p>
<p>下面我们实现一个互相关运算函数来计算多个通道的输出。</p>
<p>下面的K中有多个一组核的k，多组核计算后的多个结果通过stack堆在在一个tensor中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起</span></span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br></pre></td></tr></table></figure>
<p>stack使用举例</p>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616165717244.png" style="zoom:67%;"></p>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616165737744.png" style="zoom:67%;"></p>
<p>我们将核数组<code>K</code>同<code>K+1</code>（<code>K</code>中每个元素加一）和<code>K+2</code>连结在一起来构造一个输出通道数为<code>3</code>的卷积核</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">K = torch.stack([K, K + <span class="number">1</span>, K + <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(K.shape) <span class="comment"># torch.Size([3, 2, 2, 2])</span></span><br></pre></td></tr></table></figure>
<p>下面我们对输入数组<code>X</code>与核数组<code>K</code>做互相关运算。此时的输出含有<code>3</code>个通道。其中第一个通道的结果与之前输入数组<code>X</code>与多输入通道、单输出通道核的计算结果一致。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">res = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616170111474.png" style="zoom:67%;"></p>
<h2 id="卷积层">3. 1×1卷积层</h2>
<p>最后我们讨论卷积窗口形状为<code>1×1</code>（<span class="math inline">\(k_h=k_w=1\)</span>）的多通道卷积层。我们通常称之为<code>1×1</code>卷积层，并将其中的卷积运算称为<code>1×1</code>卷积。
实际上，<code>1×1</code> 卷积的主要计算发生在通道维上。</p>
<p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。</p>
<p>输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。</p>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616172833910.png" style="zoom: 50%;"></p>
<p>下面我们使用全连接层中的矩阵乘法来实现<code>1×1</code>卷积。这里需要在矩阵乘法运算前后对数据形状做一些调整。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    X = X.view(c_i, h * w)</span><br><span class="line">    K = K.view(c_o, c_i)</span><br><span class="line"></span><br><span class="line">    Y = torch.mm(K, X)  <span class="comment"># 全连接层的矩阵乘法</span></span><br><span class="line">    <span class="keyword">return</span> Y.view(c_o, h, w)</span><br></pre></td></tr></table></figure>
<p>注意我们这里的矩阵乘法是 <span class="math inline">\(K*X\)</span>
,然后我们就清楚
<code>X = X.view(c_i, h * w); K = K.view(c_o, c_i)</code>
变换的意义。</p>
<p>最终我们将计算结果恢复成 (通道，高，宽) 的格式</p>
<p>做1×1
卷积时，以上函数与之前实现的互相关运算函数corr2d_multi_in_out等价。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">K = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((Y1 - Y2).norm().item() &lt; <span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-3/image-20250616173656293.png" style="zoom: 67%;"></p>
<p>之后的模型里我们将会看到1×1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.6 深度卷积神经网络（AlexNet）</title>
    <url>/2025/06/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-6/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>LeNet可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。</p>
<p>AlexNet跟LeNet结构类似，但使用了<strong>更多的卷积层</strong> 和
<strong>更大的参数空间</strong>来拟合大规模数据集ImageNet。它是浅层神经网络和深度神经网络的分界线。虽然看上去AlexNet的实现比LeNet的实现也就多了几行代码而已，但这个观念上的转变和真正优秀实验结果的产生令学术界付出了很多年。</p>
<h2 id="学习特征表示">1. 学习特征表示</h2>
<p>在相当长的时间里，特征都是基于各式各样手工设计的函数从数据中提取的。事实上，不少研究者通过提出新的特征提取函数不断改进图像分类结果。这一度为计算机视觉的发展做出了重要贡献。</p>
<p>另一些研究者则持异议。他们认为<strong>特征本身也应该由学习得来</strong>。他们还相信，为了表征足够复杂的输入，特征本身应该分级表示。持这一想法的研究者相信，多层神经网络可能可以学得数据的多级表征，并逐级表示越来越抽象的概念或模式。</p>
<h2 id="alexnet">2. AlexNet</h2>
<p>2012年，AlexNet横空出世。
AlexNet使用了8层卷积神经网络，并以很大的优势赢得了ImageNet
2012图像识别挑战赛。它首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。</p>
<p><img src="/2025/06/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-6/image-20250629182230918.png"></p>
<p>图中虽然分了两层，但只要看下面完整的一层就行了</p>
<p>AlexNet与LeNet的设计理念非常相似，但也有显著的区别。</p>
<p>第一，与相对较小的LeNet相比，AlexNet包含8层变换，其中有<strong>5层卷积</strong>和<strong>2层全连接隐藏层</strong>，以及<strong>1个全连接输出层</strong>。下面我们来详细描述这些层的设计。AlexNet第一层中的卷积窗口形状是11×11。因为ImageNet中绝大多数图像的高和宽均比MNIST图像的高和宽大10倍以上，ImageNet图像的物体占用更多的像素，第二层中的卷积窗口形状减小到5×5，之后全采用3×3。此外，第一、第二和第五个卷积层之后都使用了窗口形状为3×3、步幅为2的最大池化层。紧接着最后一个卷积层的是两个输出个数为4096的全连接层。这两个巨大的全连接层带来将近1
GB的模型参数。</p>
<p>第二，AlexNet将sigmoid激活函数改成了更加简单的<strong>ReLU激活函数</strong>。一方面，ReLU激活函数的计算更简单，例如它并没有sigmoid激活函数中的求幂运算。另一方面，ReLU激活函数在不同的参数初始化方法下使模型更容易训练。</p>
<p>第三，AlexNet通过<strong>丢弃法</strong>（参见3.13节）来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法。“模型复杂度”可以理解为模型的拟合能力。一个太复杂的模型容易“记住”训练数据，导致在新数据上表现差，即过拟合。因此这里的意思是通过丢弃法解决过拟合问题。</p>
<p>第四，AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。</p>
<p>下面我们实现稍微简化过的AlexNet。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, <span class="number">11</span>, <span class="number">4</span>), <span class="comment"># in_channels, out_channels, kernel_size, stride, padding</span></span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。</span></span><br><span class="line">            <span class="comment"># 前两个卷积层后不使用池化层来减小输入的高和宽</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">         <span class="comment"># 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合</span></span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line"></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = <span class="variable language_">self</span>.conv(img)</span><br><span class="line">        output = <span class="variable language_">self</span>.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>可以先通过以下代码确定每层输入输出是否有误</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input:&quot;</span>, x.shape)</span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.conv):</span><br><span class="line">        x = layer(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;After conv layer <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;After flatten:&quot;</span>, x.shape)</span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.fc):</span><br><span class="line">        x = layer(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;After fc layer <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;x.shape&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>填入测试数据查看结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = AlexNet()</span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># 例如用224x224输入</span></span><br><span class="line">output = net(input_tensor)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-6/image-20250629190645750.png" style="zoom:80%;"></p>
<p>下一节VGG网络中，我们会使用更加方便的工具进行网络层参数的审查</p>
<h2 id="读取数据">3. 读取数据</h2>
<p>虽然论文中AlexNet使用ImageNet数据集，但因为ImageNet数据集训练时间较长，我们仍用前面的Fashion-MNIST数据集来演示AlexNet。</p>
<p>读取数据的时候我们额外做了一步将图像高和宽扩大到AlexNet使用的图像高和宽224。这个可以通过torchvision.transforms.Resize实例来实现。<code>torchvision.transforms.Resize</code>
在放大图像时，会使用插值方式来填充像素，从而生成目标大小的图像。</p>
<p>也就是说，我们在ToTensor前使用Resize，然后使用Compose实例来将这两个变换串联以方便调用。</p>
<p><img src="/2025/06/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-6/image-20250630221813612.png"></p>
<h2 id="训练">4. 训练</h2>
<p>这时候我们可以开始训练AlexNet了。相对于LeNet，由于图片尺寸变大了而且模型变大了，所以需要更大的显存，也需要更长的训练时间了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/06/29/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-6/image-20250630225005781.png" style="zoom:80%;"></p>
<p>与LeNet相比提升了很多，当然训练所需时间也多了很多</p>
<h2 id="小结">5.小结</h2>
<p>LeNet是仅有2层卷积的网络，而AlexNet则可以理解为更多卷积层的LeNet网络。</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.7 使用重复元素的网络（VGG）</title>
    <url>/2025/07/09/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-7/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>AlexNet在LeNet的基础上增加了3个卷积层。但AlexNet作者对它们的卷积窗口、输出通道数和构造顺序均做了大量的调整。虽然AlexNet指明了深度卷积神经网络可以取得出色的结果，但并没有提供简单的规则以指导后来的研究者如何设计新的网络。</p>
<p>我们将在本章的后续几节里介绍几种不同的<strong>深度网络设计思路</strong>。</p>
<p>本节介绍VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group
[1]。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p>
<h2 id="vgg块">1. VGG块</h2>
<p>VGG块的组成规律是：连续使用数个相同的填充为<code>1</code>、窗口形状为<code>3×3</code>的卷积层后接上一个步幅为<code>2</code>、窗口形状为<code>2×2</code>的最大池化层。</p>
<p>简单来说，VGG块结构如下</p>
<ol type="1">
<li>n个卷积层：保持输入的高和宽不变</li>
<li>池化层：对其输出减半。</li>
</ol>
<p>对于<strong>给定的感受野</strong>（与输出有关的输入图片的局部大小），采用<strong>堆积的小卷积核</strong>优于采用大的卷积核，因为可以<strong>增加网络深度</strong>来保证学习更复杂的模式，而且代价还比较小（<strong>参数更少</strong>）。</p>
<p>在VGG中</p>
<ol type="1">
<li>使用了<strong><code>3</code>个<code>3x3</code>卷积核</strong>来代替<strong><code>7x7</code>卷积核</strong>，</li>
<li>使用了<strong><code>2</code>个<code>3x3</code>卷积核</strong>来代替<strong><code>5x5</code>卷积核</strong>，</li>
</ol>
<p>举例如下图，<code>2</code>个<code>3x3</code>实现<code>1</code>个<code>5x5</code>图上的<strong>感受野</strong></p>
<p><img src="/2025/07/09/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-7/image-20250709171825398.png" style="zoom:67%;"></p>
<p>在VGG网络中，3x3卷积层的堆叠确实能逐层扩大感受野，但它的作用不仅仅局限于此。每一层卷积后的通道数（特征图数量）的变化，实际上意味着网络在该层对输入数据的特征表达能力发生了变化。</p>
<p>• 通道数的增加：表示网络希望在该层提取更多维度、更复杂的特征。</p>
<p>•
通道数的减少（虽然VGG中减少较少见）：通常是为了压缩特征维度，减少计算量或提炼关键信息。</p>
<p>以下代码实现了VGG块，简单来说就是： n个
(3x3卷积层+<code>relu</code>激活) +
最后一个<code>maxPool</code>池化层</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure>
<h2 id="vgg网络">2. VGG网络</h2>
<p>与AlexNet和LeNet一样，VGG网络由卷积层模块后接全连接层模块构成。</p>
<p>卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块里<strong>卷积层个数</strong>和<strong>输入输出通道数</strong>。即
( <strong>卷积层数</strong>, <strong>输入channel数</strong>,
<strong>输出channel数</strong>)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br></pre></td></tr></table></figure>
<p>下面我们实现VGG-11。</p>
<p>（我们能发现我们已经学习的3种网络，其全连接层都是由3层组成的，这是一个标准的带隐藏层的全连接层）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span></span>):</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">&quot;vgg_block_&quot;</span> + <span class="built_in">str</span>(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>下面构造一个高和宽均为<code>224</code>的单通道数据样本来观察每一层的输出形状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)</span></span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27;output shape: &#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/09/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-7/image-20250716103025753.png"></p>
<p>可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。</p>
<p>VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。</p>
<h2 id="获取数据和训练模型">3. 获取数据和训练模型</h2>
<p>因为VGG-11计算上比AlexNet更加复杂，出于测试的目的我们构造一个通道数更小，或者说更窄的网络在Fashion-MNIST数据集上进行训练。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ratio = <span class="number">8</span></span><br><span class="line">small_conv_arch = [(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>//ratio), (<span class="number">1</span>, <span class="number">64</span>//ratio, <span class="number">128</span>//ratio), (<span class="number">2</span>, <span class="number">128</span>//ratio, <span class="number">256</span>//ratio),</span><br><span class="line">                   (<span class="number">2</span>, <span class="number">256</span>//ratio, <span class="number">512</span>//ratio), (<span class="number">2</span>, <span class="number">512</span>//ratio, <span class="number">512</span>//ratio)]</span><br><span class="line">net = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)</span><br></pre></td></tr></table></figure>
<p>模型训练过程与上一节的AlexNet中的类似。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/09/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-7/image-20250716104220789.png"></p>
<p>我们通过vgg的卷积块实现的网络，效果和之前的AlexNet相近。</p>
<p>VGG-11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。</p>
<h2 id="summary工具包观察参数变换">4. summary工具包观察参数变换</h2>
<p>我们可以用summary工具观察我们pytorch网络的参数结构变换。</p>
<p><code>pip install torchsummary</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch, fc_features, fc_hidden_units)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line">summary(net, (<span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>), device=<span class="built_in">str</span>(device))</span><br></pre></td></tr></table></figure>
<p>用法很简单，<code>summary(模型, 输入)</code> 即可。</p>
<p>输出中，每行的内容如列标所示，对应的Layer有着对应的Output
Shape，即经过该层Layer后的输出形状</p>
<p>第一个参数-1不用管。指batch_size</p>
<p><img src="/2025/07/09/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-7/image-20250716104521588.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.8 网络中的网络（NiN）</title>
    <url>/2025/07/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-8/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>前几节介绍的<code>LeNet</code>、<code>AlexNet</code>和<code>VGG</code>在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。也就是
<strong>卷积层+全连接</strong> 的模式。</p>
<p>其中，<code>AlexNet</code>和<code>VGG</code>对<code>LeNet</code>的改进主要在于如何对这两个模块加宽（增加通道数）和加深。</p>
<p>本节我们介绍网络中的网络（NiN）[1]。它提出了另外一个思路，即串联多个<strong>由
卷积层和“全连接”层 构成的小网络</strong>来构建一个深层网络。</p>
<h2 id="nin块">1. NiN块</h2>
<p>我们知道，卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。</p>
<p>如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。</p>
<p>回忆在5.3节（多输入通道和多输出通道）里介绍的1×1卷积层。它可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，<code>NiN</code><strong>使用1×1卷积层来替代全连接层</strong>，从而使空间信息能够自然传递到后面的层中去。</p>
<p>1x1卷积层不能等价于传统的全连接层，可以理解为：传统的全连接层是长宽都为1，通道数为n的特殊1x1卷积。然而我们使用1x1在图像的通道上做全连接计算，能够保留各个元素间的空间信息。</p>
<p>图5.7对比了<code>NiN</code>同<code>AlexNet</code>和<code>VGG</code>等网络在结构上的主要区别。</p>
<p><img src="/2025/07/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-8/image-20250717170006274.png"></p>
<p>NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1×1卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">        <span class="comment"># 1层卷积</span></span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2层1x1卷积</span></span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure>
<p>1x1卷积则作用于<strong>通道维度</strong>，在空间维度不变的情况下，进行<strong>跨通道特征变换</strong>。这样，组合使用卷积+1x1卷积，有助于<strong>在局部区域提取空间特征的同时，融合和重组通道特征</strong>。</p>
<p>在前期就使用1x1卷积，有助于<strong>从低层开始就对特征通道进行非线性组合与优化</strong>，提高整个网络的信息流动与表达能力。</p>
<p><strong>NIN块中使用2个连续的1x1卷积层</strong>，主要是为了让局部感知器具备<strong>更深的非线性特征抽象能力</strong>，就像在一个局部区域上堆叠了一个<strong>小型多层感知网络</strong>。</p>
<p>单个<strong>1x1卷积 + ReLU</strong>
相当于“局部一次非线性变化”。<strong>2个 1x1卷积 + ReLU</strong>
堆叠，相当于“局部两次非线性变化”，就能逐层组合出<strong>更复杂的局部特征模式</strong>，这是深度网络的基本原则。那为什么不3层呢？这个问题涉及到<strong>设计的权衡</strong>：</p>
<ol type="1">
<li>每多加一层1x1卷积意味着<strong>更多的参数、计算量</strong>，尤其在早期卷积层（特征图尺寸较大）时影响更明显。</li>
<li>NIN设计初衷是<strong>局部多层感知器</strong>，而不是在每个像素点上搭建一个“深网络”。在局部区域过度堆叠层数（3层或更多），会使得模型<strong>在局部区域内过拟合</strong>，破坏全局特征的整合。</li>
</ol>
<p>总结来说，2层足以实现有效的<strong>局部多层特征抽象</strong>；</p>
<h2 id="nin模型">2. NiN模型</h2>
<p>NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为11×11、5×5和3×3的卷积层，相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为3×3的最大池化层。</p>
<p>除使用NiN块以外，NiN还有一个设计与AlexNet显著不同：NiN去掉了AlexNet最后的3个全连接层，取而代之地，NiN使用了输出通道数等于标签类别数的NiN块，然后使用<strong>全局平均池化层对每个通道中所有元素求平均</strong>并直接用于分类。</p>
<p>这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GlobalAvgPool2d</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GlobalAvgPool2d, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># print(x.size())</span></span><br><span class="line">        <span class="comment"># print(x.size()[2:])</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line"></span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line"></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure>
<p>我们构建一个数据样本来查看每一层的输出形状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> name, blk <span class="keyword">in</span> net.named_children():</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&#x27;output shape: &#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-8/image-20250720175313396.png"></p>
<p>可以看到经过<code>GlobalAvgPool2d()</code>后，长宽为5的图像变成了长宽为1。</p>
<p>可以理解为，输出结果是网络学会的的每个类的一张响应图，通过GAP获取每张图的平均值，用来判断是否存在该类。</p>
<h2 id="获取数据和训练模型">3. 获取数据和训练模型</h2>
<p>我们依然使用Fashion-MNIST数据集来训练模型。NiN的训练与AlexNet和VGG的类似，但这里使用的学习率更大。</p>
<p><img src="/2025/07/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-8/image-20250720175801104.png"></p>
<p><img src="/2025/07/16/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-8/image-20250720191508648.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习 5.9 含并行连结的网络（GoogLeNet）</title>
    <url>/2025/07/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-9/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="inception-块">1. Inception 块</h2>
<p>GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上一节介绍的NiN块相比，这个基础块在结构上更加复杂，如图5.8所示。</p>
<p><img src="/2025/07/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-9/image-20250727042750944.png" style="zoom:80%;"></p>
<p>由图5.8可以看出，Inception块里有<strong>4条并行的线路</strong>。</p>
<p>前3条线路使用窗口大小分别是<code>1×1</code>、<code>3×3</code>和<code>5×5</code>的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×1卷积来减少输入通道数，以降低模型复杂度。
第四条线路则使用<code>3×3</code>最大池化层，后接<code>1×1</code>卷积层来改变通道数。</p>
<p>4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结，并输入接下来的层中去。</p>
<p>Inception块中可以自定义的<strong>超参数</strong>是每个层的<strong>输出通道数</strong>，我们以此来控制模型复杂度。</p>
<p><img src="/2025/07/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-9/image-20250727043435616.png" style="zoom: 67%;"></p>
<h2 id="googlenet模型">2. GoogLeNet模型</h2>
<p>GoogLeNet跟VGG一样，在<code>主体卷积部分</code>
中使用<code>5</code>个模块（block），每个模块之间使用<code>步幅为2</code>的<code>3×3</code>最大池化层来减小输出高宽。</p>
<p><strong>第一模块</strong>
使用一个<code>64通道</code>的<code>7×7</code>卷积层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>第二模块</strong>
使用2个卷积层：首先是<code>64通道</code>的<code>1×1</code>卷积层，然后是将通道增大3倍的`3×3卷积层。它对应Inception块中的第二条线路</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>第三模块</strong> 串联2个完整的Inception块。</p>
<p>第一个Inception块的输出通道数为
<code>64+128+32+32=256</code>，其中4条线路的输出通道数比例为<code>64:128:32:32=2:4:1:1</code>。其中第二、第三条线路先分别将输入通道数减小至
<code>96/192=1/2</code> 和 <code>16/192=1/12</code>
后，再接上第二层卷积层。</p>
<p>第二个Inception块输出通道数增至<code>128+192+96+64=480</code>，每条线路的输出通道数之比为<code>128:192:96:64=4:6:3:2</code>。其中第二、第三条线路先分别将输入通道数减小至<code>128/256=1/2</code>和<code>32/256=1/8</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>第四模块</strong>
更加复杂。它串联了5个Inception块，其输出通道数分别是<code>192+208+48+64=512</code>、<code>160+224+64+64=512</code>、<code>128+256+64+64=512</code>、<code>112+288+64+64=528</code>和<code>256+320+128+128=832</code>。</p>
<p>这些线路的通道数分配和第三模块中的类似，首先含<code>3×3</code>卷积层的第二条线路输出最多通道，其次是仅含1×1卷积层的第一条线路，之后是含5×5卷积层的第三条线路和含3×3最大池化层的第四条线路。其中第二、第三条线路都会先按比例减小通道数。这些比例在各个Inception块中都略有不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><strong>第五模块</strong>有输出通道数为<code>256+320+128+128=832</code>和<code>384+384+128+128=1024</code>的两个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的一致，只是在具体数值上有所不同。</p>
<p>需要注意的是，第五模块的后面紧跟输出层，该模块同NiN一样使用全局平均池化层来将每个通道的高和宽变成1。最后我们将输出变成二维数组后接上一个输出个数为标签类别数的全连接层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   d2l.GlobalAvgPool2d())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5,</span><br><span class="line">                    d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。本节里我们将输入的高和宽从224降到96来简化计算。下面演示各个模块之间的输出的形状变化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br><span class="line">X = torch.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>)</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net.children():</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;output shape: &#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-9/image-20250727063930884.png" style="zoom:67%;"></p>
<h2 id="获取数据和训练模型">3. 获取数据和训练模型</h2>
<p>我们使用高和宽均为96像素的图像来训练GoogLeNet模型。训练使用的图像依然来自Fashion-MNIST数据集。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"><span class="comment"># 如出现“out of memory”的报错信息，可减小batch_size或resize</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line"></span><br><span class="line">lr, num_epochs = <span class="number">0.001</span>, <span class="number">5</span></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/27/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A05-9/image-20250727064323133.png" style="zoom:67%;"></p>
<p>(可以看出，相比于之前的网络，其准确率也接近0.9，但是其速度平均只需要30s)</p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第5节</tag>
      </tags>
  </entry>
  <entry>
    <title>《动手学深度学习》3.1 线性回归</title>
    <url>/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="前言">1前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《动手学深度学习》原文（课本）：https://tangshusen.me/Dive-into-DL-PyTorch/#/</p>
<p>《动手学深度学习》代码：https://github.com/ShusenTang/Dive-into-DL-PyTorch</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<h2 id="线性回归模型">2 线性回归模型</h2>
<p>线性回归，是一段连续值模型，即<strong>提供任意的(x1,x2,x3...)</strong>，都有其对应的<strong>唯一结果y</strong></p>
<p>线性回归（Linear
Regression）的核心思想就是<strong>找到一条最优的直线来拟合一堆数据点</strong>，使得预测值和真实值之间的误差最小。</p>
<p>假设y是房价，房屋面积是x1，房龄是x2。</p>
<p>其中 w1 和 w2 是权重（weight），b
是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。</p>
<p><span class="math display">\[
\hat{y}{}^{(i)}={x}^{(i)}_{1}{w}_{1}+{x}^{(i)}_{2}{w}_{2}+b
\]</span> 假设y是房价，房屋面积是x1，房龄是x2。</p>
<p>其中 <span class="math inline">\({w}_{1}\)</span> 和 <span class="math inline">\({w}_{2}\)</span> 是权重（weight），<span class="math inline">\(b\)</span>
是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。</p>
<h2 id="模型训练的概念">3.模型训练的概念</h2>
<h3 id="收集训练数据">3.1 收集训练数据</h3>
<p>采集的<strong>样本数为 n</strong>，<strong>索引为 i</strong> 的样本
<span class="math display">\[
\hat{y}{}^{(i)}={x}^{(i)}_{1}{w}_{1}+{x}^{(i)}_{2}{w}_{2}+b
\]</span> ​</p>
<h3 id="损失函数">3.2 损失函数</h3>
<p>所谓的损失，就是模型预测值<span class="math inline">\(\hat{y}\)</span>与实际值y之间的误差。</p>
<p>即我们当前的$ ({w}<em>{1},{w}</em>{2},b)<span class="math inline">\(固定情况下，输入\)</span>{x}<em>{1}，{x}</em>{2}<span class="math inline">\(经过模型计算获得的估计值\)</span>$ 和 样本中 <span class="math inline">\(({x}_{1},{x}_{2},y)\)</span>中已经有实际的y
之间的误差。</p>
<p>损失函数有许多,
这里使用的<strong>平方误差函数</strong>也称为平方损失（square loss）
<span class="math display">\[
{L}^{(i)}({w}_{1},{w}_{2},b)=\frac {1} {2}(\hat{y}^{(i)}−y(i))^2
\]</span> 由于有n个样本，我们选平均值作为整体的损失函数 <span class="math display">\[
{L}({w}_{1},{w}_{2},b)
=
\frac {1} {n}\sum ^{n}_{i=1} {L}^{(i)}({w}_{1},{w}_{2},b)
=
\frac {1} {n}\sum ^{n}_{i=1} {\frac {1} {2}(\hat{y}^{(i)}−y^{(i)})^{2} }
\]</span>
在线性回归中使用平方差作为损失函数有很多好处，最重要的一点在于它是一个<strong>凸函数</strong>，这意味着它只有一个最小值，确保了我们可以通过优化算法（如梯度下降）找到最优解。它的导数是连续且容易计算的，这使得求解优化问题变得更简单。</p>
<p>这样我们就能求出最优的参数 <span class="math inline">\(({w}_{1},{w}_{2},b)\)</span></p>
<h3 id="优化算法">3.3 优化算法</h3>
<p>当模型和损失函数形式较为简单时，上面的<strong>误差最小化问题的解可以直接用公式表达出来</strong>。这类解叫作<strong>解析解（analytical
solution）</strong></p>
<p>然而，大多数深度学习模型并没有解析解，只能<strong>通过优化算法有限次迭代模型参数</strong>来尽可能降低损失函数的值。这类解叫作<strong>数值解（numerical
solution）</strong>。</p>
<p>这里我们通过偏导数对 <span class="math inline">\(({w}_{1},{w}_{2},b)\)</span> 进行优化，使得模型的
<span class="math inline">\(({w}_{1},{w}_{2},b)\)</span>
越来越接近实际的 <span class="math inline">\(({w}_{1},{w}_{2},b)\)</span>
，也就是说我们在此通过模型求出它的数值解</p>
<h4 id="对偏导的解释">3.3.1 对偏导的解释</h4>
<p>我们知道一元方程中 <span class="math inline">\(y=a{x}^{2}\)</span>，x的导数是<span class="math inline">\(2ax\)</span>，同时也代表着该点<span class="math inline">\((x,y)\)</span>的斜率是<span class="math inline">\(2ax\)</span></p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250225233408156.png"></p>
<p>在 <span class="math inline">\(x=0\)</span> 处，斜率/导数 <span class="math inline">\(ax=0\)</span>。这也是该函数最低点。</p>
<p>我们随机在<span class="math inline">\(x&gt;0\)</span>处选取点<span class="math inline">\(({x}_{1},{y}_{1})\)</span>，此时的斜率 <span class="math inline">\(2a{x}_{1}&gt;0\)</span>，那么优化更新<span class="math inline">\({x}_{1}\)</span>的时候 <span class="math inline">\(({x}_{1})&#39; = {x}_{1} - 2a{x}_{1}\)</span>
的话，$({x}<em>{1})' 必定在x<span class="math inline">\(1的左侧，即\)</span>{x}</em>{1}$<strong>减去该处的导数</strong>会向最低点的x==0移动。</p>
<p>在<span class="math inline">\(x&lt;0\)</span>处取值也是同理，<span class="math inline">\(2a{x}_{1}&lt;0\)</span>，那么<span class="math inline">\(({x}_{1})&#39; = {x}_{1} - 2a{x}_{1}\)</span>
的话，相当于<span class="math inline">\({x}_{1}\)</span>向右边移动。</p>
<p>那么这里说明一个问题，<strong>我们所求出来的偏导实际上要的只是这个值的正负</strong>，其数值大小并不重要（当然这里y=ax²中，x越大斜率越大的特点能加快到达最小值，但其它函数就不一定了）</p>
<p>倘若该 <span class="math inline">\(({x}_{1},{y}_{1})\)</span>
并非处于最低点的斜率k=0的位置的话，就会不断移动，最终到达该点（收敛）</p>
<p>（当然这就引出局部最优或者未到达局部最优但是斜率为0的位置等问题，应该以后其他课程会解决）</p>
<h3 id="继续">3.3.2 继续</h3>
<p>重新回到本章节的损失函数中，我们的目的是让模型预测的y尽可能接近真实数据<span class="math inline">\(\hat{y}\)</span>，因此损失越小越好。 <span class="math display">\[
{L}({w}_{1},{w}_{2},b)
=
\frac {1} {n}\sum ^{n}_{i=1} {L}^{(i)}({w}_{1},{w}_{2},b)
=
\frac {1} {n}\sum ^{n}_{i=1} {\frac {1} {2}(\hat{y}^{(i)}−y^{(i)})^{2} }
\]</span> 再解析y获得如下完整w参数的loss函数（手打公式打麻了）</p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226213039241.png"></p>
<p>上面公式举例对<span class="math inline">\({w}_{1}\)</span>进行求导，利用链式法则 <span class="math display">\[
E=L^{(i)}={x}^{(i)}_{1}{w}_{1}+{x}^{(i)}_{2}{w}_{2}+b−y^{(i)}
\]</span></p>
<p><span class="math display">\[
J=E^2
\]</span></p>
<p><span class="math display">\[
\frac {∂J} {∂{w}_{1}}= \frac {∂J} {∂E}*\frac {∂E}
{∂{w}_{1}}=2E*{x}^{(i)}_{1}=2{x}^{(i)}_{1}({x}^{(i)}_{1}{w}_{1}+{x}^{(i)}_{2}{w}_{2}+b−y^{(i)})
\]</span></p>
<p>因此得到如下更新公式（2是常数所以去掉了）</p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226214637799.png"></p>
<p>在上式中，<span class="math inline">\(∣B∣\)</span>
代表每个小批量中的样本个数（批量大小，batch size），<span class="math inline">\(η\)</span> 称作学习率（learning
rate）并取正数。</p>
<h2 id="线性回归的表示方法">4 线性回归的表示方法</h2>
<h3 id="神经网络图">4.1 神经网络图</h3>
<p>线性回归是一个单层神经网络</p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226215105779.png"></p>
<p>输入<span class="math inline">\({x}_{1},{x}_{2}\)</span>等参数，接受并输出层o的内部的每个节点计算都类似
<span class="math inline">\(y={x}_{1}{w}_{1}+{x}_{2}{w}_{2}+b\)</span>
,然后该节点的输出为y。</p>
<h3 id="矢量计算表达式">4.2 矢量计算表达式</h3>
<p>举例3个数据，每个数据2个特征，进行如下计算获得3个输出 <span class="math display">\[
\hat{y}^{(1)}=x^{(1)}_{1}w_{1}+x^{(1)}_{2}w_{1}+b
\\
\hat{y}^{(2)}=x^{(2)}_{1}w_{1}+x^{(2)}_{2}w_{1}+b
\\
\hat{y}^{(3)}=x^{(3)}_{1}w_{1}+x^{(3)}_{2}w_{1}+b
\]</span> 现在，我们将上面3个等式转化成矢量计算。设 <span class="math display">\[
\mathbf{\hat{y}} = \begin{bmatrix} \hat{y}^{(1)} \\ \hat{y}^{(2)} \\
\hat{y}^{(3)} \end{bmatrix} \quad
\mathbf{X} = \begin{bmatrix} x^{(1)}_{1}&amp;x^{(1)}_{2} \\
x^{(2)}_{1}&amp;x^{(2)}_{2} \\ x^{(3)}_{1}&amp;x^{(3)}_{2} \end{bmatrix}
\quad
\mathbf{w} = \begin{bmatrix} w_1 \\ w_2  \end{bmatrix}
\]</span></p>
<p>行列式计算为 <span class="math display">\[
\mathbf{\hat{y}}=\mathbf{X}\mathbf{w}+b
\]</span> 原本的损失函数变成矢量如下</p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226221037001.png"></p>
<p>小批量随机梯度下降的迭代步骤将相应地改写为</p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226221057789.png"></p>
<p><img src="/2025/02/25/%E5%8A%A8%E6%89%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A01/image-20250226221104631.png"></p>
]]></content>
      <tags>
        <tag>ai</tag>
        <tag>动手学习深度学习</tag>
        <tag>第3节</tag>
      </tags>
  </entry>
</search>
