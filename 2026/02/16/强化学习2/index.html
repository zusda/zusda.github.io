<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha256-5eIC48iZUHmSlSUz9XtjRyK2mzQkHScZY1WdMaoz74E=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.21.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<link rel="stylesheet" href="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.css"><!--我的APlayer的样式-->
<script src="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.js"></script><!--APlayer的依赖-->
<script src="https://unpkg.com/meting@2/dist/Meting.min.js"></script><!--Meting的依赖-->
<meting-js
    server="netease"
    type="playlist"
    autoplay=false 
    fixed=true 
    lrc-type=0
    id="7120442532"> 
</meting-js> 

    <meta name="description" content="前言 从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习 2. 马尔可夫决策过程">
<meta property="og:url" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="前言 从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260216223328596.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260216223512627.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217002146699.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217002233571.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217003542867.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217011334307.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217011729204.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217012626720.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217012719975.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217013037995.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217204742062.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217211608715.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217211906498.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213222585.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213541631.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213611249.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213817517.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214211409.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214339241.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214443838.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215006682.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215031613.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217220001061.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217223440639.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215006682.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215235976.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215320315.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215353789.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217231858283.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232029958.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232156124.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232205663.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232221258.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232250863.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217233141732.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260218000232071.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260218000711660.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219103740178.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219115933261.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232205663.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134112672.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134319975.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134817207.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221190810623.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221191921853.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221192217950.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221192253830.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221193523718.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194004166.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194035877.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194208734.png">
<meta property="og:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194302460.png">
<meta property="article:published_time" content="2026-02-16T13:59:33.000Z">
<meta property="article:modified_time" content="2026-02-21T11:43:10.440Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="蘑菇书">
<meta property="article:tag" content="强化想学习">
<meta property="article:tag" content="第2节">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260216223328596.png">


<link rel="canonical" href="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/","path":"2026/02/16/强化学习2/","title":"强化学习 2. 马尔可夫决策过程"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>强化学习 2. 马尔可夫决策过程 | Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8"><span class="nav-number">2.</span> <span class="nav-text">1. 马尔可夫性质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8Bmarkov-reward-process-mrp"><span class="nav-number">3.</span> <span class="nav-text">2.马尔可夫奖励过程（Markov
reward process, MRP）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9E%E6%8A%A5%E4%B8%8E%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 回报与价值函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 举例说明</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B"><span class="nav-number">4.</span> <span class="nav-text">3. 贝尔曼方程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 解析解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9B%B4%E6%96%B0"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 贝尔曼更新</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="nav-number">5.</span> <span class="nav-text">4. 马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 马尔可夫决策过程中的策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-%E5%92%8C-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E8%BF%87%E7%A8%8B%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%A5%96%E5%8A%B1%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.2.</span> <span class="nav-text">4.2
马尔可夫决策过程 和 马尔可夫过程&#x2F;马尔可夫奖励过程的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="nav-number">5.3.</span> <span class="nav-text">4.3
马尔可夫决策过程中的价值函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%9F%E6%9C%9B%E6%96%B9%E7%A8%8B"><span class="nav-number">6.</span> <span class="nav-text">5. 贝尔曼期望方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E5%92%8C%E6%8E%A7%E5%88%B6"><span class="nav-number">7.</span> <span class="nav-text">6. 策略评估和控制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0"><span class="nav-number">7.1.</span> <span class="nav-text">6.1
马尔可夫决策过程中的策略评估</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%8E%A7%E5%88%B6"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 马尔可夫决策过程控制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3"><span class="nav-number">7.3.</span> <span class="nav-text">6.3 策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3"><span class="nav-number">7.4.</span> <span class="nav-text">6.4 价值迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E4%B8%8E%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">7.5.</span> <span class="nav-text">6.5 策略迭代与价值迭代的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E6%88%98"><span class="nav-number">8.</span> <span class="nav-text">7.实战</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83"><span class="nav-number">8.1.</span> <span class="nav-text">7.1 环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3-1"><span class="nav-number">8.2.</span> <span class="nav-text">7.2策略迭代</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%E8%BF%AD%E4%BB%A3"><span class="nav-number">8.3.</span> <span class="nav-text">7.3 价值迭代迭代</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="强化学习 2. 马尔可夫决策过程 | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习 2. 马尔可夫决策过程
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2026-02-16 21:59:33" itemprop="dateCreated datePublished" datetime="2026-02-16T21:59:33+08:00">2026-02-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2026-02-21 19:43:10" itemprop="dateModified" datetime="2026-02-21T19:43:10+08:00">2026-02-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="前言">前言</h2>
<p>从零开始学习ai文章系列计划是个人在《动手学深度学习》和《磨菇书》两本书的学习中的个人笔记，文章也会以课本中的章节分开，即每个章节一片笔记。我会尽量的把主要内容以及遇到的难点进行记录与解决，如果哪里有错误的欢迎指正。或者不清晰的可以直接查看原文部分。</p>
<span id="more"></span>
<p>《蘑菇书》原文（课本）：https://datawhalechina.github.io/easy-rl/#/</p>
<p>（由于有时候公式太多，可能会直接贴图片）</p>
<p>蘑菇书的文章结构不会跟之前《动手学深度学习》按照原文章节进行，个人会适当调节。</p>
<p>虽然考虑过分多个章节，但是感觉全放在一起看比较流畅。</p>
<hr>
<h2 id="马尔可夫性质">1. 马尔可夫性质</h2>
<p><strong>马尔可夫性质</strong>（Markov
property）是指一个随机过程在给定现在状态及所有过去状态情况下，<strong>其未来状态的条件概率分布
仅依赖于当前状态</strong>。</p>
<p>我们设状态的历史为 <span class="math inline">\(h_t={s_1,s_2,s_3,…,s_t}\)</span>（<span class="math inline">\(h_t\)</span>
包含了之前的所有状态），则马尔可夫过程满足条件：(这里p是概率) <span class="math display">\[
p(s_{t+1}|s_t) = p(s_{t+1}|h_t)  
\]</span> <strong>离散时间的马尔可夫过程</strong>
也称为<strong>马尔可夫链</strong>（Markov chain）。</p>
<p>例如，图 2.2 里面有4个状态，这4个状态在 <span class="math inline">\(s_1,s_2,s_3,s_4\)</span> 之间互相转移。比如从
<span class="math inline">\(s_1\)</span> 开始，<span class="math inline">\(s_1\)</span> 有 0.1 的概率继续存留在 <span class="math inline">\(s_1\)</span> 状态，有 0.2 的概率转移到 <span class="math inline">\(s_2\)</span>，有 0.7 的概率转移到 <span class="math inline">\(s_4\)</span> 。如果 <span class="math inline">\(s_4\)</span> 是我们的当前状态，它有 0.3
的概率转移到 <span class="math inline">\(s_2\)</span> ，有 0.2
的概率转移到 <span class="math inline">\(s_3\)</span> ，有 0.5
的概率留在当前状态。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260216223328596.png" style="zoom: 50%;"></p>
<p>我们可以用状态转移矩阵（state transition matrix）P 来描述状态转移
<span class="math inline">\(p( s_{t+1}=s&#39; ∣ s_t=s)\)</span>：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260216223512627.png" style="zoom: 67%;"></p>
<p>状态转移矩阵类似于条件概率（conditional
probability），它表示当我们知道当前我们在状态st时，到达下面所有状态的概率。所以它的每一行描述的是从一个节点到达所有其他节点的概率。</p>
<h2 id="马尔可夫奖励过程markov-reward-process-mrp">2.马尔可夫奖励过程（Markov
reward process, MRP）</h2>
<p>马尔可夫奖励过程（Markov reward process, MRP）是 <strong>马尔可夫链
加上 奖励函数</strong>。</p>
<p>马尔可夫奖励过程中，状态转移矩阵和状态都与马尔可夫链一样，只是多了奖励函数（reward
function）。奖励函数R是一个期望，表示当我们到达某一个状态的时候，可以获得多大的奖励。这里另外定义了折扣因子
γ 。如果状态数是有限的，那么 R 可以是一个向量。</p>
<blockquote>
<p>奖励函数 实际上理解成 即时奖励
就行了。即执行动作后，<strong>到达某个状态获得的奖励</strong>。</p>
</blockquote>
<h3 id="回报与价值函数">2.1 回报与价值函数</h3>
<p>回报（return）可以定义为奖励的逐步叠加，假设时刻t后的奖励序列为 <span class="math inline">\(r_{t+1},r_{t+2},r_{t+3},...\)</span>
，则回报为</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217002146699.png" style="zoom: 67%;"></p>
<p>其中，T是最终时刻（t是开始时刻），γ
是折扣因子，越往后得到的奖励，折扣越多。这说明我们更希望得到现有的奖励，对未来的奖励要打折扣。</p>
<p>当我们有了回报之后，就可以定义状态的价值了，就是<strong>状态价值函数</strong>（state-value
function）。对于马尔可夫奖励过程，<strong>状态价值函数</strong>被定义成<strong>回报的期望</strong>，即：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217002233571.png" style="zoom:67%;"></p>
<p>期望就是<strong>从这个状态开始</strong>，我们可能获得多大的价值。所以期望也可以看成未来可能获得奖励的当前价值的表现，就是当我们进入某一个状态后，我们现在有多大的价值。</p>
<blockquote>
<p>我们使用折扣因子的原因如下：</p>
<ol type="1">
<li>有些马尔可夫过程是带环的，它并不会终结，我们想避免无穷的奖励。</li>
<li>我们并不能建立完美的模拟环境的模型，我们对未来的评估不一定是准确的，我们不一定完全信任模型，因为这种不确定性，所以我们对未来的评估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。</li>
<li>如果奖励是有实际价值的，我们可能更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。</li>
<li>我们也更想得到即时奖励。有些时候可以把折扣因子设为
0（γ=0），我们就只关注当前的奖励。我们也可以把折扣因子设为
1（γ=1），对未来的奖励并没有打折扣，未来获得的奖励与当前获得的奖励是一样的。折扣因子可以作为强化学习智能体的一个超参数（hyperparameter）来进行调整，通过调整折扣因子，我们可以得到不同动作的智能体。</li>
</ol>
</blockquote>
<h3 id="举例说明">2.2 举例说明</h3>
<p>以 下图2.4 举例说明我们之前所学的概念。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217003542867.png" style="zoom:67%;"></p>
<p><strong>奖励函数</strong>可以定义为：智能体<strong>进入第一个状态
<span class="math inline">\(s_1\)</span> 的时候会得到 5
的奖励</strong>，<strong>进入第七个状态 <span class="math inline">\(s_7\)</span> 的时候会得到 10
的奖励</strong>，<strong>进入其他状态都没有奖励</strong>。</p>
<p>我们对以下3种 4步骤的回合（γ=0.5）来<strong>计算回报 G</strong>。</p>
<ol type="1">
<li><span class="math inline">\(s_4,s_5,s_6,s_7\)</span> 的回报: 0 +
0.5×0 + 0.25×0 + 0.125×10 = 1.25</li>
<li><span class="math inline">\(s_4,s_3,s_2,s_1\)</span> 的回报: 0 +
0.5×0 + 0.25×0 + 0.125×5 = 0.625</li>
<li><span class="math inline">\(s_4,s_5,s_6,s_6\)</span> 的回报: 0 +
0.5×0 + 0.25×0 + 0.125×0 = 0</li>
</ol>
<p>(第一个0当成从某个初始状态 <span class="math inline">\(s_0\)</span>
必定到达 <span class="math inline">\(s_4\)</span> 即可，而并不是说从
<span class="math inline">\(s_4\)</span>
开始。因为奖励的定义是到达某一个状态后才获取的奖励)</p>
<blockquote>
<p>我们对轨迹 <span class="math inline">\(s_4,s_5,s_6,s_7\)</span>
（第一个）的奖励进行计算，<code>这里折扣因子是 0.5</code>。</p>
<ol type="1">
<li>在 <span class="math inline">\(s_4\)</span> 的时候，奖励为0。</li>
<li>下一个状态 <span class="math inline">\(s_5\)</span>
的时候，因为我们已经到了下一步，所以要把 <span class="math inline">\(s_5\)</span> 进行折扣， <span class="math inline">\(s_5\)</span> 的奖励也是0。</li>
<li>然后是 <span class="math inline">\(s_6\)</span>
，奖励也是0，折扣因子应该是0.25。</li>
<li>到达 <span class="math inline">\(s_7\)</span>
后，我们获得了一个奖励，但是因为状态 <span class="math inline">\(s_7\)</span>
的奖励是未来才获得的奖励，所以我们要对之进行3次折扣。</li>
</ol>
<p>最终这个轨迹的回报就是 1.25。类似地，我们可以得到其他轨迹的回报。</p>
</blockquote>
<p>这里就引出了一个问题，当我们有了一些轨迹的实际回报时，怎么<strong>计算它的价值函数</strong>呢？</p>
<p>比如我们想知道 <span class="math inline">\(s_4\)</span>
的价值，即当我们进入 <span class="math inline">\(s_4\)</span>
后，它的价值到底如何？一个可行的做法就是我们可以生成很多轨迹，然后把轨迹都叠加起来。比如我们可以从
<span class="math inline">\(s_4\)</span>
开始，采样生成很多轨迹，把这些轨迹的回报都计算出来，然后将其取平均值作为我们进入
<span class="math inline">\(s_4\)</span>
的价值。这其实是一种计算价值函数的办法，也就是通过蒙特卡洛（Monte
Carlo，MC）采样的方法计算 <span class="math inline">\(s_4\)</span>
的价值。</p>
<h2 id="贝尔曼方程">3. 贝尔曼方程</h2>
<p>贝尔曼方程（Bellman equation）：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217011334307.png" style="zoom:50%;"></p>
<blockquote>
<ul>
<li><span class="math inline">\(s&#39;\)</span>
可以看成未来的某一状态（全部状态集合S中的一个），</li>
<li><span class="math inline">\(p(s&#39;∣s)\)</span> 是指从 当前状态s
转移到 未来状态s′ 的概率。</li>
<li><span class="math inline">\(V(s&#39;)\)</span>
代表的是未来某一个状态 <span class="math inline">\(s&#39;\)</span>
的价值。</li>
<li>R(s) 通常定义为“处在状态 s
时，下一步获得的期望奖励”，也可以理解为“从 s
出发一步后得到的奖励的期望”。</li>
</ul>
<p>我们从当前状态开始，有一定的概率去到未来的所有状态，所以我们要把
p(s′∣s) 写上去。我们得到了未来状态后，乘一个
γ，这样就可以把未来的奖励打折扣。</p>
</blockquote>
<p>贝尔曼方程就是 <strong>当前状态 与
未来状态的迭代关系</strong>，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程因其提出者、动态规划创始人理查德
⋅ 贝尔曼（Richard Bellman）而得名
，也叫作“<strong>动态规划方程</strong>”。</p>
<h3 id="解析解">3.1 解析解</h3>
<p>假设有一个马尔可夫链如图 2.5a
所示，贝尔曼方程描述的就是当前状态到未来状态的一个转移。</p>
<p>如图 2.5b 所示，假设我们当前在 <span class="math inline">\(s_1\)</span> ， 那么它只可能去到3个未来的状态：有
0.1 的概率留在它当前位置，有 0.2 的概率去到 <span class="math inline">\(s_2\)</span> 状态，有 0.7 的概率去到 <span class="math inline">\(s_4\)</span>
状态。所以我们把<strong>状态转移概率乘它未来的状态的价值，再加上它的即时奖励（immediate
reward），就会得到它当前状态的价值。</strong></p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217011729204.png" style="zoom:67%;"></p>
<p>我们可以把贝尔曼方程写成矩阵的形式：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217012626720.png" style="zoom: 50%;"></p>
<p>当我们把贝尔曼方程写成矩阵形式后，可以直接求解：</p>
<p>我们可以直接得到<strong>解析解</strong>（analytic solution）：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217012719975.png" style="zoom: 67%;"></p>
<p>我们可以通过矩阵求逆把 V
的价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 <span class="math inline">\(O(N^3)\)</span>
。所以当状态非常多的时候，比如从10个状态到1000个状态，或者到100万个状态，当我们有100万个状态的时候，状态转移矩阵就会是一个100万乘100万的矩阵，对这样一个大矩阵求逆是非常困难的。所以这种通过<strong>解析解去求解的方法只适用于很小量状态的马尔可夫奖励过程</strong>。</p>
<h3 id="贝尔曼更新">3.2 贝尔曼更新</h3>
<p>我们也可以用动态规划的方法，一直<strong>迭代贝尔曼方程，直到价值函数收敛</strong>，我们就可以得到某个状态的价值。我们通过自举（bootstrapping）的方法不停地迭代贝尔曼方程，当最后更新的状态与我们上一个状态的区别并不大的时候，更新就可以停止，我们就可以输出最新的
V′(s)
作为它当前的状态的价值。这里就是<strong>把贝尔曼方程变成一个贝尔曼更新</strong>（Bellman
update），这样就可以得到状态的价值。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217013037995.png" style="zoom: 60%;"></p>
<p>初始化所有 V(s) 为0，对于某个状态s，根据 <span class="math inline">\(V&#39;(s)=R(s)+\gamma \sum^{}_{s&#39; \in
S}P(s&#39;|s)V(s&#39;)\)</span>
更新当前状态s的价值，对于所有状态s重复此步骤，每轮重复此过程，直到所有状态
V(s) 不再大幅度变化</p>
<blockquote>
<p>本质上就是一种价值迭代的方法。我们后面会提到策略迭代和价值迭代。</p>
</blockquote>
<h2 id="马尔可夫决策过程">4. 马尔可夫决策过程</h2>
<p>相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作）状态转移也多了一个条件，变成了
<span class="math inline">\(p(s_{t+1}=s&#39;|s_t)\)</span>
。未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。马尔可夫决策过程满足条件：<span class="math inline">\(p(s_{t+1}=s&#39;|s_t) =
p(s_{t+1}=s&#39;|h_t)\)</span> （ <span class="math inline">\(h_t\)</span> 表示：到时间 t
为止的历史信息（history）。</p>
<p>对于奖励函数，它也多了一个当前的动作，变成了 <span class="math inline">\(R(s_t=s,a_t=a)\)</span>
。当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少。</p>
<h3 id="马尔可夫决策过程中的策略">4.1 马尔可夫决策过程中的策略</h3>
<p><strong>策略</strong>定义了在<strong>某一个状态应该采取什么样的动作</strong>。一般用<span class="math inline">\(\pi\)</span> 表示我们的策略。 <span class="math display">\[
\pi(a|s)=p(a_t=a|s_t=s)
\]</span> 概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7
的概率 a=往左走，有 0.3 的概率 a=往右走，这是一个概率的表示。</p>
<p>在马尔可夫决策过程里面，状态转移函数 <span class="math inline">\(P(s&#39;∣s,a)\)</span>
基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以<strong>直接把动作进行加和，去掉
a</strong>，这样我们就可以得到对于<strong>马尔可夫奖励过程的转移</strong>，这里就没有动作，即</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217204742062.png" style="zoom: 60%;"></p>
<p>可以发现现在我们 计算 s到s′ 的概率并不需要动作a。即原本的P(s′∣s,a)
变成了P(s′∣s)。这就是所谓的<strong>马尔可夫决策过程 转换成
马尔可夫奖励过程</strong>。</p>
<h3 id="马尔可夫决策过程-和-马尔可夫过程马尔可夫奖励过程的区别">4.2
马尔可夫决策过程 和 马尔可夫过程/马尔可夫奖励过程的区别</h3>
<p>两者最主要的就是状态转移的方式。</p>
<p>在 <code>马尔可夫过程/马尔可夫奖励过程</code>
状态的转移是直接从一个状态到另一个状态，如 <span class="math inline">\(s_1 \rightarrow s_2\)</span>.</p>
<p>而在 <code>马尔可夫决策过程</code> 中，状态的转移是通过动作决定的，如
<span class="math inline">\(s_1执行动作a_1 \rightarrow s_2\)</span>
.</p>
<h3 id="马尔可夫决策过程中的价值函数">4.3
马尔可夫决策过程中的价值函数</h3>
<p>让我们回忆之前的马尔可夫决策过程中的价值函数定义为</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217211608715.png" style="zoom:50%;"></p>
<p>即从状态s开始，所有回报的期望。</p>
<p>那么在马尔可夫决策过程中，我们另外引入了一个 Q 函数（Q-function）。Q
函数也被称为动作价值函数（action-value function）。Q
函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217211906498.png" style="zoom:67%;"></p>
<p>如果对 Q 函数中的动作进行加和，就可以得到价值函数：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213222585.png" style="zoom:67%;"></p>
<p>此处我们对 Q 函数的贝尔曼方程进行推导：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213541631.png" style="zoom:60%;"></p>
<blockquote>
<p>推导说明，可跳过</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213611249.png" style="zoom:67%;"></p>
<p>我们从这步开始继续推导，首先根据状态s执行动作a后到达下一状态s'的概率，展开E</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217213817517.png"></p>
<p>根据马尔可夫性质未来的回报 未来的回报 <span class="math inline">\(G_{t+1}\)</span> 只取决于这个 s′，</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214211409.png" style="zoom:60%;"></p>
<p>将该方程带入之前一开始的展开中，得到如下公式（原来推导中最终结果）</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214339241.png" style="zoom:60%;"></p>
<p>如果我们重新合并概率P，就能得到原来推导中的下面公式（<span class="math inline">\(s_{t+1}\)</span>就是s'）</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217214443838.png" style="zoom:67%;"></p>
</blockquote>
<h2 id="贝尔曼期望方程">5. 贝尔曼期望方程</h2>
<p>我们可以把状态价值函数和 Q
函数拆解成两个部分：<strong>即时奖励</strong>和<strong>后续状态的折扣价值</strong>（discounted
value of successor state）。</p>
<p>通过对状态<strong>价值函数</strong>进行分解，我们就可以得到一个类似于之前马尔可夫奖励过程的贝尔曼方程————<strong>贝尔曼期望方程</strong>（Bellman
expectation equation）：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215006682.png" style="zoom:67%;"></p>
<p>对于 Q 函数，我们也可以做类似的分解，得到 Q
函数的贝尔曼期望方程：（其中下一状态所执行的动作a是由当前策略<span class="math inline">\(\pi\)</span> 决定的）</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215031613.png" style="zoom:67%;"></p>
<p>贝尔曼期望方程定义了当前状态与未来状态之间的关联。</p>
<blockquote>
<p>我们之前的贝尔曼方程那节提到，V(s)的定义为：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217220001061.png" style="zoom:50%;"></p>
<blockquote>
<p>R(s) 通常定义为“处在状态 s 时，下一步获得的期望奖励”，也可以理解为“从
s 出发一步后得到的奖励的期望”。这里可以当成 <span class="math inline">\(\sum_{s&#39; \in
S}{[p(s&#39;|s)*r_{s&#39;}]}\)</span>
，这里的s'与后面的未来奖励计算同步。</p>
<p>所以原式应该为 <span class="math display">\[
V(s)=\sum_{s&#39; \in S}{\{p(s&#39;|s)* [r_{s&#39;}+\gamma V(s&#39;)]\}}
\]</span></p>
</blockquote>
<p>其中</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217223440639.png" style="zoom:50%;"></p>
<p>也就是说我们的策略已经融入其中。这种方式只有在有模型的条件下才能计算，即知道状态转移概率p的情况下。我们能直接计算出p(s'|s)</p>
<p>事实上我们大多数情况下是无模型的，即不知道这个状态转移概率p。那么我们该如何得到价值V(s)呢。我们只需要按照期望的公式，重复在当前策略
<span class="math inline">\(\pi\)</span> 下采样获取 <span class="math inline">\(r_{t+1}+\gamma V_{\pi}(s_{t+1})\)</span>
，然后求平均即可。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215006682.png" style="zoom:67%;"></p>
<blockquote>
<p>我们假设参数X符合某个概率分布，我们从中采样N次，获得 <span class="math inline">\(x_1,x_2,…,x_N\)</span></p>
<p>那么根据大数定律： <span class="math display">\[
\frac{1}{N} \sum_{i=1}^{N}{x_i}≈ \mathbb{E}[X]
\]</span></p>
</blockquote>
</blockquote>
<p>我们进一步进行简单的分解，先给出式（2.8）：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215235976.png" style="zoom:67%;"></p>
<p>接着，我们再给出式(2.9)：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215320315.png" alt="image-20260217215320315" style="zoom:67%;"></p>
<p>我们把式(2.9)代入式(2.8)可得(2.10)</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217215353789.png" style="zoom:67%;"></p>
<p>式(2.10)代表当前状态的价值 与 未来状态价值之间的关联。</p>
<h2 id="策略评估和控制">6. 策略评估和控制</h2>
<h3 id="马尔可夫决策过程中的策略评估">6.1
马尔可夫决策过程中的策略评估</h3>
<blockquote>
<p>计算价值函数的过程就是策略评估。</p>
</blockquote>
<p><strong>同步备份</strong>是指每一次的迭代都会完全更新所有的状态，这对于程序资源的需求特别大。</p>
<p><strong>异步备份</strong>（asynchronous
backup）的思想就是通过某种方式，使得每一次迭代不需要更新所有的状态，因为事实上，很多状态也不需要被更新。</p>
<blockquote>
<p>备份类似于自举之间的迭代关系，将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。</p>
<p>在英文原文中，“to back up” 不仅仅有“备份”这个意思。</p>
<p>back up your car 倒车（往后移动）🚗</p>
<p>back up your argument 用证据支持（从后面支撑）</p>
<p>back up the value 从后面的状态把价值传回来（RL语境）</p>
<p>但是我们实际上更常用的表达是<strong>更新</strong></p>
<p>比如从V(s)依赖V(s')进行更新</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217231858283.png" alt="image-20260217231858283" style="zoom: 50%;"></p>
</blockquote>
<p>我们再来看一个动态的例子，推荐斯坦福大学的一个网页，这个网页模拟了式(2.18)所示的单步更新的过程中，所有格子的状态价值的变化过程。</p>
<p>https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html</p>
<blockquote>
<p>但是由于他的计算结果不符合他给出的公式</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232029958.png" style="zoom: 67%;"></p>
<p>所以我这里仅仅只是按照原文提及，不推荐。我会在后续策略迭代和价值迭代中用自己的代码重新实现。</p>
</blockquote>
<p>在b站上找到的案例，来简单解释。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232156124.png"></p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232205663.png" style="zoom:50%;"></p>
<p>根据公式</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232221258.png" style="zoom:67%;"></p>
<p>我们能看到-1.7的计算由来</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232250863.png" style="zoom:60%;"></p>
<h3 id="马尔可夫决策过程控制">6.2 马尔可夫决策过程控制</h3>
<blockquote>
<p>找到最优策略的过程就是控制</p>
</blockquote>
<p>首先介绍最佳价值函数：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217233141732.png" style="zoom:67%;"></p>
<p>最佳价值函数是指，我们搜索一种策略 <span class="math inline">\(\pi\)</span> ， 让每个状态的价值最大。<span class="math inline">\(V^∗\)</span>
就是到达每一个状态，它的值的最大化情况。
<strong>在这种最大化情况中，我们得到的策略就是最佳策略</strong>，即</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260218000232071.png" style="zoom:67%;"></p>
<p>最佳策略使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个最佳价值函数，就可以认为某个马尔可夫决策过程的环境可解。在这种情况下，最佳价值函数是一致的，环境中可达到的上限的值是一致的，但这里可能有多个最佳策略，多个最佳策略可以取得相同的最佳价值。</p>
<p>当取得最佳价值函数后，我们可以通过对 Q
函数进行最大化来得到最佳策略：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260218000711660.png" style="zoom:67%;"></p>
<p>当Q函数收敛后，因为 Q
函数是关于状态与动作的函数，所以如果在某个状态采取某个动作，可以使得 Q
函数最大化，那么这个动作就是最佳的动作。如果我们能优化出一个 Q 函数
<span class="math inline">\(Q^∗(s,a)\)</span> ，就可以直接在 Q
函数中取一个让 Q 函数值最大化的动作的值，就可以提取出最佳策略。</p>
<p>搜索最佳策略有两种常用的方法：<strong>策略迭代</strong>和<strong>价值迭代</strong>。</p>
<h3 id="策略迭代">6.3 策略迭代</h3>
<p>策略迭代由两个步骤组成：<strong>策略评估</strong>和<strong>策略改进</strong>（policy
improvement）。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219103740178.png" style="zoom: 50%;"></p>
<p>如图 2.21a 所示，</p>
<p>第一个步骤是<strong>策略评估</strong>，当前我们在优化策略 <span class="math inline">\(\pi\)</span>
，在优化过程中得到一个最新的策略。我们先固定这个策略不变，然后估计它的价值，即
<strong>给定当前的策略函数 来估计 状态价值函数</strong>。</p>
<p>第二个步骤是<strong>策略改进</strong>，得到
状态价值函数后，我们可以进一步推算出它的 Q 函数。得到 Q
函数后，我们直接对 Q 函数进行最大化，通过<strong>在 Q
函数做一个贪心的搜索来进一步改进策略</strong>。</p>
<p>这两个步骤一直在迭代进行。所以如图 2.21b
所示，在策略迭代里面，在初始化的时候，我们有一个初始化的状态价值函数 V
和 策略 <span class="math inline">\(\pi\)</span>
，然后在这两个步骤之间迭代。</p>
<p>图 2.21b 上面的线就是我们当前状态价值函数的值，下面的线是策略的值。
策略迭代的过程与踢皮球一样。我们先给定当前已有的策略函数，计算它的状态价值函数。算出状态价值函数后，我们会得到一个
Q 函数。我们对Q
函数采取贪心的策略，这样就像踢皮球，“踢”回策略。然后进一步改进策略，得到一个改进的策略后，它还不是最佳的策略，我们再进行策略评估，又会得到一个新的价值函数。基于这个新的价值函数再进行
Q 函数的最大化，这样逐渐迭代，状态价值函数和策略就会收敛。</p>
<blockquote>
<p>经典策略迭代中，环境具有有限状态集合 S，同时动作集合 A
也是有限的，因此可选策略数量是有限的，所以实质上就是：</p>
<blockquote>
<p>单调改进 + 有限策略空间 → 必然在有限步内到达最优策略</p>
</blockquote>
<p>我们这里的策略<span class="math inline">\(\pi\)</span>
是与参数无关的，因此对每个状态能进行直接的修改。后续我们会使用网络作为我们的策略，而网络是由各种参数构成的，因此我们的策略是与参数θ相关的，我们的<strong>策略也从<span class="math inline">\(\pi\)</span> 变成了 <span class="math inline">\(\pi_{\theta}\)</span></strong> 。</p>
<p>相比原来有许多的不同，</p>
<ol type="1">
<li>即使对于某一状态我们通过修改参数对其进行调整，也会影响到其他状态。</li>
<li>策略集合不再是“所有策略”，它只是一个子集。比如线性方程无法完全拟合曲线。原来的策略中所有状态都有单独的动作进行修改，即存在全部可能。而使用网络后只能表达部分的策略。</li>
</ol>
<p>因为参数 <span class="math inline">\(\theta\)</span>
是一个连续数，其空间是无限的，可能出现震荡、鞍点、局部最优等现象。所以无法确定在有限步内能到达最优策略。也就是说，后续网络策略
<span class="math inline">\(\pi_{\theta}\)</span>
并不严格遵从这里说的策略迭代理论。</p>
</blockquote>
<h3 id="价值迭代">6.4 价值迭代</h3>
<p>最优性原理：一个策略<span class="math inline">\(\pi\)</span> 在状态 s
达到了最优价值，那么对于任何能够从 s 到达的
s′的所有状态，在策略中都已经达到了最优价值。</p>
<p>举个例子方便理解，假设从A走到C：A → B → C</p>
<p>如果当前 <span class="math inline">\(V(A)\)</span> 在策略<span class="math inline">\(\pi\)</span>
下是最优的，那么根据最优性原理，可以得出 <span class="math inline">\(V(B)\)</span> 价值也是最优的。</p>
<p>为什么会得出这个结论呢，我们可以用价值更新公式来理解：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219115933261.png" style="zoom:67%;"></p>
<p>我们当前的 <span class="math inline">\(V(A)\)</span>
的价值计算需要下一状态价值<span class="math inline">\(V(B)\)</span>，那么如果当前策略策略 <span class="math inline">\(\pi\)</span> 是最优的话，当前 <span class="math inline">\(V(A)=V^*(A)\)</span>
（当前的V(A)是最大的)。那么很自然得出计算V(A)所需的V(B)也是最大的，即
<span class="math inline">\(V(B)=V^*(B)\)</span>。</p>
<p>依次推理，所有后续的状态价值V(s)都是最优的。这也就是所谓的后继的状态的每一步都按照最优的策略去做。</p>
<p>这和我们价值迭代有什么关系呢？价值迭代做的工作类似于价值的反向传播，每次迭代做一步传播。从终点状态开始逐步的传播到其他状态，就像之前的下图一样。如果某状态达到了最优，则该状态与终点之间就存在了最优线路。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260217232205663.png" style="zoom:50%;"></p>
<h3 id="策略迭代与价值迭代的区别">6.5 策略迭代与价值迭代的区别</h3>
<p>其核心区别在于更新的目标不同。</p>
<p>在策略迭代中，我们的更新目标是策略。</p>
<blockquote>
<p>举例：为了更新策略</p>
<p>在评估阶段：我们会根据当前策略，计算此时的所有状态s的Q(s,a)值。</p>
<p>在控制阶段：我们会根据计算出来的Q(s,a)，调整原来的策略，将动作动作概率重新分配到能获得最大的Q(s,a)的动作上，从而获得新的策略。</p>
</blockquote>
<p>在价值迭代中，我们的更新目标是价值。</p>
<blockquote>
<p>正如我们之前所说：价值迭代做的工作类似于价值的反向传播。</p>
<p>举例：</p>
<p>首先我们根据公式 <span class="math display">\[
Q(s,a)=R(s,a)+\gamma \sum^{}_{s&#39; \in S}P(s&#39;|s,a)V(s&#39;)
\]</span> 计算出状态s下的所有Q值（这里没有策略<span class="math inline">\(\pi\)</span> 参与，）</p>
<p>然后 <span class="math inline">\(当前的V(s)=最大的Q(s,a)\)</span></p>
<p>直到所有状态的V值稳定下来，我们策略就是执行能获取最大V(s)值的动作a。</p>
</blockquote>
<p>以上仅仅是为了解释说明 策略迭代 和 价值迭代
区别而举的例子，实际上后续由于网络的参与，我们价值也由网络替代，而不是固定的值。</p>
<h2 id="实战">7.实战</h2>
<h3 id="环境">7.1 环境</h3>
<p>和之前连接中的格子一样，终点是绿色那个，红色的是奖励为-1的陷阱。灰色为障碍，进去会回退到原来的格子中，边界也是。其他白色的是奖励为0的无害区域。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134112672.png" style="zoom:67%;"></p>
<p>正如之前所说，链接中执行和文章所写的对不上，因此个人自制环境进行表达。</p>
<p>上下左右箭头代表最优动作a的方向，初始是均等随机，因此四个箭头都有。左边的数字则是价值。也可以理解为最高的Q(s,a)的值。除此外陷阱、终点、障碍等位置均与上图一直。其表达请自行查看。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134319975.png"></p>
<p>可以再看一个效果最终图</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260219134817207.png"></p>
<p>环境代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># === GridWorld 基本参数 ===</span></span><br><span class="line">rows, cols = <span class="number">10</span>, <span class="number">10</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">theta = <span class="number">1e-4</span></span><br><span class="line">actions = [<span class="string">&#x27;↑&#x27;</span>, <span class="string">&#x27;↓&#x27;</span>, <span class="string">&#x27;←&#x27;</span>, <span class="string">&#x27;→&#x27;</span>]</span><br><span class="line">action_delta = &#123;<span class="string">&#x27;↑&#x27;</span>: (-<span class="number">1</span>, <span class="number">0</span>), <span class="string">&#x27;↓&#x27;</span>: (<span class="number">1</span>, <span class="number">0</span>), <span class="string">&#x27;←&#x27;</span>: (<span class="number">0</span>, -<span class="number">1</span>), <span class="string">&#x27;→&#x27;</span>: (<span class="number">0</span>, <span class="number">1</span>)&#125;</span><br><span class="line">goal = (<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">obstacles = &#123;(<span class="number">2</span>, <span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">6</span>),(<span class="number">2</span>,<span class="number">7</span>),(<span class="number">2</span>,<span class="number">8</span>)</span><br><span class="line">             ,(<span class="number">3</span>,<span class="number">4</span>),(<span class="number">4</span>,<span class="number">4</span>),(<span class="number">5</span>,<span class="number">4</span>),(<span class="number">6</span>,<span class="number">4</span>),(<span class="number">7</span>,<span class="number">4</span>)&#125; <span class="comment"># 障碍 (y↓,x→)</span></span><br><span class="line">punishment=&#123;(<span class="number">3</span>,<span class="number">3</span>),(<span class="number">4</span>,<span class="number">5</span>),(<span class="number">4</span>,<span class="number">6</span>),(<span class="number">5</span>,<span class="number">6</span>),(<span class="number">5</span>,<span class="number">8</span>),(<span class="number">6</span>,<span class="number">8</span>),(<span class="number">7</span>,<span class="number">3</span>),(<span class="number">7</span>,<span class="number">5</span>),(<span class="number">7</span>,<span class="number">6</span>),&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># === 初始化状态值函数和策略 ===</span></span><br><span class="line">V = np.zeros((rows, cols))</span><br><span class="line">policy = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(cols):</span><br><span class="line">        <span class="keyword">if</span> (r, c) <span class="keyword">in</span> obstacles <span class="keyword">or</span> (r, c) == goal:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        policy[(r, c)] = &#123;a: <span class="number">0.25</span> <span class="keyword">for</span> a <span class="keyword">in</span> actions&#125;</span><br><span class="line">        <span class="comment"># policy[(r, c)] = &#123;&#x27;↑&#x27;: 0.25, &#x27;↓&#x27;: 0.25, &#x27;←&#x27;: 0.25, &#x27;→&#x27;: 0.25&#125;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>还有用来打印展示的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_policy</span>(<span class="params">V, policy, iteration,ss=<span class="string">&#x27;&#x27;</span>,show_a=<span class="literal">True</span></span>):</span><br><span class="line">    cell_width = <span class="number">12</span>  <span class="comment"># 每个格子占固定宽度</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;打印每次迭代的值函数与策略&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;\n=== Policy Iteration <span class="subst">&#123;iteration&#125;</span> === <span class="subst">&#123;ss&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(rows):</span><br><span class="line">        line = <span class="string">&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(cols):</span><br><span class="line">            <span class="keyword">if</span> (r, c) <span class="keyword">in</span> obstacles:</span><br><span class="line">                text = <span class="string">&quot;■■■&quot;</span></span><br><span class="line">            <span class="keyword">elif</span> (r, c) <span class="keyword">in</span> punishment:</span><br><span class="line">                actions_str = <span class="string">&#x27;&#x27;</span>.join([a <span class="keyword">for</span> a, prob <span class="keyword">in</span> policy[(r, c)].items() <span class="keyword">if</span> prob &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> show_a:</span><br><span class="line">                    text = <span class="string">f&quot;■<span class="subst">&#123;V[r,c]:<span class="number">.2</span>f&#125;</span>■<span class="subst">&#123;actions_str&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    text = <span class="string">f&quot;■<span class="subst">&#123;V[r, c]:<span class="number">.2</span>f&#125;</span>■&quot;</span></span><br><span class="line">            <span class="keyword">elif</span> (r, c) == goal:</span><br><span class="line">                text = <span class="string">f&quot;■T■&quot;</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                actions_str = <span class="string">&#x27;&#x27;</span>.join([a <span class="keyword">for</span> a, prob <span class="keyword">in</span> policy[(r, c)].items() <span class="keyword">if</span> prob &gt; <span class="number">0</span>])</span><br><span class="line">                <span class="keyword">if</span> show_a:</span><br><span class="line">                    text = <span class="string">f&quot;<span class="subst">&#123;V[r, c]:<span class="number">.2</span>f&#125;</span><span class="subst">&#123;actions_str&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    text = <span class="string">f&quot;<span class="subst">&#123;V[r, c]:<span class="number">.2</span>f&#125;</span>&quot;</span></span><br><span class="line">            <span class="comment"># 居中对齐</span></span><br><span class="line">            line += text.center(cell_width)</span><br><span class="line">        <span class="built_in">print</span>(line)</span><br></pre></td></tr></table></figure>
<h3 id="策略迭代-1">7.2策略迭代</h3>
<p>首先来查看<strong>主流程</strong>。通过<code>policy_evaluation</code>
函数进行策略评估，算出当前每个状态的价值。然后通过<code>policy_improvement</code>
进行策略改进。每一轮重复执行这两步骤。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">iteration = <span class="number">0</span></span><br><span class="line"><span class="comment"># 策略迭代</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    iteration += <span class="number">1</span></span><br><span class="line">    V = policy_evaluation(V, policy, gamma, theta)</span><br><span class="line">    print_policy(V, policy, iteration,<span class="string">&#x27;策略评估&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    policy, stable = policy_improvement(V, policy, gamma)</span><br><span class="line">    print_policy(V, policy, iteration,<span class="string">&#x27;策略改进&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>接下来看<strong>策略评估</strong>内容。</p>
<p>主要看下图红框中的两行</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221190810623.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v += prob * (reward + gamma * V[nr, nc])</span><br></pre></td></tr></table></figure>
<p>我们将s能到达的下一状态s‘的价值V[nr, nc]
按照概率进行加权求和。将此结果当作状态s的新的价值。</p>
<p>然后是<strong>策略改进</strong>的部分。</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221191921853.png"></p>
<p>最后我们能看一下效果，首先是第一轮的策略评估和策略改进</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221192217950.png"></p>
<p>然后对比第20轮</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221192253830.png"></p>
<h3 id="价值迭代迭代">7.3 价值迭代迭代</h3>
<p>也还是先看<strong>主流程</strong>。可以看到，循环内只有一个策略评估在不断迭代。因为与动作无关，所以我们打印的时候关闭了动作。</p>
<p>在价值迭代结束后，我们进行一次策略改进。（我们原来的策略迭代中就是选择最优策略，因此这里可以重复使用。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    iteration += <span class="number">1</span></span><br><span class="line">    V = policy_evaluation2(V, policy, gamma, theta)</span><br><span class="line">    print_policy(V, policy, iteration,<span class="string">&#x27;策略评估&#x27;</span>,show_a=<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">policy, stable = policy_improvement(V, policy, gamma)</span><br><span class="line">print_policy(V, policy, iteration, <span class="string">&#x27;策略改进&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>然后是价值迭代的函数内容</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221193523718.png"></p>
<p>看上去和之前是不是很像，红框的地方就是差异点。我们之前说过，价值迭代，不需要策略参与，所以之前我们按照策略给出了动作概率prob，对各个动作进行加权求和。而这里，我们直接对各个可能的动作进行价值迭代计算，不需要使用策略给出的概率。然后我们选择最高的更新值为我们的价值。</p>
<p>查看下运行效果。</p>
<p>前几轮：</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194004166.png"></p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194035877.png"></p>
<p>像不像一口小喷泉，从目标处开始将价值扩散到所有的状态。</p>
<p>最终20轮</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194208734.png"></p>
<p>然后我们最终进行一次策略改进</p>
<p><img src="/2026/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/image-20260221194302460.png"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ai/" rel="tag"># ai</a>
              <a href="/tags/%E8%98%91%E8%8F%87%E4%B9%A6/" rel="tag"># 蘑菇书</a>
              <a href="/tags/%E5%BC%BA%E5%8C%96%E6%83%B3%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化想学习</a>
              <a href="/tags/%E7%AC%AC2%E8%8A%82/" rel="tag"># 第2节</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2026/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A01/" rel="prev" title="强化学习 1. 强化学习基础">
                  <i class="fa fa-angle-left"></i> 强化学习 1. 强化学习基础
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">John Doe</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        访客数：<span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        阅读量：<span id="busuanzi_value_site_pv"></span>
      </span>
    </span>



  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
